This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: _private, .specstory
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
    quality.mdc
.github/
  workflows/
    push.yml
    release.yml
cursor/
  rules/
    0project.mdc
src/
  twat_coding/
    pystubnik/
      backends/
        __init__.py
        ast_backend.py
        base.py
        mypy_backend.py
      core/
        config.py
        shared_types.py
        types.py
        utils.py
      processors/
        __init__.py
        docstring.py
        file_importance.py
        importance.py
        imports.py
        stub_generation.py
        type_inference.py
      types/
        docstring.py
        type_system.py
      utils/
        ast_utils.py
        display.py
        memory.py
      __init__.py
      config.py
      errors.py
      read_imports.py
      README.md
    __init__.py
    twat_coding.py
tests/
  test_package.py
.gitignore
.pre-commit-config.yaml
cleanup.py
LICENSE
MANIFEST.in
mypy.ini
package.toml
pyproject.toml
README.md
TODO.md

================================================================
Files
================================================================

================
File: .cursor/rules/0project.mdc
================
---
description: About this project
globs: 
---
---
name: About this project
key: 0project
---

`twat-coding` is a Python toolkit focused on code analysis and transformation. Its main package `pystubnik` generates "smart stubs" - a hybrid between full source code and type stubs that helps LLMs understand large codebases efficiently. Smart stubs preserve essential code structure (signatures, types, imports) while intelligently reducing verbosity based on code importance.

## 1. Overview

Pystubnik creates a "shadow" directory structure that mirrors your Python package, containing smart stubs for all Python files. These smart stubs are designed to be more informative than traditional `.pyi` stub files while being more concise than full source code.

### 1.1. What are Smart Stubs?

Smart stubs are an intermediate representation that includes:
- All function and class signatures with type hints
- All imports (organized and optimized)
- Docstrings (with configurable length limits)
- Important/relevant code sections
- Truncated versions of large data structures and strings
- Simplified function bodies for non-critical code

The verbosity level is automatically adjusted based on the code's importance and complexity.

## 2. Architecture

### 2.1. Backends

#### 2.1.1. AST Backend
- Uses Python's built-in AST module for precise control
- Preserves code structure while reducing verbosity
- Configurable truncation of large literals and sequences
- Maintains type information and docstrings
- Supports Python 3.12+ features (type parameters, etc.)

#### 2.1.2. MyPy Backend
- Leverages mypy's stubgen for type information
- Better type inference capabilities
- Handles special cases (dataclasses, properties)
- Supports type comment extraction

### 2.2. Processors

#### 2.2.1. Import Processor
- Analyzes and organizes imports
- Groups by type (stdlib, third-party, local)
- Handles relative imports
- Detects and removes duplicates

#### 2.2.2. Docstring Processor
- Configurable docstring preservation
- Format detection and conversion
- Type information extraction
- Length-based truncation

#### 2.2.3. Importance Processor
- Scores code elements by importance
- Pattern-based importance detection
- Inheritance-aware scoring
- Configurable filtering

================
File: .cursor/rules/cleanup.mdc
================
---
description: Run `cleanup.py` script before and after changes
globs: 
---
Before you do any changes or if I say "cleanup", run the `./cleanup.py install && source .venv/bin/activate && ./cleanup.py update | cat` script in the main folder. Analyze the results, describe recent changes @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `./cleanup.py update | cat` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done, and `- [!]` if they're NEXT TODO. 

Don't use `pip`, use `uv pip`.

================
File: .cursor/rules/filetree.mdc
================
---
description: File tree of the project
globs: 
---
[ 960]  .
├── [  64]  .benchmarks
├── [  96]  .cursor
│   └── [ 224]  rules
│       ├── [2.2K]  0project.mdc
│       ├── [ 632]  cleanup.mdc
│       ├── [3.8K]  filetree.mdc
│       └── [2.0K]  quality.mdc
├── [  96]  .github
│   └── [ 128]  workflows
│       ├── [2.7K]  push.yml
│       └── [1.4K]  release.yml
├── [3.5K]  .gitignore
├── [ 532]  .pre-commit-config.yaml
├── [  96]  .specstory
│   └── [ 512]  history
│       ├── [2.0K]  .what-is-this.md
│       ├── [ 63K]  adjusting-todo-priorities-from-cleanup-status.md
│       ├── [ 97K]  cleanup-script-execution-and-todo-management-1.md
│       ├── [172K]  cleanup-script-execution-and-todo-management.md
│       ├── [333K]  command-execution-and-todo-update.md
│       ├── [ 248]  detailed-overview-of-python-stubbing-tools.md
│       ├── [ 25K]  integrating-importance-analysis-modules.md
│       ├── [267K]  managing-todo-list-tasks.md
│       ├── [3.0K]  overview-of-python-tools-and-their-functions.md
│       ├── [ 58K]  project-directory-structure-for-python-package.md
│       ├── [ 64K]  reviewing-and-tracking-todo-progress.md
│       ├── [ 56K]  task-management-from-todo-md.md
│       ├── [4.9K]  task-organization-and-cleanup-process.md
│       └── [165K]  todo-list-cleanup-and-review.md
├── [1.0K]  LICENSE
├── [ 173]  MANIFEST.in
├── [1.4K]  README.md
├── [ 17K]  TODO.md
├── [ 12K]  cleanup.py
├── [  96]  cursor
│   └── [  96]  rules
│       └── [  31]  0project.mdc
├── [  96]  dist
│   └── [   1]  .gitkeep
├── [ 305]  mypy.ini
├── [ 426]  package.toml
├── [4.8K]  pyproject.toml
├── [237K]  repomix-output.txt
├── [ 128]  src
│   └── [ 256]  twat_coding
│       ├── [ 144]  __init__.py
│       ├── [ 416]  pystubnik
│       │   ├── [5.3K]  README.md
│       │   ├── [ 16K]  __init__.py
│       │   ├── [ 224]  backends
│       │   │   ├── [1.4K]  __init__.py
│       │   │   ├── [ 13K]  ast_backend.py
│       │   │   ├── [2.1K]  base.py
│       │   │   └── [1.1K]  mypy_backend.py
│       │   ├── [8.8K]  config.py
│       │   ├── [ 224]  core
│       │   │   ├── [6.1K]  config.py
│       │   │   ├── [ 492]  shared_types.py
│       │   │   ├── [3.9K]  types.py
│       │   │   └── [8.3K]  utils.py
│       │   ├── [5.1K]  errors.py
│       │   ├── [ 320]  processors
│       │   │   ├── [1.2K]  __init__.py
│       │   │   ├── [5.0K]  docstring.py
│       │   │   ├── [ 16K]  file_importance.py
│       │   │   ├── [7.5K]  importance.py
│       │   │   ├── [6.6K]  imports.py
│       │   │   ├── [ 11K]  stub_generation.py
│       │   │   └── [7.4K]  type_inference.py
│       │   ├── [1.6K]  read_imports.py
│       │   ├── [ 160]  types
│       │   │   ├── [9.2K]  docstring.py
│       │   │   └── [9.7K]  type_system.py
│       │   └── [ 192]  utils
│       │       ├── [5.6K]  ast_utils.py
│       │       ├── [1.8K]  display.py
│       │       └── [5.2K]  memory.py
│       └── [1.6K]  twat_coding.py
└── [ 128]  tests
    └── [2.3K]  test_package.py

20 directories, 61 files

================
File: .cursor/rules/quality.mdc
================
---
description: Quality
globs: 
---
- **Verify Information**: Always verify information before presenting it. Do not make assumptions or speculate without clear evidence.
- **No Apologies**: Never use apologies.
- **No Whitespace Suggestions**: Don't suggest whitespace changes.
- **No Inventions**: Don't invent major changes other than what's explicitly requested.
- **No Unnecessary Confirmations**: Don't ask for confirmation of information already provided in the context.
- **Preserve Existing Code**: Don't remove unrelated code or functionalities. Pay attention to preserving existing structures.
- **No Implementation Checks**: Don't ask the user to verify implementations that are visible in the provided context.
- **No Unnecessary Updates**: Don't suggest updates or changes to files when there are no actual modifications needed.
- **No Current Implementation**: Don't show or discuss the current implementation unless specifically requested.
- **Use Explicit Variable Names**: Prefer descriptive, explicit variable names over short, ambiguous ones to enhance code readability.
- **Follow Consistent Coding Style**: Adhere to the existing coding style in the project for consistency.
- **Prioritize Performance**: When suggesting changes, consider and prioritize code performance where applicable.
- **Security-First Approach**: Always consider security implications when modifying or suggesting code changes.
- **Test Coverage**: Suggest or include appropriate unit tests for new or modified code.
- **Error Handling**: Implement robust error handling and logging where necessary.
- **Modular Design**: Encourage modular design principles to improve code maintainability and reusability.
- **Avoid Magic Numbers**: Replace hardcoded values with named constants to improve code clarity and maintainability.
- **Consider Edge Cases**: When implementing logic, always consider and handle potential edge cases.
- **Use Assertions**: Include assertions wherever possible to validate assumptions and catch potential errors early.

================
File: .github/workflows/push.yml
================
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_coding --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5

================
File: .github/workflows/release.yml
================
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-coding
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

================
File: cursor/rules/0project.mdc
================
---
description: 
globs: 
---

================
File: src/twat_coding/pystubnik/backends/__init__.py
================
"""Backend interface and registry for stub generation.

This module provides the abstract base class for stub generation backends
and a registry to manage different backend implementations.
"""

from abc import abstractmethod
from collections.abc import Coroutine
from pathlib import Path
from typing import Any, Protocol

from ..core.types import StubResult


class StubBackend(Protocol):
    """Protocol defining the interface for stub generation backends."""

    @abstractmethod
    async def generate_stub(
        self, source_path: Path, output_path: Path | None = None
    ) -> str:
        """Generate stub for the given source file.

        Args:
            source_path: Path to the source file
            output_path: Optional path to write the stub file

        Returns:
            Generated stub content as string
        """
        ...


_backends: dict[str, type[StubBackend]] = {}


def register_backend(name: str, backend: type[StubBackend]) -> None:
    """Register a new backend implementation."""
    _backends[name] = backend


def get_backend(name: str) -> type[StubBackend]:
    """Get a registered backend by name."""
    if name not in _backends:
        msg = f"Backend '{name}' not found"
        raise KeyError(msg)
    return _backends[name]


def list_backends() -> list[str]:
    """List all registered backend names."""
    return list(_backends.keys())

================
File: src/twat_coding/pystubnik/backends/ast_backend.py
================
#!/usr/bin/env -S uv run
"""AST-based stub generation backend.

This module implements stub generation using Python's ast module,
based on the original make_stubs_ast.py implementation.
"""

import ast
import asyncio
import functools
import weakref
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import Any, ClassVar, cast, Union

from loguru import logger

from .. import _convert_to_stub_gen_config
from ..config import StubConfig
from ..core.config import (
    Backend,
    PathConfig,
    ProcessingConfig,
    RuntimeConfig,
    StubGenConfig,
    TruncationConfig,
)
from ..core.types import StubResult
from ..errors import ASTError, ErrorCode
from ..processors import Processor
from ..utils.ast_utils import attach_parents, truncate_literal
from ..utils.display import print_progress
from ..utils.memory import MemoryMonitor, stream_process_ast
from . import StubBackend


@dataclass
class ASTCacheEntry:
    """Cache entry for parsed AST nodes."""

    node: ast.AST
    source_hash: str
    access_time: float


class SignatureExtractor(ast.NodeTransformer):
    """Extract type signatures and docstrings from AST."""

    def __init__(
        self, config: StubGenConfig, file_size: int = 0, importance_score: float = 1.0
    ):
        """Initialize the signature extractor.

        Args:
            config: Configuration for stub generation
            file_size: Size of the source file
            importance_score: Importance score for the file
        """
        super().__init__()
        self.config = config
        self.file_size = file_size
        self.importance_score = importance_score

    def _preserve_docstring(self, body: list[ast.stmt]) -> list[ast.stmt]:
        """Preserve docstrings based on importance and configuration.

        Args:
            body: List of AST statements

        Returns:
            List of AST statements with docstrings preserved or removed
        """
        if not body:
            return []
        if (
            self.file_size > self.config.truncation.max_file_size
            and self.importance_score < 0.7
        ):
            return []  # Skip docstrings for big, low-importance files
        match body[0]:
            case ast.Expr(value=ast.Constant(value=str() as docstring)):
                if (
                    len(docstring) > self.config.truncation.max_docstring_length
                    and self.importance_score < 0.9
                ):
                    return []  # Skip long docstrings unless very important
                return [body[0]]
            case _:
                return []

    def visit_Module(self, node: ast.Module) -> ast.Module:
        """Process module node.

        Args:
            node: Module AST node

        Returns:
            Processed module node
        """
        node.body = [
            stmt
            for stmt in node.body
            if isinstance(
                stmt, ast.FunctionDef | ast.ClassDef | ast.Import | ast.ImportFrom
            )
            or (isinstance(stmt, ast.Expr) and isinstance(stmt.value, ast.Constant))
        ]
        return cast(ast.Module, self.generic_visit(node))

    def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.FunctionDef:
        """Process function definition.

        Args:
            node: Function definition AST node

        Returns:
            Processed function node
        """
        docstring = self._preserve_docstring(node.body)
        node.body = docstring + [ast.Expr(value=ast.Constant(value=...))]
        return node

    def visit_ClassDef(self, node: ast.ClassDef) -> ast.ClassDef:
        """Process class definition.

        Args:
            node: Class definition AST node

        Returns:
            Processed class node
        """
        docstring = self._preserve_docstring(node.body)
        node.body = docstring + [
            stmt
            for stmt in node.body
            if isinstance(stmt, ast.FunctionDef | ast.ClassDef)
        ]
        return cast(ast.ClassDef, self.generic_visit(node))


class ASTBackend(StubBackend):
    """AST-based stub generation backend with advanced concurrency."""

    # Class-level LRU cache for parsed AST nodes
    _ast_cache: ClassVar[dict[Path, Any]] = {}
    _ast_cache_lock: ClassVar[asyncio.Lock] = asyncio.Lock()
    _max_cache_size: ClassVar[int] = 100

    def __init__(self, config: Union[StubConfig, StubGenConfig] | None = None) -> None:
        """Initialize the backend.

        Args:
            config: Configuration for stub generation
        """
        super().__init__(config)
        self.processors: list[Processor] = []  # List of processors to apply to stubs
        self._node_registry: weakref.WeakValueDictionary[int, ast.AST] = weakref.WeakValueDictionary()

        # Handle different config types
        if isinstance(config, StubConfig):
            self._executor = ThreadPoolExecutor(
                max_workers=config.max_workers if config.parallel else 1,
                thread_name_prefix="ast_backend",
            )
            self.include_patterns = config.include_patterns
            self.exclude_patterns = config.exclude_patterns
        else:  # StubGenConfig or None
            stub_config = config or StubGenConfig(paths=PathConfig(), runtime=RuntimeConfig())
            self._executor = ThreadPoolExecutor(
                max_workers=stub_config.runtime.max_workers if stub_config.runtime.parallel else 1,
                thread_name_prefix="ast_backend",
            )
            self.include_patterns = ["*.py"]  # Default patterns
            self.exclude_patterns = ["test_*.py", "*_test.py"]

        self._memory_monitor = MemoryMonitor()

    @property
    def config(self) -> StubConfig | StubGenConfig:
        """Get the current configuration.

        Returns:
            Current configuration
        """
        return self._config

    async def generate_stub(self, source_path: Path) -> StubResult:
        """Generate a stub for a Python source file.

        Args:
            source_path: Path to the source file

        Returns:
            Generated stub result
        """
        try:
            # Read source file
            source = await self._run_in_executor(source_path.read_text)

            # Parse AST
            tree = await self._run_in_executor(
                functools.partial(ast.parse, source, filename=str(source_path))
            )
            attach_parents(tree)

            # Transform AST
            stub_gen_config = self.config if isinstance(self.config, StubGenConfig) else _convert_to_stub_gen_config(self.config)
            transformer = SignatureExtractor(stub_gen_config, len(source))
            transformed = transformer.visit(tree)

            # Generate stub content
            stub_content = ast.unparse(transformed)

            # Create StubResult
            result = StubResult(
                source_path=source_path,
                stub_content=stub_content,
                imports=[],  # TODO: Extract imports
                errors=[],
                importance_score=0.0,
                metadata={},
            )

            # Apply processors if available
            if hasattr(self, "processors"):
                for processor in self.processors:
                    result = processor.process(result)

            return result

        except Exception as e:
            logger.error(f"Error generating stub for {source_path}: {e}")
            raise ASTError(
                code=ErrorCode.AST_TRANSFORM_ERROR,
                message=f"Failed to generate stub for {source_path}",
                details={"error": str(e)},
            ) from e

    async def process_directory(self, directory: Path) -> dict[Path, StubResult]:
        """Process a directory recursively.

        Args:
            directory: Directory to process

        Returns:
            Dictionary mapping output paths to stub results

        Raises:
            ASTError: If directory processing fails
        """
        try:
            # Find all Python files matching patterns
            python_files: list[Path] = []
            for pattern in self.include_patterns:
                python_files.extend(directory.rglob(pattern))

            # Filter out excluded files
            for pattern in self.exclude_patterns:
                python_files = [f for f in python_files if not f.match(pattern)]

            # Process files with progress reporting
            results: dict[Path, StubResult] = {}
            total = len(python_files)
            for i, path in enumerate(python_files, 1):
                try:
                    print_progress("Processing files", i, total)
                    result = await self.generate_stub(path)
                    results[path] = result
                except Exception as e:
                    logger.error(f"Failed to process {path}: {e}")
                    if isinstance(self.config, StubGenConfig) and self.config.runtime.ignore_errors:
                        continue
                    raise

            return results

        except Exception as e:
            raise ASTError(
                f"Failed to process directory {directory}: {e}",
                ErrorCode.AST_PARSE_ERROR,
                source=str(directory),
            ) from e

    async def process_module(self, module_name: str) -> StubResult:
        """Process a module by its import name.

        Args:
            module_name: Fully qualified module name

        Returns:
            Generated stub result

        Raises:
            StubGenerationError: If module processing fails
        """
        # TODO: Implement module processing
        raise NotImplementedError

    async def process_package(self, package_path: Path) -> dict[Path, StubResult]:
        """Process a package directory recursively.

        Args:
            package_path: Path to the package directory

        Returns:
            Dictionary mapping output paths to stub results

        Raises:
            StubGenerationError: If package processing fails
        """
        return await self.process_directory(package_path)

    def cleanup(self) -> None:
        """Clean up resources."""
        self._executor.shutdown(wait=True)
        self._ast_cache.clear()
        self._node_registry.clear()
        self._memory_monitor.stop()

    async def _run_in_executor(self, func: Any, *args: Any) -> Any:
        """Run a function in the thread pool executor.

        Args:
            func: Function to run
            *args: Arguments to pass to the function

        Returns:
            Result of the function
        """
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(self._executor, func, *args)

================
File: src/twat_coding/pystubnik/backends/base.py
================
#!/usr/bin/env -S uv run
"""Base interface for stub generation backends."""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any, Union

from .. import _convert_to_stub_gen_config
from ..config import StubConfig
from ..core.config import PathConfig, StubGenConfig
from ..core.types import StubResult


class StubBackend(ABC):
    """Base class for stub generation backends."""

    def __init__(self, config: Union[StubConfig, StubGenConfig] | None = None) -> None:
        """Initialize the backend.

        Args:
            config: Configuration for stub generation
        """
        # Convert StubConfig to StubGenConfig if needed
        if isinstance(config, StubConfig):
            self._config = _convert_to_stub_gen_config(config)
        else:
            self._config = config or StubGenConfig(paths=PathConfig())

    @abstractmethod
    async def generate_stub(self, source_path: Path) -> StubResult:
        """Generate a stub for a Python source file.

        Args:
            source_path: Path to the source file

        Returns:
            Generated stub result
        """
        raise NotImplementedError

    @abstractmethod
    async def process_module(self, module_name: str) -> StubResult:
        """Process a module by its import name.

        Args:
            module_name: Fully qualified module name

        Returns:
            Generated stub result

        Raises:
            StubGenerationError: If module processing fails
        """
        raise NotImplementedError

    @abstractmethod
    async def process_package(self, package_path: Path) -> dict[Path, StubResult]:
        """Process a package directory recursively.

        Args:
            package_path: Path to the package directory

        Returns:
            Dictionary mapping output paths to stub results

        Raises:
            StubGenerationError: If package processing fails
        """
        raise NotImplementedError

    @abstractmethod
    def cleanup(self) -> None:
        """Clean up any resources used by the backend.

        This method should be called when the backend is no longer needed.
        """
        pass

    def __enter__(self) -> "StubBackend":
        """Enter the context manager."""
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit the context manager."""
        self.cleanup()

================
File: src/twat_coding/pystubnik/backends/mypy_backend.py
================
"""MyPy-based stub generation backend.

This module implements stub generation using MyPy's stubgen functionality,
based on the original make_stubs_mypy.py implementation.
"""

from pathlib import Path

from ..core.config import StubGenConfig
from . import StubBackend


class MypyBackend(StubBackend):
    """MyPy-based stub generation backend."""

    def __init__(self, config: StubGenConfig | None = None):
        """Initialize the backend.

        Args:
            config: Stub generation configuration
        """
        super().__init__()
        self.config = config

    async def generate_stub(
        self, source_path: Path, output_path: Path | None = None
    ) -> str:
        """Generate stub for the given source file using MyPy's stubgen.

        Args:
            source_path: Path to the source file
            output_path: Optional path to write the stub file

        Returns:
            Generated stub content as string
        """
        # TODO: Port functionality from make_stubs_mypy.py
        msg = "MyPy backend not yet implemented"
        raise NotImplementedError(msg)

================
File: src/twat_coding/pystubnik/core/config.py
================
#!/usr/bin/env python3
"""Configuration for pystubnik stub generation."""

import sys
from collections.abc import Mapping, Sequence
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import Any

from .shared_types import TruncationConfig


class Backend(Enum):
    """Available stub generation backends."""

    AST = auto()  # Use Python's AST for precise control
    MYPY = auto()  # Use MyPy's stubgen for better type inference
    HYBRID = auto()  # Use both and merge results


class ImportanceLevel(Enum):
    """Importance levels for code elements."""

    CRITICAL = 1.5  # Must preserve exactly
    HIGH = 1.2  # Should preserve most details
    NORMAL = 1.0  # Standard preservation
    LOW = 0.8  # Minimal preservation
    IGNORE = 0.0  # Can be omitted


def _default_importance_keywords() -> set[str]:
    """Get default importance keywords.

    Returns:
        Set of default importance keywords
    """
    return {"important", "critical", "essential", "main", "key"}


@dataclass(frozen=True)
class ProcessingConfig:
    """Configuration for stub processing."""

    include_docstrings: bool = True
    include_private: bool = False
    include_type_comments: bool = True
    infer_property_types: bool = True
    export_less: bool = False
    importance_patterns: Mapping[str, float] = field(default_factory=dict)
    importance_keywords: set[str] = field(default_factory=_default_importance_keywords)


def _default_include_patterns() -> list[str]:
    """Get default include patterns.

    Returns:
        List of default include patterns
    """
    return ["*.py"]


def _default_exclude_patterns() -> list[str]:
    """Get default exclude patterns.

    Returns:
        List of default exclude patterns
    """
    return ["test_*.py", "*_test.py"]


@dataclass(frozen=True)
class PathConfig:
    """Configuration for file paths and search."""

    output_dir: Path = Path("out")
    doc_dir: Path | None = None
    search_paths: Sequence[Path] = field(default_factory=list)
    modules: Sequence[str] = field(default_factory=list)
    packages: Sequence[str] = field(default_factory=list)
    files: Sequence[Path] = field(default_factory=list)
    include_patterns: list[str] = field(default_factory=_default_include_patterns)
    exclude_patterns: list[str] = field(default_factory=_default_exclude_patterns)


@dataclass(frozen=True)
class RuntimeConfig:
    """Configuration for runtime behavior."""

    backend: Backend = Backend.HYBRID
    python_version: tuple[int, int] = field(
        default_factory=lambda: sys.version_info[:2]
    )
    interpreter: Path = field(default_factory=lambda: Path(sys.executable))
    no_import: bool = False
    inspect: bool = False
    parse_only: bool = False
    ignore_errors: bool = True
    verbose: bool = False
    quiet: bool = True
    parallel: bool = True
    max_workers: int | None = None

    @classmethod
    def create(
        cls,
        *,
        python_version: tuple[int, int] | None = None,
        interpreter: Path | str | None = None,
        **kwargs: Any,
    ) -> "RuntimeConfig":
        """Create a RuntimeConfig with optional version and interpreter.

        Args:
            python_version: Optional Python version tuple
            interpreter: Optional interpreter path
            **kwargs: Additional configuration options

        Returns:
            RuntimeConfig instance
        """
        if python_version is None:
            python_version = sys.version_info[:2]

        if interpreter is None:
            interpreter = Path(sys.executable)
        elif isinstance(interpreter, str):
            interpreter = Path(interpreter)

        return cls(
            python_version=python_version,
            interpreter=interpreter,
            **kwargs,
        )


@dataclass(frozen=True)
class StubGenConfig:
    """Main configuration for stub generation."""

    paths: PathConfig
    runtime: RuntimeConfig = field(default_factory=RuntimeConfig)
    processing: ProcessingConfig = field(default_factory=ProcessingConfig)
    truncation: TruncationConfig = field(default_factory=TruncationConfig)

    # Convenience properties to access common settings
    @property
    def include_patterns(self) -> list[str]:
        """Get include patterns."""
        return self.paths.include_patterns

    @property
    def exclude_patterns(self) -> list[str]:
        """Get exclude patterns."""
        return self.paths.exclude_patterns

    @property
    def ignore_errors(self) -> bool:
        """Get ignore errors setting."""
        return self.runtime.ignore_errors

    def get_file_locations(self, source_path: Path) -> tuple[Path, Path]:
        """Get input and output paths for a source file.

        Args:
            source_path: Path to source file

        Returns:
            Tuple of (input_path, output_path)
        """
        try:
            rel_path = source_path.relative_to(self.paths.output_dir)
            output_path = self.paths.output_dir / rel_path
            return source_path, output_path
        except ValueError as e:
            raise ValueError(
                f"Source file {source_path} is not within output directory {self.paths.output_dir}"
            ) from e

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "StubGenConfig":
        """Create configuration from a dictionary."""
        paths = PathConfig(**data.get("paths", {}))
        runtime = RuntimeConfig(**data.get("runtime", {}))
        processing = ProcessingConfig(**data.get("processing", {}))
        truncation = TruncationConfig(**data.get("truncation", {}))
        return cls(
            paths=paths,
            runtime=runtime,
            processing=processing,
            truncation=truncation,
        )

    def to_dict(self) -> dict[str, Any]:
        """Convert configuration to a dictionary."""
        return {
            "paths": {
                k: str(v) if isinstance(v, Path) else v
                for k, v in self.paths.__dict__.items()
            },
            "runtime": self.runtime.__dict__,
            "processing": self.processing.__dict__,
            "truncation": self.truncation.__dict__,
        }

================
File: src/twat_coding/pystubnik/core/shared_types.py
================
"""Shared types for pystubnik."""

from dataclasses import dataclass


@dataclass(frozen=True)
class TruncationConfig:
    """Configuration for code truncation."""

    max_sequence_length: int = 4  # For lists, dicts, sets, tuples
    max_string_length: int = 17  # For strings except docstrings
    max_docstring_length: int = 150  # Default max length for docstrings
    max_file_size: int = 3_000  # Default max file size before removing all docstrings
    truncation_marker: str = "..."

================
File: src/twat_coding/pystubnik/core/types.py
================
#!/usr/bin/env python3
"""Core type definitions for pystubnik."""

from collections.abc import Sequence
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Protocol, Union


@dataclass(frozen=True)
class ArgInfo:
    """Information about a function argument."""

    name: str
    type: str | None = None
    default: bool = False
    default_value: Any | None = None
    is_kwonly: bool = False
    is_posonly: bool = False
    is_variadic: bool = False


@dataclass(frozen=True)
class FunctionInfo:
    """Information about a function or method."""

    name: str
    args: Sequence[ArgInfo]
    return_type: str | None = None
    docstring: str | None = None
    decorators: Sequence[str] = ()
    is_async: bool = False
    is_generator: bool = False
    is_abstract: bool = False
    is_property: bool = False
    is_classmethod: bool = False
    is_staticmethod: bool = False
    importance_score: float = 1.0


@dataclass(frozen=True)
class ClassInfo:
    """Information about a class."""

    name: str
    bases: Sequence[str] = ()
    docstring: str | None = None
    decorators: Sequence[str] = ()
    methods: Sequence[FunctionInfo] = ()
    properties: Sequence[FunctionInfo] = ()
    class_vars: dict[str, str] = field(default_factory=dict)  # name -> type
    instance_vars: dict[str, str] = field(default_factory=dict)  # name -> type
    is_abstract: bool = False
    importance_score: float = 1.0


@dataclass(frozen=True)
class ModuleInfo:
    """Information about a module."""

    name: str
    path: Path | None = None
    docstring: str | None = None
    imports: Sequence[str] = ()
    functions: Sequence[FunctionInfo] = ()
    classes: Sequence[ClassInfo] = ()
    variables: dict[str, str] = field(default_factory=dict)  # name -> type
    all_names: Sequence[str] | None = None
    importance_score: float = 1.0


class StubBackend(Protocol):
    """Protocol for stub generation backends."""

    def generate_module_info(self, module_path: Path) -> ModuleInfo:
        """Generate module information from a source file."""
        ...

    def generate_stub(self, module_info: ModuleInfo) -> str:
        """Generate stub content from module information."""
        ...


class ImportProcessor(Protocol):
    """Protocol for import processing."""

    def process_imports(self, source: str) -> Sequence[str]:
        """Extract and process imports from source code."""
        ...

    def sort_imports(self, imports: Sequence[str]) -> Sequence[str]:
        """Sort imports in a standard way."""
        ...


class DocstringProcessor(Protocol):
    """Protocol for docstring processing."""

    def process_docstring(
        self, docstring: str | None, context: dict[str, Any]
    ) -> tuple[str | None, dict[str, Any]]:
        """Process a docstring and extract type information."""
        ...


class ImportanceScorer(Protocol):
    """Protocol for importance scoring."""

    def calculate_score(
        self,
        name: str,
        docstring: str | None = None,
        context: dict[str, Any] | None = None,
    ) -> float:
        """Calculate importance score for a symbol."""
        ...


class ImportType(Enum):
    """Types of imports that can be found in Python code."""

    STANDARD_LIB = "stdlib"
    THIRD_PARTY = "third_party"
    LOCAL = "local"
    RELATIVE = "relative"


@dataclass
class ImportInfo:
    """Information about an import statement."""

    module_name: str
    import_type: "ImportType"
    imported_names: list[str]
    is_from_import: bool = False
    relative_level: int = 0


@dataclass
class StubResult:
    """Result of stub generation."""

    source_path: Path
    stub_content: str
    imports: list[ImportInfo]
    errors: list[str]
    importance_score: float = 0.0
    metadata: dict[str, Any] = field(default_factory=dict)


# Type aliases
PathLike = Union[str, Path]
ImportMap = dict[str, ImportInfo]
StubMap = dict[PathLike, StubResult]

================
File: src/twat_coding/pystubnik/core/utils.py
================
#!/usr/bin/env python3
"""Utility functions for pystubnik."""

import ast
import os
import re
import sys
from collections.abc import Callable, Iterable, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import TypeVar

from loguru import logger

T = TypeVar("T")
U = TypeVar("U")


def process_parallel(
    items: Iterable[T],
    process_func: Callable[[T], U],
    max_workers: int | None = None,
    desc: str = "",
) -> list[U]:
    """Process items in parallel using ThreadPoolExecutor.

    Args:
        items: Items to process
        process_func: Function to process each item
        max_workers: Maximum number of worker threads
        desc: Description for progress reporting

    Returns:
        List of processed results
    """
    if max_workers is None:
        max_workers = os.cpu_count() or 1

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_func, item): item for item in items}
        for future in as_completed(futures):
            item = futures[future]
            try:
                result = future.result()
                results.append(result)
                if desc:
                    logger.debug(f"{desc}: Processed {item}")
            except Exception as e:
                logger.error(f"Failed to process {item}: {e}")
                if not isinstance(e, KeyboardInterrupt):
                    logger.debug(f"Error details: {type(e).__name__}: {e}")
                else:
                    raise
    return results


def find_python_files(
    path: Path | str,
    exclude_patterns: Sequence[str] = (".*", "*_test.py", "test_*.py"),
) -> list[Path]:
    """Find Python files in directory recursively.

    Args:
        path: Directory to search
        exclude_patterns: Glob patterns to exclude

    Returns:
        List of Python file paths
    """
    path = Path(path)
    if not path.exists():
        msg = f"Path does not exist: {path}"
        raise FileNotFoundError(msg)

    def is_excluded(p: Path) -> bool:
        return any(p.match(pattern) for pattern in exclude_patterns)

    if path.is_file():
        return [path] if path.suffix == ".py" and not is_excluded(path) else []

    result = []
    for root, _, files in os.walk(path):
        root_path = Path(root)
        if is_excluded(root_path):
            continue
        for file in files:
            file_path = root_path / file
            if file_path.suffix == ".py" and not is_excluded(file_path):
                result.append(file_path)
    return sorted(result)


def normalize_docstring(docstring: str | None) -> str | None:
    """Normalize a docstring by fixing indentation and removing redundant whitespace.

    Args:
        docstring: Raw docstring

    Returns:
        Normalized docstring or None if input is None
    """
    if not docstring:
        return None

    # Remove common leading whitespace from every line
    lines = docstring.expandtabs().splitlines()

    # Find minimum indentation (first line doesn't count)
    indent = sys.maxsize
    for line in lines[1:]:
        stripped = line.lstrip()
        if stripped:
            indent = min(indent, len(line) - len(stripped))

    # Remove indentation (but don't strip first line)
    trimmed = [lines[0].strip()]
    if indent < sys.maxsize:
        for line in lines[1:]:
            trimmed.append(line[indent:].rstrip())

    # Strip trailing empty lines
    while trimmed and not trimmed[-1]:
        trimmed.pop()

    # Return normalized docstring
    return "\n".join(trimmed)


def get_qualified_name(node: ast.AST) -> str:
    """Get qualified name from an AST node.

    Args:
        node: AST node (typically Name or Attribute)

    Returns:
        Qualified name as string
    """
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        return f"{get_qualified_name(node.value)}.{node.attr}"
    return ""


def parse_type_string(type_str: str) -> str:
    """Parse and normalize a type string.

    Args:
        type_str: Type string to parse

    Returns:
        Normalized type string
    """
    # Remove redundant spaces
    type_str = re.sub(r"\s+", " ", type_str.strip())

    # Handle union types (both new and old syntax)
    type_str = re.sub(r"Union\[(.*?)\]", r"(\1)", type_str)
    type_str = re.sub(r"\s*\|\s*", " | ", type_str)

    # Handle optional types
    type_str = re.sub(r"Optional\[(.*?)\]", r"\1 | None", type_str)

    # Normalize nested brackets
    depth = 0
    result = []
    for char in type_str:
        if char == "[":
            if depth > 0:
                result.append(" ")
            depth += 1
        elif char == "]":
            depth -= 1
        elif char == "," and depth > 0:
            result.append(", ")
            continue
        result.append(char)

    return "".join(result)


class ImportTracker:
    """Track and manage imports in a module."""

    def __init__(self) -> None:
        self.imports: dict[str, set[str]] = {}  # module -> names
        self.import_froms: dict[
            str, dict[str, str | None]
        ] = {}  # module -> {name -> alias}
        self.explicit_imports: set[str] = set()  # Explicitly requested imports

    def add_import(self, module: str, name: str | None = None) -> None:
        """Add an import."""
        if name:
            self.imports.setdefault(module, set()).add(name)
        else:
            self.imports.setdefault(module, set())
        self.explicit_imports.add(module)

    def add_import_from(
        self, module: str, names: Sequence[tuple[str, str | None]]
    ) -> None:
        """Add a from-import."""
        self.import_froms.setdefault(module, {}).update(dict(names))

    def get_import_lines(self) -> list[str]:
        """Get sorted import lines."""
        lines = []

        # Regular imports
        for module in sorted(self.imports):
            names = sorted(self.imports[module])
            if names:
                items = ", ".join(names)
                lines.append(f"from {module} import {items}")
            else:
                lines.append(f"import {module}")

        # From-imports
        for module in sorted(self.import_froms):
            name_map = self.import_froms[module]
            if name_map:
                items = ", ".join(
                    f"{name} as {alias}" if alias else name
                    for name, alias in sorted(name_map.items())
                )
                lines.append(f"from {module} import {items}")

        return lines


def setup_logging(level: str = "INFO") -> None:
    """Configure logging for the package."""
    logger.remove()  # Remove default handler
    logger.add(
        lambda msg: print(msg),
        level=level,
        format=(
            "<green>{time:HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
            "<level>{message}</level>"
        ),
    )


def read_source_file(path: str | Path) -> tuple[str, str | None]:
    """Read a Python source file and detect its encoding.

    Args:
        path: Path to the source file

    Returns:
        Tuple of (source_code, encoding)
    """
    path = Path(path)
    try:
        with path.open("rb") as f:
            source = f.read()

        # Try to detect encoding from first two lines
        encoding = "utf-8"  # default
        for line in source.split(b"\n")[:2]:
            if line.startswith(b"#") and b"coding:" in line:
                encoding = line.split(b"coding:")[-1].strip().decode("ascii")
                break

        return source.decode(encoding), encoding
    except Exception as e:
        logger.error(f"Failed to read {path}: {e}")
        return "", None


def parse_source(source: str) -> ast.AST | None:
    """Parse Python source code into an AST.

    Args:
        source: Python source code string

    Returns:
        AST if parsing successful, None otherwise
    """
    try:
        return ast.parse(source)
    except SyntaxError as e:
        logger.error(f"Failed to parse source: {e}")
        return None


def normalize_path(path: str | Path) -> Path:
    """Normalize a path to an absolute Path object.

    Args:
        path: Path-like object to normalize

    Returns:
        Normalized absolute Path
    """
    return Path(path).resolve()

================
File: src/twat_coding/pystubnik/processors/__init__.py
================
"""Processor interface and registry for stub generation.

This module provides the base classes and interfaces for different
processors that can be used during stub generation.
"""

from abc import abstractmethod
from typing import Protocol

from ..core.types import StubResult


class Processor(Protocol):
    """Protocol defining the interface for stub processors."""

    @abstractmethod
    def process(self, stub_result: StubResult) -> StubResult:
        """Process a stub generation result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result
        """
        ...


_processors: dict[str, type[Processor]] = {}


def register_processor(name: str, processor: type[Processor]) -> None:
    """Register a new processor implementation."""
    _processors[name] = processor


def get_processor(name: str) -> type[Processor]:
    """Get a registered processor by name."""
    if name not in _processors:
        msg = f"Processor '{name}' not found"
        raise KeyError(msg)
    return _processors[name]


def list_processors() -> list[str]:
    """List all registered processor names."""
    return list(_processors.keys())

================
File: src/twat_coding/pystubnik/processors/docstring.py
================
"""Docstring processing functionality."""

from dataclasses import dataclass
from typing import Literal

from docstring_parser import parse
from docstring_parser.common import DocstringStyle
from loguru import logger

from ..core.types import (
    ClassInfo,
    FunctionInfo,
    ModuleInfo,
    StubResult,
)
from ..errors import StubGenerationError
from ..types.type_system import TypeInfo
from . import Processor


class TypeInferenceError(StubGenerationError):
    """Error raised when type inference fails."""

    def __init__(self, message: str, details: dict[str, str] | None = None) -> None:
        super().__init__(message, "TYPE001", details)


@dataclass
class DocstringResult:
    """Result of docstring processing."""

    docstring: str | None
    signatures: list[FunctionInfo]
    param_types: dict[str, TypeInfo]
    return_type: TypeInfo | None
    yield_type: TypeInfo | None
    raises: list[tuple[TypeInfo, str]]  # (exception_type, description)


class DocstringProcessor(Processor):
    """Processes docstrings to extract type information and signatures."""

    def __init__(
        self,
        style: Literal["google", "numpy", "rest"] = "google",
        max_length: int | None = None,
        preserve_sections: list[str] | None = None,
    ) -> None:
        """Initialize docstring processor.

        Args:
            style: Docstring style to use for parsing
            max_length: Maximum length for docstrings before truncation
            preserve_sections: List of section names to always preserve
        """
        self.style = DocstringStyle.GOOGLE
        if style == "numpy":
            self.style = DocstringStyle.NUMPYDOC
        elif style == "rest":
            self.style = DocstringStyle.REST

        self.max_length = max_length
        self.preserve_sections = preserve_sections or [
            "Args",
            "Returns",
            "Yields",
            "Raises",
        ]

    def process(self, stub_result: StubResult) -> StubResult:
        """Process docstrings in a stub result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result
        """
        logger.debug(f"Processing docstrings for {stub_result.source_path}")

        # Process module docstring
        module_info = self._process_module_docstring(stub_result)

        # Process function docstrings
        for function in module_info.functions:
            self._process_function_docstring(function)

        # Process class docstrings
        for class_info in module_info.classes:
            self._process_class_docstring(class_info)

            # Process method docstrings
            for method in class_info.methods:
                self._process_function_docstring(method)

            # Process property docstrings
            for prop in class_info.properties:
                self._process_function_docstring(prop)

        return stub_result

    def _process_module_docstring(self, stub_result: StubResult) -> ModuleInfo:
        """Process the module-level docstring.

        Args:
            stub_result: The stub generation result

        Returns:
            The processed module info
        """
        logger.debug("Processing module docstring")
        # TODO: Implement module docstring processing
        return ModuleInfo(name="", path=stub_result.source_path)

    def _process_function_docstring(self, function: FunctionInfo) -> None:
        """Process a function or method docstring.

        Args:
            function: The function info to process
        """
        if not function.docstring:
            return

        logger.debug(f"Processing docstring for function {function.name}")

        try:
            docstring = parse(function.docstring, style=self.style)

            # Extract parameter types
            for param in docstring.params:
                if param.type_name:
                    arg = next(
                        (a for a in function.args if a.name == param.arg_name), None
                    )
                    if arg:
                        # Update arg type if found in docstring
                        object.__setattr__(arg, "type", param.type_name)

            # Extract return type
            if docstring.returns and docstring.returns.type_name:
                object.__setattr__(function, "return_type", docstring.returns.type_name)

        except Exception as e:
            logger.warning(f"Failed to parse docstring for {function.name}: {e}")

    def _process_class_docstring(self, class_info: ClassInfo) -> None:
        """Process a class docstring.

        Args:
            class_info: The class info to process
        """
        if not class_info.docstring:
            return

        logger.debug(f"Processing docstring for class {class_info.name}")

        try:
            parse(class_info.docstring, style=self.style)
            # TODO: Extract class-level type information

        except Exception as e:
            logger.warning(f"Failed to parse docstring for {class_info.name}: {e}")

================
File: src/twat_coding/pystubnik/processors/file_importance.py
================
"""Enhanced file importance analysis for Python packages.

This module provides tools to analyze and rank Python files based on multiple metrics:
- Dependency centrality (PageRank)
- Code complexity
- Test coverage
- Documentation quality
"""

import argparse
import ast
import importlib.util
import json
import os
from dataclasses import dataclass, field
from typing import Any, TypeVar, cast

import networkx as nx
import toml
from coverage import Coverage
from pydocstyle import check as pydocstyle_check
from radon.complexity import cc_visit

# Type variables for better type safety
T = TypeVar("T")


def _cast_or_default(value: Any, default: T) -> T:
    """Cast a value to the type of the default or return the default if casting fails."""
    try:
        if isinstance(value, type(default)):
            return cast(T, value)
        return default
    except Exception:
        return default


# Optional dependency error messages
COVERAGE_MISSING = "coverage package not installed. Coverage analysis will be disabled."
PYDOCSTYLE_MISSING = (
    "pydocstyle package not installed. Documentation quality analysis will be disabled."
)
RADON_MISSING = (
    "radon package not installed. Code complexity analysis will be disabled."
)


def _check_optional_dependencies() -> dict[str, bool]:
    """Check which optional dependencies are available.

    Returns:
        Dictionary mapping dependency names to their availability status
    """
    available = {
        "coverage": True,
        "pydocstyle": True,
        "radon": True,
    }

    if not importlib.util.find_spec("coverage"):
        available["coverage"] = False
        print(COVERAGE_MISSING)

    if not importlib.util.find_spec("pydocstyle"):
        available["pydocstyle"] = False
        print(PYDOCSTYLE_MISSING)

    if not importlib.util.find_spec("radon"):
        available["radon"] = False
        print(RADON_MISSING)

    return available


# Check optional dependencies at module import time
AVAILABLE_DEPS = _check_optional_dependencies()


@dataclass
class FileImportanceConfig:
    """Configuration for file importance analysis."""

    exclude_dirs: list[str] = field(default_factory=list)
    centrality: str = "pagerank"  # One of: pagerank, betweenness, eigenvector
    coverage_data: str | None = None
    weights: dict[str, float] = field(
        default_factory=lambda: {
            "centrality": 0.4,
            "complexity": 0.3,
            "coverage": 0.15,
            "doc_quality": 0.15,
        }
    )


def find_py_files(package_dir: str, exclude_dirs: list[str] | None = None) -> list[str]:
    """Find all .py files in the package directory, excluding specified directories.

    Args:
        package_dir: Root directory to search
        exclude_dirs: List of directory names to exclude

    Returns:
        List of Python file paths
    """
    py_files = []
    exclude_dirs = exclude_dirs or []
    for root, _, files in os.walk(package_dir):
        if any(exclude in root for exclude in exclude_dirs):
            continue
        for file in files:
            if file.endswith(".py"):
                py_files.append(os.path.join(root, file))
    return py_files


def is_entry_point(file_path: str) -> bool:
    """Check if the file contains if __name__ == '__main__'."""
    try:
        with open(file_path, encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=file_path)
        for node in ast.walk(tree):
            if (
                isinstance(node, ast.If)
                and isinstance(node.test, ast.Compare)
                and isinstance(node.test.left, ast.Name)
                and node.test.left.id == "__name__"
                and len(node.test.ops) == 1
                and isinstance(node.test.ops[0], ast.Eq)
                and len(node.test.comparators) == 1
                and isinstance(node.test.comparators[0], ast.Constant)
                and node.test.comparators[0].value == "__main__"
            ):
                return True
    except Exception:
        return False
    return False


def get_additional_entry_points(package_dir: str) -> list[str]:
    """Parse pyproject.toml for additional entry points like console scripts."""
    entry_points = []
    pyproject_path = os.path.join(package_dir, "pyproject.toml")
    if os.path.exists(pyproject_path):
        try:
            with open(pyproject_path) as f:
                data = toml.load(f)
                scripts = data.get("project", {}).get("scripts", {})
                for script in scripts.values():
                    if os.path.exists(script) and script.endswith(".py"):
                        entry_points.append(script)
        except Exception:
            pass
    return entry_points


def _parse_import_node(
    source_file: str, node: ast.Import | ast.ImportFrom, py_files: list[str]
) -> list[tuple[str, str]]:
    """Parse a single import node and return edges for the import graph.

    Args:
        source_file: Path to the source file containing the import
        node: The AST import node to parse
        py_files: List of all Python files in the package

    Returns:
        List of (source, target) tuples representing import edges
    """
    edges = []
    if isinstance(node, ast.Import):
        for name in node.names:
            imported = name.name.split(".")[0]
            for target in py_files:
                if os.path.basename(target) == f"{imported}.py":
                    edges.append((source_file, target))
    elif isinstance(node, ast.ImportFrom) and node.module:
        imported = node.module.split(".")[0]
        for target in py_files:
            if os.path.basename(target) == f"{imported}.py":
                edges.append((source_file, target))
    return edges


def _parse_imports_from_file(file: str, py_files: list[str]) -> list[tuple[str, str]]:
    """Parse imports from a Python file and return edges for the import graph.

    Args:
        file: Path to the Python file to parse
        py_files: List of all Python files in the package

    Returns:
        List of (source, target) tuples representing import edges
    """
    edges = []
    try:
        with open(file, encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=file)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import | ast.ImportFrom):
                edges.extend(_parse_import_node(file, node, py_files))
    except Exception:
        pass
    return edges


def build_import_graph(package_dir: str, py_files: list[str]) -> nx.DiGraph:
    """Build the import graph using importlab and convert to networkx.

    Args:
        package_dir: Root directory of the Python package
        py_files: List of Python files to analyze

    Returns:
        A networkx DiGraph representing the import dependencies
    """
    # Create networkx graph
    G = nx.DiGraph()
    for file in py_files:
        G.add_node(file)

    # Parse imports from each file and add edges
    for file in py_files:
        edges = _parse_imports_from_file(file, py_files)
        G.add_edges_from(edges)

    return G


def calculate_complexity(file_path: str) -> float:
    """Calculate average cyclomatic complexity using radon."""
    if not AVAILABLE_DEPS["radon"]:
        return 0.0

    try:
        with open(file_path, encoding="utf-8") as f:
            code = f.read()
        complexities = cc_visit(code)
        if not complexities:
            return 0.0
        total_complexity = sum(
            _cast_or_default(c.complexity, 0.0) for c in complexities
        )
        return total_complexity / len(complexities)
    except Exception as e:
        print(f"Warning: Failed to calculate complexity for {file_path}: {e}")
        return 0.0


def calculate_coverage(file_path: str, coverage_data: str | None) -> float:
    """Get test coverage percentage for the file.

    Args:
        file_path: Path to the Python file
        coverage_data: Path to coverage data file

    Returns:
        Coverage percentage between 0 and 100
    """
    if not AVAILABLE_DEPS["coverage"]:
        return 0.0

    if not coverage_data or not os.path.exists(coverage_data):
        return 0.0
    try:
        cov = Coverage(data_file=coverage_data)
        cov.load()
        analysis = cov._analyze(file_path)
        total_lines = _cast_or_default(analysis.numbers.n_statements, 0)
        covered_lines = _cast_or_default(analysis.numbers.n_executed, 0)
        return (covered_lines / total_lines * 100) if total_lines > 0 else 0.0
    except Exception as e:
        print(f"Warning: Failed to calculate coverage for {file_path}: {e}")
        return 0.0


def calculate_doc_quality(file_path: str) -> float:
    """Assess documentation quality using pydocstyle."""
    if not AVAILABLE_DEPS["pydocstyle"]:
        return 0.0

    try:
        violations = list(pydocstyle_check([file_path], ignore=["D100", "D101"]))
        return max(0.0, 1.0 - (len(violations) / 10))
    except Exception as e:
        print(f"Warning: Failed to assess documentation quality for {file_path}: {e}")
        return 0.0


def _calculate_centrality(
    G: nx.DiGraph,
    py_files: list[str],
    centrality_measure: str,
    entry_points: dict[str, bool],
) -> dict[str, float]:
    """Calculate centrality scores for files in the import graph.

    Args:
        G: The import dependency graph
        py_files: List of Python files
        centrality_measure: Type of centrality to calculate
        entry_points: Dict mapping files to their entry point status

    Returns:
        Dictionary mapping files to their centrality scores
    """
    personalization_dict = {
        file: 1.0 if entry_points[file] else 0.0 for file in py_files
    }

    if centrality_measure == "pagerank":
        # If all values are 0, use uniform personalization
        if sum(personalization_dict.values()) == 0:
            personalization_dict = {file: 1.0 for file in py_files}
        result = nx.pagerank(G, personalization=personalization_dict)
        return {k: float(v) for k, v in result.items()}
    elif centrality_measure == "betweenness":
        result = nx.betweenness_centrality(G)
        return {k: float(v) for k, v in result.items()}
    elif centrality_measure == "eigenvector":
        try:
            result = nx.eigenvector_centrality(G, max_iter=500)
            return {k: float(v) for k, v in result.items()}
        except nx.PowerIterationFailedConvergence:
            return {file: 0.0 for file in py_files}
    else:
        print(
            f"Warning: Unsupported centrality measure '{centrality_measure}'. "
            "Using PageRank."
        )
        result = nx.pagerank(G)
        return {k: float(v) for k, v in result.items()}


def _print_results(
    sorted_files: list[tuple[str, float]],
    package_dir: str,
    entry_points: dict[str, bool],
    centrality: dict[str, float],
    complexity_scores: dict[str, float],
    coverage_scores: dict[str, float],
    doc_scores: dict[str, float],
) -> None:
    """Print analysis results in a formatted table.

    Args:
        sorted_files: List of (file, score) tuples sorted by score
        package_dir: Root directory of the package
        entry_points: Dict mapping files to their entry point status
        centrality: Dict mapping files to their centrality scores
        complexity_scores: Dict mapping files to their complexity scores
        coverage_scores: Dict mapping files to their coverage scores
        doc_scores: Dict mapping files to their documentation quality scores
    """
    header = (
        f"{'File Path':<50} {'Composite':<10} {'Centrality':<10} "
        f"{'Complexity':<10} {'Coverage':<10} {'Doc':<10} {'Entry':<6} {'Init'}"
    )
    print(header)
    print("-" * len(header))

    for file, score in sorted_files:
        is_entry = "Yes" if entry_points.get(file, False) else "No"
        is_init = "Yes" if os.path.basename(file) == "__init__.py" else "No"
        rel_path = os.path.relpath(file, package_dir)

        # Split long line into multiple lines for readability
        print(
            f"{rel_path:<50} {score:<10.3f} "
            f"{centrality.get(file, 0):<10.3f} "
            f"{complexity_scores.get(file, 0):<10.1f} "
            f"{coverage_scores.get(file, 0):<10.1f} "
            f"{doc_scores.get(file, 0):<10.2f} "
            f"{is_entry:<6} {is_init}"
        )


def prioritize_files(
    package_dir: str, config: FileImportanceConfig
) -> dict[str, float]:
    """Analyze and prioritize .py files in the package based on multiple metrics.

    Args:
        package_dir: Root directory of the Python package
        config: Configuration for importance analysis

    Returns:
        Dictionary mapping file paths to their composite importance scores
    """
    # Step 1: Collect .py files
    py_files = find_py_files(package_dir, config.exclude_dirs)
    if not py_files:
        print("No Python files found in the package directory.")
        return {}

    # Step 2: Build import graph and identify entry points
    G = build_import_graph(package_dir, py_files)
    entry_points = {file: is_entry_point(file) for file in py_files}
    additional_eps = get_additional_entry_points(package_dir)
    for ep in additional_eps:
        if ep in py_files:
            entry_points[ep] = True

    # Step 3: Calculate metrics
    centrality = _calculate_centrality(G, py_files, config.centrality, entry_points)
    complexity_scores = {file: calculate_complexity(file) for file in py_files}
    coverage_scores = {
        file: calculate_coverage(file, config.coverage_data) for file in py_files
    }
    doc_scores = {file: calculate_doc_quality(file) for file in py_files}

    # Step 4: Normalize complexity
    max_complexity = max(complexity_scores.values(), default=1.0)
    complexity_normalized = {
        file: score / max_complexity for file, score in complexity_scores.items()
    }

    # Step 5: Compute composite scores
    weights = config.weights
    composite_scores: dict[str, float] = {}
    for file in py_files:
        score = (
            weights["centrality"] * centrality.get(file, 0.0)
            + weights["complexity"] * complexity_normalized.get(file, 0.0)
            + weights["coverage"] * (coverage_scores.get(file, 0.0) / 100)
            + weights["doc_quality"] * doc_scores.get(file, 0.0)
        )
        composite_scores[file] = score

    # Step 6: Sort and display results
    sorted_files = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)
    _print_results(
        sorted_files,
        package_dir,
        entry_points,
        centrality,
        complexity_scores,
        coverage_scores,
        doc_scores,
    )

    return composite_scores


def load_config(config_file: str | None) -> dict[str, Any]:
    """Load configuration from a JSON file."""
    if config_file and os.path.exists(config_file):
        try:
            with open(config_file) as f:
                config = json.load(f)
                return cast(dict[str, Any], config)
        except Exception as e:
            print(
                "Warning: Failed to load config file "
                f"'{config_file}': {e}. Using defaults."
            )
    return {}


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "Prioritize .py files in a Python package based on multiple metrics."
        )
    )
    parser.add_argument("package_dir", help="Path to the package directory")
    parser.add_argument("--config", help="Path to configuration JSON file")
    parser.add_argument("--coverage-data", help="Path to coverage.py data file")
    parser.add_argument("--exclude-dirs", nargs="+", help="Directories to exclude")
    parser.add_argument(
        "--centrality",
        choices=["pagerank", "betweenness", "eigenvector"],
        default="pagerank",
        help="Centrality measure to use",
    )
    args = parser.parse_args()

    config = FileImportanceConfig()
    user_config = load_config(args.config)

    if user_config:
        config.weights.update(user_config.get("weights", {}))
        config.exclude_dirs.extend(user_config.get("exclude_dirs", []))
        if "centrality" in user_config:
            config.centrality = user_config["centrality"]

    if args.coverage_data:
        config.coverage_data = args.coverage_data
    if args.exclude_dirs:
        config.exclude_dirs.extend(args.exclude_dirs)
    if args.centrality:
        config.centrality = args.centrality

    prioritize_files(args.package_dir, config)

================
File: src/twat_coding/pystubnik/processors/importance.py
================
#!/usr/bin/env -S uv run
"""Importance scoring for symbols and code elements."""

import ast
import re
from dataclasses import dataclass, field
from typing import Any

from loguru import logger

from ..core.types import StubResult
from . import Processor
from .file_importance import FileImportanceConfig, prioritize_files


@dataclass
class ImportanceConfig:
    """Configuration for importance scoring."""

    patterns: dict[str, float] = field(default_factory=dict)
    keywords: set[str] = field(
        default_factory=lambda: {
            "important",
            "critical",
            "essential",
            "main",
            "key",
        }
    )
    min_score: float = 0.5
    docstring_weight: float = 0.01  # Per word in docstring
    public_weight: float = 1.1
    special_weight: float = 1.2

    # File-level importance settings
    file_importance: FileImportanceConfig = field(default_factory=FileImportanceConfig)


class ImportanceProcessor(Processor):
    """Calculate importance scores for code elements."""

    def __init__(self, config: ImportanceConfig | None = None) -> None:
        """Initialize the importance processor.

        Args:
            config: Configuration for importance scoring
        """
        self.config = config or ImportanceConfig()
        self._file_scores: dict[str, float] = {}

    def process(self, stub_result: StubResult) -> StubResult:
        """Process a stub result to calculate importance scores.

        This method combines both symbol-level and file-level importance scoring:
        1. First, it calculates file-level importance if not already done
        2. Then, it adjusts symbol scores based on their containing file's importance
        3. Finally, it applies symbol-specific scoring rules

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result with importance scores
        """
        # Calculate file-level importance if needed
        if not self._file_scores:
            try:
                package_dir = str(stub_result.source_path.parent)
                self._file_scores = prioritize_files(
                    package_dir, self.config.file_importance
                )
            except Exception as e:
                logger.warning(f"Failed to calculate file importance: {e}")

        # Get file importance score
        file_score = self._file_scores.get(str(stub_result.source_path), 0.5)

        # Adjust stub result's importance score
        stub_result.importance_score = file_score
        stub_result.metadata["file_score"] = file_score

        # Process individual symbols
        try:
            tree = ast.parse(stub_result.stub_content)
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef | ast.ClassDef):
                    docstring = ast.get_docstring(node)
                    score = self.calculate_importance(
                        name=node.name,
                        docstring=docstring,
                        is_public=not node.name.startswith("_"),
                        is_special=node.name.startswith("__")
                        and node.name.endswith("__"),
                        extra_info={"file_path": stub_result.source_path},
                    )
                    stub_result.metadata[f"{node.name}_score"] = score

                    # For low importance functions, replace body with ellipsis
                    if score < 0.7 and isinstance(node, ast.FunctionDef):
                        node.body = [ast.Expr(value=ast.Constant(value=Ellipsis))]

            # Update stub content with modified AST
            stub_result.stub_content = ast.unparse(tree)
        except Exception as e:
            logger.warning(
                f"Error processing symbols in {stub_result.source_path}: {e}"
            )

        return stub_result

    def _get_file_score(self, extra_info: dict[str, Any] | None) -> float:
        """Get the file-level base score."""
        if extra_info and "file_path" in extra_info:
            file_path = extra_info["file_path"]
            return self._file_scores.get(str(file_path), 1.0)
        return 1.0

    def _calculate_pattern_score(self, name: str) -> float:
        """Calculate score based on importance patterns."""
        score = 1.0
        for pattern, weight in self.config.patterns.items():
            if re.search(pattern, name):
                score *= weight
        return score

    def _calculate_docstring_score(self, docstring: str | None) -> float:
        """Calculate score based on docstring quality."""
        if not docstring:
            return 1.0

        score = 1.0
        word_count = len(docstring.split())
        score += word_count * self.config.docstring_weight

        # Check for importance keywords
        for keyword in self.config.keywords:
            if keyword.lower() in docstring.lower():
                score *= 1.1  # Small boost for each importance keyword

        return score

    def _calculate_visibility_score(self, is_public: bool, is_special: bool) -> float:
        """Calculate score based on symbol visibility."""
        score = 1.0
        if is_public:
            score *= self.config.public_weight
        if is_special:
            score *= self.config.special_weight
        return score

    def calculate_importance(
        self,
        name: str,
        docstring: str | None = None,
        is_public: bool = True,
        is_special: bool = False,
        extra_info: dict[str, Any] | None = None,
    ) -> float:
        """Calculate importance score for a symbol.

        Args:
            name: Symbol name
            docstring: Associated docstring
            is_public: Whether the symbol is public
            is_special: Whether it's a special method
            extra_info: Additional information for scoring

        Returns:
            Importance score between 0 and 1
        """
        try:
            # Calculate component scores
            file_score = self._get_file_score(extra_info)
            pattern_score = self._calculate_pattern_score(name)
            docstring_score = self._calculate_docstring_score(docstring)
            visibility_score = self._calculate_visibility_score(is_public, is_special)

            # Combine scores
            final_score = (
                file_score * pattern_score * docstring_score * visibility_score
            )

            # Ensure score is between 0 and 1
            return max(min(final_score, 1.0), 0.0)

        except Exception as e:
            logger.warning(f"Error calculating importance for {name}: {e}")
            return self.config.min_score

    def should_include(
        self,
        name: str,
        score: float | None = None,
        docstring: str | None = None,
        is_public: bool = True,
        is_special: bool = False,
        extra_info: dict[str, Any] | None = None,
    ) -> bool:
        """Determine if a symbol should be included based on importance.

        Args:
            name: Symbol name
            score: Pre-calculated importance score
            docstring: Associated docstring
            is_public: Whether the symbol is public
            is_special: Whether it's a special method
            extra_info: Additional information for scoring

        Returns:
            True if the symbol should be included
        """
        if score is None:
            score = self.calculate_importance(
                name,
                docstring,
                is_public,
                is_special,
                extra_info,
            )
        return score >= self.config.min_score

================
File: src/twat_coding/pystubnik/processors/imports.py
================
"""Import processor for stub generation."""

import ast
from pathlib import Path

from ..core.types import ImportInfo, ImportType, StubResult
from . import Processor


class ImportProcessor(Processor):
    """Processor for analyzing and organizing imports in stub generation."""

    def __init__(self) -> None:
        self.stdlib_modules: set[str] = self._get_stdlib_modules()

    def process(self, stub_result: StubResult) -> StubResult:
        """Process imports in the stub result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result with organized imports
        """
        tree = ast.parse(stub_result.stub_content)
        imports = self._analyze_imports(tree)

        # Group imports by type
        grouped_imports = self._group_imports(imports)

        # Generate new import section
        new_imports = self._format_imports(grouped_imports)

        # Replace imports in stub content
        stub_result.stub_content = self._replace_imports(tree, new_imports)
        stub_result.imports = list(imports.values())

        return stub_result

    def _get_stdlib_modules(self) -> set[str]:
        """Get a set of standard library module names."""
        import sysconfig

        stdlib_path = sysconfig.get_path("stdlib")
        if not stdlib_path:
            return set()

        stdlib_modules = set()
        for path in Path(stdlib_path).glob("**/*.py"):
            module_name = path.stem
            if module_name != "__init__":
                stdlib_modules.add(module_name)

        return stdlib_modules

    def _analyze_imports(self, tree: ast.AST) -> dict[str, ImportInfo]:
        """Analyze imports in an AST.

        Args:
            tree: The AST to analyze

        Returns:
            Dictionary mapping module names to ImportInfo
        """
        imports: dict[str, ImportInfo] = {}

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for name in node.names:
                    module_name = name.name
                    import_type = self._get_import_type(module_name)
                    imports[module_name] = ImportInfo(
                        module_name=module_name,
                        import_type=import_type,
                        imported_names=[name.asname or name.name],
                    )
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                import_type = self._get_import_type(module)
                if module not in imports:
                    imports[module] = ImportInfo(
                        module_name=module,
                        import_type=import_type,
                        imported_names=[],
                        is_from_import=True,
                        relative_level=node.level,
                    )
                for name in node.names:
                    imports[module].imported_names.append(name.asname or name.name)

        return imports

    def _get_import_type(self, module_name: str) -> ImportType:
        """Determine the type of an import.

        Args:
            module_name: Name of the module

        Returns:
            The ImportType of the module
        """
        if not module_name:
            return ImportType.RELATIVE

        base_module = module_name.split(".")[0]
        if base_module in self.stdlib_modules:
            return ImportType.STANDARD_LIB
        elif "." in module_name:
            return ImportType.LOCAL
        else:
            return ImportType.THIRD_PARTY

    def _group_imports(
        self, imports: dict[str, ImportInfo]
    ) -> dict[ImportType, list[ImportInfo]]:
        """Group imports by their type.

        Args:
            imports: Dictionary of imports to group

        Returns:
            Dictionary mapping import types to lists of imports
        """
        grouped: dict[ImportType, list[ImportInfo]] = {
            ImportType.STANDARD_LIB: [],
            ImportType.THIRD_PARTY: [],
            ImportType.LOCAL: [],
            ImportType.RELATIVE: [],
        }

        for import_info in imports.values():
            grouped[import_info.import_type].append(import_info)

        # Sort each group
        for group in grouped.values():
            group.sort(key=lambda x: x.module_name)

        return grouped

    def _format_imports(
        self, grouped_imports: dict[ImportType, list[ImportInfo]]
    ) -> str:
        """Format grouped imports into a string.

        Args:
            grouped_imports: Dictionary of grouped imports

        Returns:
            Formatted import section as a string
        """
        sections = []

        for import_type in ImportType:
            imports = grouped_imports[import_type]
            if not imports:
                continue

            section = []
            for import_info in imports:
                if import_info.is_from_import:
                    relative = "." * import_info.relative_level
                    names = ", ".join(sorted(import_info.imported_names))
                    section.append(
                        f"from {relative}{import_info.module_name} import {names}"
                    )
                else:
                    section.append(f"import {import_info.module_name}")

            if section:
                sections.append("\n".join(sorted(section)))

        return "\n\n".join(sections) + "\n\n"

    def _replace_imports(self, tree: ast.AST, new_imports: str) -> str:
        """Replace imports in the AST with new formatted imports.

        Args:
            tree: The AST to modify
            new_imports: The new import section

        Returns:
            Modified source code as a string
        """
        # Find the last import node
        last_import = None
        for node in ast.walk(tree):
            if isinstance(node, ast.Import | ast.ImportFrom):
                last_import = node

        if last_import:
            # Get the source up to the first import and after the last import
            source_lines = ast.unparse(tree).split("\n")
            first_import_line = min(
                node.lineno
                for node in ast.walk(tree)
                if isinstance(node, ast.Import | ast.ImportFrom)
            )
            last_import_line = max(
                node.end_lineno or node.lineno
                for node in ast.walk(tree)
                if isinstance(node, ast.Import | ast.ImportFrom)
            )

            before = "\n".join(source_lines[: first_import_line - 1])
            after = "\n".join(source_lines[last_import_line:])

            return f"{before}\n{new_imports}{after}"

        return ast.unparse(tree)

================
File: src/twat_coding/pystubnik/processors/stub_generation.py
================
#!/usr/bin/env -S uv run
"""Stub generation processor for creating type stub files."""

import ast
from dataclasses import dataclass, field
from pathlib import Path
from typing import ClassVar, cast

from ..errors import ErrorCode, StubGenerationError
from ..types.type_system import TypeRegistry
from ..utils.ast_utils import TruncationConfig, attach_parents


@dataclass
class StubConfig:
    """Configuration for stub generation."""

    # Output settings
    line_length: int = 88
    sort_imports: bool = True
    add_header: bool = True
    header_template: str = (
        "# Generated by pystubnik\n# Do not edit this file directly\n\n"
    )

    # Content settings
    include_docstrings: bool = True
    include_private: bool = False
    include_type_comments: bool = True
    infer_property_types: bool = True
    export_less: bool = False

    # Truncation settings
    truncation: TruncationConfig = field(default_factory=TruncationConfig)


class StubGenerator:
    """Generate type stubs from Python source code."""

    # Common import patterns to preserve
    ESSENTIAL_IMPORTS: ClassVar[set[str]] = {
        "typing",
        "dataclasses",
        "enum",
        "abc",
        "contextlib",
        "pathlib",
        "collections.abc",
    }

    def __init__(
        self,
        config: StubConfig | None = None,
        type_registry: TypeRegistry | None = None,
    ) -> None:
        """Initialize the stub generator.

        Args:
            config: Configuration for stub generation
            type_registry: Registry for type resolution
        """
        self.config = config or StubConfig()
        self.type_registry = type_registry or TypeRegistry()

    def generate_stub(self, source_path: Path, tree: ast.AST | None = None) -> str:
        """Generate a type stub for a Python source file.

        Args:
            source_path: Path to the source file
            tree: Optional pre-parsed AST

        Returns:
            Generated stub content

        Raises:
            StubGenerationError: If stub generation fails
        """
        try:
            # Parse source if not provided
            if tree is None:
                with source_path.open() as f:
                    tree = ast.parse(f.read(), filename=str(source_path))

            # Attach parent references for better context
            attach_parents(tree)

            # Process the AST
            if not isinstance(tree, ast.Module):
                raise StubGenerationError(
                    "Expected Module AST node",
                    ErrorCode.AST_PARSE_ERROR,
                    source=str(source_path),
                )
            processed = self._process_module(tree)

            # Generate stub content
            return self._generate_content(processed)

        except Exception as e:
            raise StubGenerationError(
                f"Failed to generate stub for {source_path}: {e}",
                ErrorCode.AST_TRANSFORM_ERROR,
                source=str(source_path),
            ) from e

    def _process_module(self, node: ast.Module) -> ast.Module:
        """Process a module AST for stub generation.

        Args:
            node: Module AST to process

        Returns:
            Processed module AST
        """
        # Create a new module for the stub
        stub = ast.Module(body=[], type_ignores=[])

        # Process imports and definitions
        imports = self._collect_imports(node)
        definitions = self._collect_definitions(node)

        # Build the stub body
        stub.body = self._build_stub_body(imports, definitions)

        return stub

    def _collect_imports(self, node: ast.Module) -> list[ast.Import | ast.ImportFrom]:
        """Collect and process import statements.

        Args:
            node: Module AST to process

        Returns:
            List of processed import statements
        """
        imports = [
            child
            for child in node.body
            if isinstance(child, ast.Import | ast.ImportFrom)
            and self._should_keep_import(child)
        ]

        if self.config.sort_imports and imports:
            # Sort imports by module name
            imports_with_keys = [
                (self._import_sort_key(cast(ast.Import | ast.ImportFrom, imp)), imp)
                for imp in imports
            ]
            imports_with_keys.sort(key=lambda x: x[0])
            imports = [imp for _, imp in imports_with_keys]

        return imports

    def _collect_definitions(self, node: ast.Module) -> list[ast.stmt]:
        """Collect and process module definitions.

        Args:
            node: Module AST to process

        Returns:
            List of processed definitions
        """
        definitions = []
        for child in node.body:
            if not isinstance(child, ast.Import | ast.ImportFrom):
                if processed := self._process_node(child):
                    if isinstance(processed, ast.stmt):
                        definitions.append(processed)
        return definitions

    def _build_stub_body(
        self,
        imports: list[ast.Import | ast.ImportFrom],
        definitions: list[ast.stmt],
    ) -> list[ast.stmt]:
        """Build the final stub body from components.

        Args:
            imports: List of processed import statements
            definitions: List of processed definitions

        Returns:
            Complete list of statements for the stub body
        """
        body: list[ast.stmt] = []

        # Add header if configured
        if self.config.add_header:
            header = ast.Expr(value=ast.Constant(value=self.config.header_template))
            body.append(header)

        # Add imports
        body.extend(imports)

        # Add separator if needed
        if imports and definitions:
            body.append(ast.Expr(value=ast.Constant(value="")))

        # Add definitions
        body.extend(definitions)

        return body

    def _process_node(self, node: ast.AST) -> ast.stmt | None:
        """Process a single AST node for stub generation.

        Args:
            node: AST node to process

        Returns:
            Processed node or None if it should be excluded
        """
        match node:
            case ast.ClassDef():
                return self._process_class(node)
            case ast.FunctionDef() | ast.AsyncFunctionDef():
                return self._process_function(node)
            case ast.AnnAssign() | ast.Assign():
                return self._process_assignment(node)
            case _:
                return None

    def _process_class(self, node: ast.ClassDef) -> ast.ClassDef | None:
        """Process a class definition for stub generation.

        Args:
            node: Class definition to process

        Returns:
            Processed class definition or None if it should be excluded
        """
        if not self.config.include_private and node.name.startswith("_"):
            return None

        # Create new class with same name and bases
        stub_class = ast.ClassDef(
            name=node.name,
            bases=node.bases,
            keywords=node.keywords,
            body=[],
            decorator_list=node.decorator_list,
            type_params=[],  # For Python 3.12+
        )

        # Preserve docstring if configured
        if self.config.include_docstrings:
            docstring = ast.get_docstring(node)
            if docstring:
                stub_class.body.append(ast.Expr(value=ast.Constant(value=docstring)))

        # Process class body
        for child in node.body:
            if processed := self._process_node(child):
                stub_class.body.append(processed)

        return stub_class

    def _process_function(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> ast.FunctionDef | ast.AsyncFunctionDef | None:
        """Process a function definition for stub generation.

        Args:
            node: Function definition to process

        Returns:
            Processed function definition or None if it should be excluded
        """
        if not self.config.include_private and node.name.startswith("_"):
            return None

        # Create new function with same signature
        stub_func = type(node)(
            name=node.name,
            args=node.args,
            returns=node.returns,
            type_params=[],  # For Python 3.12+
            body=[ast.Expr(value=ast.Constant(value=...))],  # Use ellipsis for body
            decorator_list=node.decorator_list,
        )

        # Preserve docstring if configured
        if self.config.include_docstrings:
            docstring = ast.get_docstring(node)
            if docstring:
                stub_func.body.insert(0, ast.Expr(value=ast.Constant(value=docstring)))

        return stub_func

    def _process_assignment(self, node: ast.AnnAssign | ast.Assign) -> ast.stmt | None:
        """Process an assignment for stub generation.

        Args:
            node: Assignment to process

        Returns:
            Processed assignment or None if it should be excluded
        """
        match node:
            case ast.AnnAssign():
                # Preserve annotated assignments
                return node
            case ast.Assign(
                targets=[ast.Name() as name_node], value=ast.Constant() as value
            ):
                # Only keep module-level assignments of constants
                if not name_node.id.startswith("_") or name_node.id.isupper():
                    return ast.AnnAssign(
                        target=name_node,
                        annotation=ast.Name(id=type(value.value).__name__),
                        value=value,
                        simple=1,
                    )
        return None

    def _should_keep_import(self, node: ast.Import | ast.ImportFrom) -> bool:
        """Check if an import should be kept in the stub.

        Args:
            node: Import node to check

        Returns:
            True if the import should be kept
        """
        if isinstance(node, ast.ImportFrom):
            return node.module in self.ESSENTIAL_IMPORTS or any(
                name.name.isupper() for name in node.names
            )
        return any(
            name.name in self.ESSENTIAL_IMPORTS
            or name.name.split(".")[0] in self.ESSENTIAL_IMPORTS
            for name in node.names
        )

    def _import_sort_key(self, node: ast.Import | ast.ImportFrom) -> tuple[int, str]:
        """Get sort key for import statements.

        Args:
            node: Import node to sort

        Returns:
            Tuple of (import type, module name) for sorting
        """
        if isinstance(node, ast.ImportFrom):
            return (1, node.module or "")
        return (0, node.names[0].name)

    def _generate_content(self, node: ast.Module) -> str:
        """Generate stub content from processed AST.

        Args:
            node: Processed AST

        Returns:
            Generated stub content
        """
        return ast.unparse(node)

================
File: src/twat_coding/pystubnik/processors/type_inference.py
================
#!/usr/bin/env -S uv run
"""Type inference processor for stub generation."""

import ast
import re
from typing import Any, ClassVar

from loguru import logger

from ..types.type_system import TypeInferenceError, TypeInfo, TypeRegistry


class TypeInferenceProcessor:
    """Processor for inferring types from code analysis."""

    # Common type patterns in variable names
    TYPE_PATTERNS: ClassVar[dict[re.Pattern[str], type]] = {
        re.compile(r"_str$|_string$"): str,
        re.compile(r"_int$|_count$|_index$"): int,
        re.compile(r"_float$|_ratio$|_rate$"): float,
        re.compile(r"_bool$|_flag$|is_|has_|can_"): bool,
        re.compile(r"_list$|_array$"): list,
        re.compile(r"_dict$|_map$"): dict,
        re.compile(r"_set$"): set,
        re.compile(r"_tuple$"): tuple,
    }

    def __init__(
        self,
        type_registry: TypeRegistry | None = None,
        confidence_threshold: float = 0.5,
        infer_from_usage: bool = True,
        infer_from_assignments: bool = True,
        infer_from_returns: bool = True,
        infer_from_defaults: bool = True,
    ) -> None:
        """Initialize the type inference processor.

        Args:
            type_registry: Registry for type resolution
            confidence_threshold: Minimum confidence for inferred types
            infer_from_usage: Whether to infer types from usage patterns
            infer_from_assignments: Whether to infer types from assignments
            infer_from_returns: Whether to infer types from return statements
            infer_from_defaults: Whether to infer types from default values
        """
        self.type_registry = type_registry or TypeRegistry()
        self.confidence_threshold = confidence_threshold
        self.infer_from_usage = infer_from_usage
        self.infer_from_assignments = infer_from_assignments
        self.infer_from_returns = infer_from_returns
        self.infer_from_defaults = infer_from_defaults

    def infer_types(self, node: ast.AST) -> dict[str, TypeInfo]:
        """Infer types for variables and expressions in an AST.

        Args:
            node: AST node to analyze

        Returns:
            Dictionary mapping names to inferred type information

        Raises:
            TypeInferenceError: If type inference fails
        """
        try:
            inferred_types: dict[str, TypeInfo] = {}

            # Collect type information from different sources
            if self.infer_from_assignments:
                self._infer_from_assignments(node, inferred_types)
            if self.infer_from_usage:
                self._infer_from_usage(node, inferred_types)
            if self.infer_from_returns:
                self._infer_from_returns(node, inferred_types)

            # Filter by confidence threshold
            return {
                name: type_info
                for name, type_info in inferred_types.items()
                if type_info.confidence >= self.confidence_threshold
            }

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to infer types: {e}",
                details={"node_type": type(node).__name__},
            ) from e

    def _infer_from_assignments(
        self, node: ast.AST, types: dict[str, TypeInfo]
    ) -> None:
        """Infer types from assignment statements.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        for child in ast.walk(node):
            match child:
                case ast.AnnAssign(target=ast.Name(id=name), annotation=annotation):
                    # Handle explicitly annotated assignments
                    try:
                        type_info = self.type_registry.resolve_type(
                            annotation, f"annotation:{name}"
                        )
                        types[name] = type_info
                    except Exception as e:
                        logger.warning(
                            f"Failed to resolve type annotation for {name}: {e}"
                        )

                case ast.Assign(targets=[ast.Name(id=name)], value=value):
                    # Infer from assigned value
                    if self.infer_from_defaults and isinstance(value, ast.Constant):
                        type_info = TypeInfo(
                            annotation=type(value.value),
                            source="default",
                            confidence=0.7,
                            metadata={"value": value.value},
                        )
                        types[name] = type_info

                    # Infer from variable name patterns
                    for pattern, typ in self.TYPE_PATTERNS.items():
                        if pattern.search(name):
                            types[name] = TypeInfo(
                                annotation=typ,
                                source="pattern",
                                confidence=0.6,
                                metadata={"pattern": pattern.pattern},
                            )

    def _infer_from_usage(self, node: ast.AST, types: dict[str, TypeInfo]) -> None:
        """Infer types from usage patterns.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        # Track attribute access
        for child in ast.walk(node):
            if isinstance(child, ast.Attribute):
                if isinstance(child.value, ast.Name):
                    name = child.value.id
                    # Record attribute access for potential type inference
                    if name not in types:
                        types[name] = TypeInfo(
                            annotation=Any,
                            source="usage",
                            confidence=0.3,
                            metadata={"attributes": {child.attr}},
                        )
                    else:
                        attrs = types[name].metadata.get("attributes", set())
                        attrs.add(child.attr)
                        types[name].metadata["attributes"] = attrs

    def _infer_from_returns(self, node: ast.AST, types: dict[str, TypeInfo]) -> None:
        """Infer return types from return statements.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        for child in ast.walk(node):
            if isinstance(child, ast.FunctionDef | ast.AsyncFunctionDef):
                return_types = set()
                for return_node in ast.walk(child):
                    if isinstance(return_node, ast.Return) and return_node.value:
                        if isinstance(return_node.value, ast.Constant):
                            return_types.add(type(return_node.value.value))
                        elif isinstance(return_node.value, ast.Name):
                            name = return_node.value.id
                            if name in types:
                                return_types.add(types[name].annotation)

                if return_types:
                    types[child.name] = TypeInfo(
                        annotation=next(iter(return_types))
                        if len(return_types) == 1
                        else Any,
                        source="returns",
                        confidence=0.5,
                        metadata={"return_types": list(return_types)},
                    )

================
File: src/twat_coding/pystubnik/types/docstring.py
================
#!/usr/bin/env -S uv run
"""Docstring type extraction and processing."""

import ast
import re
from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Any, ClassVar

from docstring_parser import parse as parse_docstring
from loguru import logger

from ..core.types import ArgInfo
from ..types.type_system import TypeInferenceError, TypeInfo, TypeRegistry


class DocstringStyle(Enum):
    """Docstring style."""

    GOOGLE = auto()
    NUMPY = auto()
    SPHINX = auto()
    EPYTEXT = auto()
    UNKNOWN = auto()


@dataclass
class DocstringInfo:
    """Information extracted from a docstring."""

    style: DocstringStyle
    description: str = ""
    args: list[ArgInfo] = field(default_factory=list)
    returns: str = ""
    raises: list[str] = field(default_factory=list)
    examples: list[str] = field(default_factory=list)
    notes: list[str] = field(default_factory=list)
    see_also: list[str] = field(default_factory=list)
    references: list[str] = field(default_factory=list)
    todo: list[str] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    version: str = ""
    deprecated: bool = False
    since: str = ""
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class DocstringTypeInfo:
    """Type information extracted from docstrings."""

    param_types: dict[str, TypeInfo]
    return_type: TypeInfo | None
    yield_type: TypeInfo | None
    raises: list[tuple[TypeInfo, str]]  # (exception_type, description)


class DocstringTypeExtractor:
    """Extract type information from docstrings."""

    # Common type mappings for docstring type names
    TYPE_MAPPINGS: ClassVar[dict[str, Any]] = {
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
        "list": list,
        "dict": dict,
        "tuple": tuple,
        "set": set,
        "None": type(None),
        "any": Any,
        # Add more mappings as needed
    }

    def __init__(self, type_registry: TypeRegistry) -> None:
        """Initialize the docstring type extractor.

        Args:
            type_registry: Type registry for resolving types
        """
        self.type_registry = type_registry
        self._param_pattern = re.compile(
            r":param\s+(\w+)\s*:\s*(?:\(([^)]+)\))?\s*([^\n]+)"
        )
        self._type_pattern = re.compile(r":type\s+(\w+)\s*:\s*([^\n]+)")
        self._rtype_pattern = re.compile(r":rtype:\s*([^\n]+)")
        self._returns_pattern = re.compile(r":returns?:\s*([^\n]+)")
        self._yields_pattern = re.compile(r":yields?:\s*([^\n]+)")
        self._raises_pattern = re.compile(r":raises?\s+([^:]+):\s*([^\n]+)")

    def _extract_param_types(self, doc: Any) -> dict[str, TypeInfo]:
        """Extract parameter types from docstring."""
        param_types = {}
        for param in doc.params:
            if param.type_name:
                try:
                    type_info = self._parse_type_string(param.type_name)
                    param_types[param.arg_name] = type_info
                except TypeInferenceError as e:
                    logger.warning(
                        f"Failed to parse type for parameter {param.arg_name}: {e}"
                    )
        return param_types

    def _extract_return_type(self, doc: Any) -> TypeInfo | None:
        """Extract return type from docstring."""
        if doc.returns and doc.returns.type_name:
            try:
                return self._parse_type_string(doc.returns.type_name)
            except TypeInferenceError as e:
                logger.warning(f"Failed to parse return type: {e}")
        return None

    def _extract_yield_type(self, doc: Any) -> TypeInfo | None:
        """Extract yield type from docstring."""
        if hasattr(doc, "yields") and doc.yields and doc.yields.type_name:
            try:
                return self._parse_type_string(doc.yields.type_name)
            except TypeInferenceError as e:
                logger.warning(f"Failed to parse yield type: {e}")
        return None

    def _extract_raises(self, doc: Any) -> list[tuple[TypeInfo, str]]:
        """Extract raises information from docstring."""
        raises = []
        for raises_section in doc.raises:
            if raises_section.type_name:
                try:
                    exc_type = self._parse_type_string(raises_section.type_name)
                    raises.append((exc_type, raises_section.description or ""))
                except TypeInferenceError as e:
                    logger.warning(
                        f"Failed to parse exception type {raises_section.type_name}: {e}"
                    )
        return raises

    def extract_types(
        self,
        node: ast.AsyncFunctionDef | ast.FunctionDef | ast.ClassDef | ast.Module,
    ) -> DocstringTypeInfo | None:
        """Extract type information from an AST node's docstring.

        Args:
            node: AST node to process

        Returns:
            Extracted type information or None if no docstring found

        Raises:
            TypeInferenceError: If docstring parsing fails
        """
        docstring = ast.get_docstring(node)
        if not docstring:
            return None

        # Parse docstring
        doc = parse_docstring(docstring)

        return DocstringTypeInfo(
            param_types=self._extract_param_types(doc),
            return_type=self._extract_return_type(doc),
            yield_type=self._extract_yield_type(doc),
            raises=self._extract_raises(doc),
        )

    def _parse_type_string(self, type_str: str) -> TypeInfo:
        """Parse a type string from a docstring.

        Args:
            type_str: Type string to parse

        Returns:
            Parsed type information

        Raises:
            TypeInferenceError: If type string parsing fails
        """
        try:
            # Clean up type string
            type_str = type_str.strip()

            # Handle simple types
            if type_str in self.TYPE_MAPPINGS:
                return TypeInfo(
                    annotation=self.TYPE_MAPPINGS[type_str],
                    source="docstring",
                    confidence=0.8,
                    metadata={"original": type_str},
                )

            # Handle union types (e.g., "str or None")
            if " or " in type_str:
                types = [
                    self._parse_type_string(t.strip()) for t in type_str.split(" or ")
                ]
                if not types:
                    raise TypeInferenceError("Empty union type")
                if len(types) == 1:
                    return types[0]
                return TypeInfo(
                    annotation=tuple(t.annotation for t in types)[0]
                    | tuple(t.annotation for t in types)[1:],
                    source="docstring",
                    confidence=0.7,
                    metadata={"original": type_str, "union_types": types},
                )

            # Handle generic types (e.g., "List[str]")
            match = re.match(r"(\w+)\[(.*)\]", type_str)
            if match:
                container, content = match.groups()
                if container.lower() in ("list", "set", "tuple"):
                    elem_type = self._parse_type_string(content)
                    container_type = self.TYPE_MAPPINGS.get(container.lower(), list)
                    return TypeInfo(
                        annotation=container_type[elem_type.annotation],  # type: ignore
                        source="docstring",
                        confidence=0.7,
                        metadata={
                            "original": type_str,
                            "container": container,
                            "element_type": elem_type,
                        },
                    )
                elif container.lower() == "dict":
                    key_type, value_type = map(str.strip, content.split(","))
                    key_info = self._parse_type_string(key_type)
                    value_info = self._parse_type_string(value_type)
                    return TypeInfo(
                        annotation=dict[key_info.annotation, value_info.annotation],  # type: ignore
                        source="docstring",
                        confidence=0.7,
                        metadata={
                            "original": type_str,
                            "key_type": key_info,
                            "value_type": value_info,
                        },
                    )

            # Try to resolve as a type alias or fall back to Any
            if type_str in self.type_registry._type_aliases:
                return TypeInfo(
                    annotation=self.type_registry._type_aliases[type_str],
                    source="docstring",
                    confidence=0.6,
                    metadata={"original": type_str, "is_alias": True},
                )

            # If all else fails, return Any with low confidence
            return TypeInfo(
                annotation=Any,
                source="docstring",
                confidence=0.1,
                metadata={"original": type_str, "unresolved": True},
            )

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to parse type string '{type_str}': {e}"
            ) from e

================
File: src/twat_coding/pystubnik/types/type_system.py
================
#!/usr/bin/env -S uv run
"""Type system implementation with support for advanced type features."""

import ast
from dataclasses import dataclass
from typing import (
    Annotated,
    Any,
    Protocol,
    TypeVar,
    Union,
    get_args,
    get_origin,
    get_type_hints,
    runtime_checkable,
)

from ..errors import ErrorCode, StubGenerationError


class TypeInferenceError(StubGenerationError):
    """Error during type inference."""

    def __init__(
        self,
        message: str,
        code: ErrorCode = ErrorCode.TYPE_INFERENCE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
    ) -> None:
        """Initialize type inference error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
        """
        super().__init__(message, code, details, source=source, line_number=line_number)


@dataclass
class TypeInfo:
    """Information about a type annotation."""

    annotation: Any  # The type annotation itself
    source: str  # Where the type came from (annotation, docstring, inference)
    confidence: float  # Confidence score (0-1)
    metadata: dict[str, Any]  # Additional type metadata


@runtime_checkable
class TypeProtocol(Protocol):
    """Protocol for types that can be used in type hints."""

    __name__: str


class TypeRegistry:
    """Registry for type information and aliases."""

    def __init__(self) -> None:
        """Initialize the type registry."""
        self._type_aliases: dict[str, Any] = {}
        self._type_vars: dict[str, TypeVar] = {}
        self._protocols: dict[str, type[TypeProtocol]] = {}
        self._type_cache: dict[tuple[Any, str], TypeInfo] = {}

    def register_alias(self, name: str, target: Any) -> None:
        """Register a type alias.

        Args:
            name: Alias name
            target: Target type
        """
        self._type_aliases[name] = target

    def register_type_var(self, name: str, type_var: TypeVar) -> None:
        """Register a type variable.

        Args:
            name: Type variable name
            type_var: Type variable
        """
        self._type_vars[name] = type_var

    def register_protocol(self, protocol_class: type[TypeProtocol]) -> None:
        """Register a Protocol class.

        Args:
            protocol_class: Protocol class to register
        """
        self._protocols[protocol_class.__name__] = protocol_class

    def resolve_type(self, type_hint: Any, context: str = "") -> TypeInfo:
        """Resolve and normalize a type hint.

        Args:
            type_hint: Type hint to resolve
            context: Context for caching

        Returns:
            Resolved type information

        Raises:
            TypeInferenceError: If type resolution fails
        """
        cache_key = (type_hint, context)
        if cache_key in self._type_cache:
            return self._type_cache[cache_key]

        try:
            # Handle type aliases
            if isinstance(type_hint, str) and type_hint in self._type_aliases:
                resolved = self._type_aliases[type_hint]
            else:
                resolved = type_hint

            # Handle Annotated types
            origin = get_origin(resolved)
            if origin is Annotated:
                args = get_args(resolved)
                if not args:
                    raise TypeInferenceError("Empty Annotated type")
                base_type, *metadata = args
                type_info = TypeInfo(
                    annotation=base_type,
                    source="annotation",
                    confidence=1.0,
                    metadata={"annotations": metadata},
                )
            # Handle TypeVar
            elif isinstance(resolved, TypeVar):
                type_info = TypeInfo(
                    annotation=resolved,
                    source="typevar",
                    confidence=1.0,
                    metadata={
                        "name": resolved.__name__,
                        "bound": resolved.__bound__,
                        "constraints": resolved.__constraints__,
                    },
                )
            # Handle Protocol
            elif _is_protocol(resolved):
                type_info = TypeInfo(
                    annotation=resolved,
                    source="protocol",
                    confidence=1.0,
                    metadata={
                        "attributes": {
                            name: get_type_hints(resolved)[name]
                            for name in dir(resolved)
                            if not name.startswith("_")
                        }
                    },
                )
            # Handle generic types
            elif origin is not None:
                args = get_args(resolved)
                type_info = TypeInfo(
                    annotation=resolved,
                    source="generic",
                    confidence=1.0,
                    metadata={
                        "origin": origin,
                        "args": [self.resolve_type(arg).annotation for arg in args],
                    },
                )
            # Handle simple types
            else:
                type_info = TypeInfo(
                    annotation=resolved,
                    source="direct",
                    confidence=1.0,
                    metadata={},
                )

            self._type_cache[cache_key] = type_info
            return type_info

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to resolve type {type_hint}: {e}",
                details={"context": context},
            ) from e

    def merge_types(
        self,
        types: list[TypeInfo],
        *,
        prefer_explicit: bool = True,
    ) -> TypeInfo:
        """Merge multiple type information entries.

        Args:
            types: List of type information to merge
            prefer_explicit: Whether to prefer explicit annotations over inferred ones

        Returns:
            Merged type information

        Raises:
            TypeInferenceError: If type merging fails
        """
        if not types:
            raise TypeInferenceError("No types to merge")
        if len(types) == 1:
            return types[0]

        try:
            # Sort by confidence and source preference
            sorted_types = sorted(
                types,
                key=lambda t: (
                    t.confidence,
                    1 if t.source == "annotation" and prefer_explicit else 0,
                ),
                reverse=True,
            )

            # Use the highest confidence type as base
            base = sorted_types[0]

            # Merge metadata from all types
            merged_metadata = {}
            for type_info in sorted_types:
                merged_metadata.update(type_info.metadata)

            return TypeInfo(
                annotation=base.annotation,
                source=base.source,
                confidence=base.confidence,
                metadata=merged_metadata,
            )

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to merge types: {e}",
                details={"types": [str(t.annotation) for t in types]},
            ) from e


def extract_type_from_docstring(docstring: str) -> TypeInfo | None:
    """Extract type information from a docstring.

    Args:
        docstring: Docstring to parse

    Returns:
        Extracted type information or None if no type found
    """
    # TODO: Implement docstring type extraction
    # This will be implemented in a separate commit
    return None


def infer_type_from_usage(node: ast.AST) -> TypeInfo | None:
    """Infer type information from usage patterns.

    Args:
        node: AST node to analyze

    Returns:
        Inferred type information or None if inference not possible
    """
    # TODO: Implement type inference from usage
    # This will be implemented in a separate commit
    return None


def _is_protocol(resolved: Any) -> bool:
    """Check if a type is a Protocol.

    Args:
        resolved: Type to check

    Returns:
        True if the type is a Protocol
    """
    try:
        return (
            isinstance(resolved, type)
            and hasattr(resolved, "__mro__")
            and Protocol in resolved.__mro__
        )
    except (AttributeError, TypeError):
        return False


def _resolve_type(type_hint: Any) -> Any:
    """Resolve a type hint to its concrete type.

    Args:
        type_hint: Type hint to resolve

    Returns:
        Resolved type
    """
    try:
        if isinstance(type_hint, str):
            # Handle forward references
            return eval(type_hint, globals(), locals())
        return type_hint
    except (NameError, SyntaxError):
        return type_hint


def _is_subtype(type_hint: Any, expected_type: Any) -> bool:
    """Check if a type hint is a subtype of an expected type.

    Args:
        type_hint: Type hint to check
        expected_type: Expected type

    Returns:
        True if type_hint is a subtype of expected_type
    """
    resolved = _resolve_type(type_hint)
    expected = _resolve_type(expected_type)

    # Handle Protocol types
    if _is_protocol(expected):
        return True  # Assume Protocol compatibility for now

    # Handle Union types
    if hasattr(expected, "__origin__") and expected.__origin__ is Union:
        return any(_is_subtype(resolved, t) for t in expected.__args__)

    # Handle normal types
    try:
        return isinstance(resolved, type) and issubclass(resolved, expected)
    except TypeError:
        return False

================
File: src/twat_coding/pystubnik/utils/ast_utils.py
================
#!/usr/bin/env -S uv run
"""AST manipulation utilities."""

import ast
from typing import cast
from weakref import WeakKeyDictionary

from ..core.shared_types import TruncationConfig

# Global dict to store parent references
_parent_refs: WeakKeyDictionary[ast.AST, ast.AST] = WeakKeyDictionary()


def _get_parent(node: ast.AST) -> ast.AST | None:
    """Get parent node if it exists.

    Args:
        node: AST node

    Returns:
        Parent node if it exists, None otherwise
    """
    return _parent_refs.get(node)


def _truncate_string(s: str, config: TruncationConfig) -> str:
    """Truncate a string value according to config.

    Args:
        s: String to truncate
        config: Truncation configuration

    Returns:
        Truncated string
    """
    if len(s) <= config.max_string_length:
        return s
    return f"{s[: config.max_string_length]}{config.truncation_marker}"


def _truncate_bytes(b: bytes, config: TruncationConfig) -> bytes:
    """Truncate a bytes value according to config.

    Args:
        b: Bytes to truncate
        config: Truncation configuration

    Returns:
        Truncated bytes
    """
    if len(b) <= config.max_string_length:
        return b
    return b[: config.max_string_length] + b"..."


def _truncate_sequence(
    node: ast.List | ast.Set | ast.Tuple, config: TruncationConfig
) -> ast.AST:
    """Truncate a sequence (list, set, tuple) according to config.

    Args:
        node: AST node representing the sequence
        config: Truncation configuration

    Returns:
        Truncated AST node
    """
    truncated_elts = []
    for i, e in enumerate(node.elts):
        if i < config.max_sequence_length:
            truncated_elts.append(cast(ast.expr, truncate_literal(e, config)))
        else:
            truncated_elts.append(ast.Constant(value=config.truncation_marker))
            break
    return type(node)(elts=truncated_elts)


def _truncate_dict(node: ast.Dict, config: TruncationConfig) -> ast.Dict:
    """Truncate a dictionary according to config.

    Args:
        node: AST node representing the dictionary
        config: Truncation configuration

    Returns:
        Truncated AST node
    """
    pairs = []
    for i, (k, v) in enumerate(zip(node.keys, node.values, strict=False)):
        if i < config.max_sequence_length:
            new_v = cast(ast.expr, truncate_literal(v, config))
            pairs.append((k, new_v))
        else:
            pairs.append(
                (
                    ast.Constant(value="..."),
                    ast.Constant(value="..."),
                )
            )
            break
    return ast.Dict(
        keys=[k for k, _ in pairs],
        values=[cast(ast.expr, v) for _, v in pairs],
    )


def truncate_literal(node: ast.AST, config: TruncationConfig) -> ast.AST:
    """Truncate literal values in AST nodes.

    Args:
        node: AST node to process
        config: Truncation configuration

    Returns:
        Processed AST node
    """
    result = node  # Default to returning the original node
    match node:
        case ast.Constant(value=str() as s):
            if len(s) > config.max_string_length:
                result = ast.Constant(
                    value=f"{s[: config.max_string_length]}{config.truncation_marker}"
                )
        case ast.Constant(value=bytes() as b):
            return ast.Constant(value=_truncate_bytes(b, config))

        case ast.List() | ast.Set() | ast.Tuple():
            if len(node.elts) > config.max_sequence_length:
                node.elts = node.elts[: config.max_sequence_length] + [
                    ast.Constant(value=config.truncation_marker)
                ]
                result = node
        case ast.Dict():
            if len(node.keys) > config.max_sequence_length:
                node.keys = node.keys[: config.max_sequence_length] + [
                    ast.Constant(value=config.truncation_marker)
                ]
                node.values = node.values[: config.max_sequence_length] + [
                    ast.Constant(value=config.truncation_marker)
                ]
                result = node
        case _:
            return node

    return result


def attach_parents(node: ast.AST) -> None:
    """Attach parent references to AST nodes.

    Args:
        node: AST node to process
    """
    for child in ast.walk(node):
        for _field, value in ast.iter_fields(child):
            if isinstance(value, list):
                for item in value:
                    if isinstance(item, ast.AST):
                        item.parent = child  # type: ignore
            elif isinstance(value, ast.AST):
                value.parent = child  # type: ignore


def get_docstring(node: ast.AST) -> str | None:
    """Get docstring from an AST node.

    Args:
        node: AST node to process

    Returns:
        Docstring if found, None otherwise
    """
    match node:
        case ast.Module() | ast.ClassDef() | ast.FunctionDef():
            if (
                node.body
                and isinstance(node.body[0], ast.Expr)
                and isinstance(node.body[0].value, ast.Constant)
                and isinstance(node.body[0].value.value, str)
            ):
                return node.body[0].value.value
    return None


def is_empty_expr(node: ast.AST) -> bool:
    """Check if a node is an empty or whitespace-only expression.

    Args:
        node: AST node to check

    Returns:
        True if node is an empty expression
    """
    return (
        isinstance(node, ast.Expr)
        and isinstance(node.value, ast.Constant)
        and (not node.value.value or str(node.value.value).isspace())
    )

================
File: src/twat_coding/pystubnik/utils/display.py
================
#!/usr/bin/env -S uv run
"""Display utilities for file trees and progress indicators."""

from pathlib import Path
from typing import Any

from rich.console import Console
from rich.tree import Tree

console = Console()


def print_file_tree(paths: list[Path]) -> None:
    """Print a tree representation of file paths.

    Args:
        paths: List of paths to display
    """
    tree: dict[str, Any] = {}
    for path in sorted(paths):
        add_to_tree(tree, path.parts)

    root = Tree("📁 Project")
    build_rich_tree(root, tree)
    console.print(root)


def add_to_tree(tree: dict[str, Any], components: list[str] | tuple[str, ...]) -> None:
    """Add a path to the tree structure.

    Args:
        tree: Tree dictionary to update
        components: Path components
    """
    current = tree
    for component in components:
        if component not in current:
            current[component] = {}
        current = current[component]


def build_rich_tree(tree: Tree, data: dict[str, Any], prefix: str = "") -> None:
    """Build a rich Tree from the tree dictionary.

    Args:
        tree: Rich Tree to update
        data: Tree dictionary
        prefix: Current path prefix
    """
    for name, subtree in sorted(data.items()):
        icon = "📁" if subtree else "📄"
        branch = tree.add(f"{icon} {name}")
        if subtree:
            build_rich_tree(branch, subtree, f"{prefix}/{name}" if prefix else name)


def print_progress(message: str, current: int, total: int) -> None:
    """Print progress information.

    Args:
        message: Progress message
        current: Current progress value
        total: Total progress value
    """
    percentage = (current / total) * 100 if total > 0 else 0
    console.print(f"{message}: [{current}/{total}] {percentage:.1f}%")

================
File: src/twat_coding/pystubnik/utils/memory.py
================
#!/usr/bin/env -S uv run
"""Memory monitoring utilities."""

import ast
import asyncio
import gc
import os
import threading
from collections.abc import AsyncGenerator, Generator
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any

from loguru import logger

try:
    import psutil
    from memory_profiler import profile as memory_profile

    HAS_MEMORY_TOOLS = True
except ImportError:
    HAS_MEMORY_TOOLS = False
    psutil = None
    memory_profile = None


@dataclass
class MemoryStats:
    """Memory usage statistics."""

    rss: int  # Resident Set Size in bytes
    vms: int  # Virtual Memory Size in bytes
    shared: int  # Shared memory in bytes
    text: int  # Text segment memory in bytes
    data: int  # Data segment memory in bytes
    lib: int  # Library memory in bytes
    dirty: int  # Dirty pages in bytes
    peak_rss: int  # Peak RSS memory in bytes


class MemoryMonitor:
    """Monitor memory usage of the process."""

    def __init__(self, interval: float = 1.0) -> None:
        """Initialize the memory monitor.

        Args:
            interval: Monitoring interval in seconds
        """
        if not HAS_MEMORY_TOOLS:
            logger.warning(
                "psutil and memory_profiler not available, memory monitoring disabled"
            )
            return

        self.interval = interval
        self._process = psutil.Process(os.getpid())
        self._stop_event = threading.Event()
        self._stats: list[MemoryStats] = []
        self._monitor_thread: threading.Thread | None = None

    def start(self) -> None:
        """Start monitoring memory usage."""
        if not HAS_MEMORY_TOOLS:
            return

        if self._monitor_thread is not None:
            return

        def _monitor() -> None:
            while not self._stop_event.is_set():
                try:
                    meminfo = self._process.memory_info()
                    stats = MemoryStats(
                        rss=meminfo.rss,
                        vms=meminfo.vms,
                        shared=meminfo.shared,
                        text=meminfo.text,
                        data=meminfo.data,
                        lib=meminfo.lib,
                        dirty=meminfo.dirty,
                        peak_rss=self._process.memory_info().rss,
                    )
                    self._stats.append(stats)
                    logger.debug(
                        f"Memory usage: RSS={stats.rss / 1024 / 1024:.1f}MB, "
                        f"VMS={stats.vms / 1024 / 1024:.1f}MB"
                    )
                except Exception as e:
                    logger.error(f"Failed to collect memory stats: {e}")
                self._stop_event.wait(self.interval)

        self._monitor_thread = threading.Thread(target=_monitor, daemon=True)
        self._monitor_thread.start()

    def stop(self) -> None:
        """Stop monitoring memory usage."""
        if not HAS_MEMORY_TOOLS or self._monitor_thread is None:
            return
        self._stop_event.set()
        self._monitor_thread.join()
        self._monitor_thread = None

    @property
    def stats(self) -> list[MemoryStats]:
        """Get collected memory statistics."""
        return self._stats.copy()

    @property
    def peak_memory(self) -> int:
        """Get peak memory usage in bytes."""
        if not self._stats:
            return 0
        return max(stat.peak_rss for stat in self._stats)

    def clear_stats(self) -> None:
        """Clear collected statistics."""
        self._stats.clear()


@contextmanager
def memory_monitor(interval: float = 1.0) -> Generator[MemoryMonitor, None, None]:
    """Context manager for memory monitoring.

    Args:
        interval: Monitoring interval in seconds

    Yields:
        Memory monitor instance
    """
    monitor = MemoryMonitor(interval)
    monitor.start()
    try:
        yield monitor
    finally:
        monitor.stop()


async def stream_process_ast(
    node: ast.AST,
    chunk_size: int = 1000,
    gc_interval: int = 10,
) -> AsyncGenerator[list[ast.AST], None]:
    """Process AST nodes in chunks to reduce memory usage.

    Args:
        node: AST node to process
        chunk_size: Number of nodes to process in each chunk
        gc_interval: Number of chunks between garbage collection

    Yields:
        Lists of processed AST nodes
    """
    nodes = list(ast.walk(node))
    chunks = [nodes[i : i + chunk_size] for i in range(0, len(nodes), chunk_size)]

    for i, chunk in enumerate(chunks):
        # Process nodes in the chunk
        processed = []
        for node in chunk:
            # Add your node processing logic here
            processed.append(node)

        # Run garbage collection periodically
        if i > 0 and i % gc_interval == 0:
            gc.collect()

        # Let other tasks run
        await asyncio.sleep(0)

        yield processed


def profile_memory(func: Any) -> Any:
    """Decorator to profile memory usage of a function.

    Args:
        func: Function to profile

    Returns:
        Profiled function
    """
    if not HAS_MEMORY_TOOLS:
        logger.warning("memory_profiler not available, profiling disabled")
        return func
    return memory_profile(func)

================
File: src/twat_coding/pystubnik/__init__.py
================
#!/usr/bin/env python3
"""Smart stub generation for Python code.

This package provides tools for generating high-quality type stubs for Python code,
with support for multiple backends and intelligent processing of imports,
docstrings, and code importance.
"""

import asyncio
import sys
from collections.abc import Mapping, Sequence
from importlib.metadata import version
from pathlib import Path
from typing import Literal, Protocol

from loguru import logger

from .backends import StubBackend
from .backends.ast_backend import ASTBackend
from .backends.mypy_backend import MypyBackend
from .config import StubConfig
from .core.config import (
    Backend,
    ImportanceLevel,
    PathConfig,
    ProcessingConfig,
    RuntimeConfig,
    StubGenConfig,
    TruncationConfig,
)
from .core.types import (
    ArgInfo,
    ClassInfo,
    FunctionInfo,
    ModuleInfo,
    PathLike,
    StubResult,
)
from .core.utils import setup_logging
from .errors import ASTError, ConfigError, ErrorCode, MyPyError, StubGenerationError
from .processors.docstring import DocstringProcessor
from .processors.importance import ImportanceConfig, ImportanceProcessor
from .processors.imports import ImportProcessor


def _convert_to_stub_config(config: StubGenConfig) -> StubConfig:
    """Convert StubGenConfig to StubConfig.

    Args:
        config: Source configuration

    Returns:
        Converted configuration
    """
    # Convert paths to strings for search_paths
    search_paths = [str(p) for p in config.paths.search_paths]

    # Convert files to list[Path]
    files = list(config.paths.files)  # Convert Sequence to list

    return StubConfig(
        input_path=config.paths.files[0] if config.paths.files else Path("."),
        output_path=config.paths.output_dir,
        backend="ast",  # Default to AST backend
        parallel=config.runtime.parallel,
        max_workers=config.runtime.max_workers,
        infer_types=config.processing.infer_property_types,
        preserve_literals=True,  # We handle this in the AST backend
        docstring_type_hints=config.processing.include_docstrings,
        python_version=config.runtime.python_version,
        no_import=config.runtime.no_import,
        inspect=config.runtime.inspect,
        doc_dir=str(config.paths.doc_dir) if config.paths.doc_dir else "",
        search_paths=search_paths,
        interpreter=config.runtime.interpreter,
        ignore_errors=config.runtime.ignore_errors,
        parse_only=config.runtime.parse_only,
        include_private=config.processing.include_private,
        modules=list(config.paths.modules),  # Convert Sequence to list
        packages=list(config.paths.packages),  # Convert Sequence to list
        files=files,
        verbose=config.runtime.verbose,
        quiet=config.runtime.quiet,
        export_less=config.processing.export_less,
        importance_patterns=dict(config.processing.importance_patterns),
        max_docstring_length=config.truncation.max_docstring_length,
        include_type_comments=config.processing.include_type_comments,
        infer_property_types=config.processing.infer_property_types,
        line_length=88,  # Default line length
        sort_imports=True,  # Default to sorting imports
        add_header=True,  # Default to adding header
    )


def _convert_to_stub_gen_config(config: StubConfig) -> StubGenConfig:
    """Convert StubConfig to StubGenConfig.

    Args:
        config: Source configuration

    Returns:
        Converted configuration
    """
    return StubGenConfig(
        paths=PathConfig(
            output_dir=config.output_path or Path("out"),
            doc_dir=Path(config.doc_dir) if config.doc_dir else None,
            search_paths=[Path(p) for p in config.search_paths],
            modules=list(config.modules),
            packages=list(config.packages),
            files=list(config.files),
        ),
        runtime=RuntimeConfig.create(
            backend=Backend.AST if config.backend == "ast" else Backend.MYPY,
            python_version=config.python_version,
            interpreter=config.interpreter,
            no_import=config.no_import,
            inspect=config.inspect,
            parse_only=config.parse_only,
            ignore_errors=config.ignore_errors,
            verbose=config.verbose,
            quiet=config.quiet,
            parallel=config.parallel,
            max_workers=config.max_workers,
        ),
        processing=ProcessingConfig(
            include_docstrings=config.docstring_type_hints,
            include_private=config.include_private,
            include_type_comments=config.include_type_comments,
            infer_property_types=config.infer_property_types,
            export_less=config.export_less,
            importance_patterns=dict(config.importance_patterns),
        ),
        truncation=TruncationConfig(
            max_docstring_length=config.max_docstring_length,
        ),
    )


class Processor(Protocol):
    """Protocol for stub processors."""

    def process(self, stub_result: StubResult) -> StubResult:
        """Process a stub result.

        Args:
            stub_result: Input stub result

        Returns:
            Processed stub result
        """
        ...


class SmartStubGenerator:
    """Main interface for generating smart stubs."""

    def __init__(
        self,
        *,
        # Path configuration
        output_dir: str | Path = "out",
        doc_dir: str | Path | None = None,
        search_paths: Sequence[str | Path] = (),
        modules: Sequence[str] = (),
        packages: Sequence[str] = (),
        files: Sequence[str | Path] = (),
        # Runtime configuration
        backend: Backend | str = Backend.HYBRID,
        python_version: tuple[int, int] | None = None,
        interpreter: str | Path | None = None,
        no_import: bool = False,
        inspect: bool = False,
        parse_only: bool = False,
        ignore_errors: bool = True,
        verbose: bool = False,
        quiet: bool = True,
        parallel: bool = True,
        max_workers: int | None = None,
        # Processing configuration
        include_docstrings: bool = True,
        include_private: bool = False,
        include_type_comments: bool = True,
        infer_property_types: bool = True,
        export_less: bool = False,
        importance_patterns: Mapping[str, float] | None = None,
        importance_keywords: set[str] | None = None,
        # Truncation configuration
        max_sequence_length: int | None = None,
        max_string_length: int | None = None,
        max_docstring_length: int | None = None,
        max_file_size: int | None = None,
        truncation_marker: str | None = None,
    ):
        """Initialize the stub generator with configuration.

        Args:
            output_dir: Directory to write stubs to
            doc_dir: Directory containing .rst documentation
            search_paths: Module search paths
            modules: Module names to process
            packages: Package names to process recursively
            files: Specific files to process
            backend: Stub generation backend (AST, MYPY, or HYBRID)
            python_version: Python version to target
            interpreter: Python interpreter to use
            no_import: Don't import modules
            inspect: Use runtime inspection
            parse_only: Only parse, no semantic analysis
            ignore_errors: Continue on errors
            verbose: Show detailed output
            quiet: Minimal output
            parallel: Use parallel processing
            max_workers: Maximum worker threads
            include_docstrings: Include docstrings in stubs
            include_private: Include private symbols
            include_type_comments: Include type comments
            infer_property_types: Try to infer property types
            export_less: Don't export imported names
            importance_patterns: Patterns for importance scoring
            importance_keywords: Keywords indicating importance
            max_sequence_length: Maximum sequence length
            max_string_length: Maximum string length
            max_docstring_length: Maximum docstring length
            max_file_size: Maximum file size
            truncation_marker: Marker for truncated content
        """
        # Convert backend string to enum
        if isinstance(backend, str):
            backend = Backend[backend.upper()]

        # Create configuration objects
        self.config = StubGenConfig(
            paths=PathConfig(
                output_dir=Path(output_dir),
                doc_dir=Path(doc_dir) if doc_dir else None,
                search_paths=[Path(p) for p in search_paths],
                modules=list(modules),
                packages=list(packages),
                files=[Path(f) for f in files],
            ),
            runtime=RuntimeConfig.create(
                backend=backend,
                python_version=python_version,
                interpreter=interpreter,
                no_import=no_import,
                inspect=inspect,
                parse_only=parse_only,
                ignore_errors=ignore_errors,
                verbose=verbose,
                quiet=quiet,
                parallel=parallel,
                max_workers=max_workers,
            ),
            processing=ProcessingConfig(
                include_docstrings=include_docstrings,
                include_private=include_private,
                include_type_comments=include_type_comments,
                infer_property_types=infer_property_types,
                export_less=export_less,
                importance_patterns=dict(importance_patterns or {}),
                importance_keywords=set(importance_keywords or set()),
            ),
            truncation=TruncationConfig(
                max_sequence_length=max_sequence_length or 4,
                max_string_length=max_string_length or 17,
                max_docstring_length=max_docstring_length or 150,
                max_file_size=max_file_size or 3_000,
                truncation_marker=truncation_marker or "...",
            ),
        )

        # Configure logging
        logger.remove()
        logger.add(
            lambda msg: print(msg, end=""),
            format="<blue>{time:HH:mm:ss}</blue> | {message}",
            level="DEBUG" if verbose else "INFO",
            catch=True,
        )

        # Initialize processors
        self.processors: list[Processor] = [
            DocstringProcessor(
                style="google",
                max_length=self.config.truncation.max_docstring_length,
                preserve_sections=["Args", "Returns", "Yields", "Raises"],
            ),
            ImportanceProcessor(
                config=ImportanceConfig(
                    patterns=dict(self.config.processing.importance_patterns),
                    keywords=self.config.processing.importance_keywords,
                )
            ),
            ImportProcessor(),
        ]

    def _initialize_backend(self) -> StubBackend:
        """Initialize the appropriate backend based on configuration.

        Returns:
            Initialized backend
        """
        # Convert StubGenConfig to StubConfig for the backend
        stub_config = _convert_to_stub_config(self.config)

        if self.config.runtime.backend == Backend.AST:
            return ASTBackend(stub_config)
        elif self.config.runtime.backend == Backend.MYPY:
            return MypyBackend(stub_config)
        else:
            raise ConfigError(
                f"Unknown backend: {self.config.runtime.backend}",
                ErrorCode.CONFIG_VALIDATION_ERROR,
            )

    def _process_file(self, backend: StubBackend, file_path: Path) -> None:
        """Process a single file."""
        try:
            # Generate stub
            result = asyncio.run(backend.generate_stub(file_path))
            if isinstance(result, str):
                stub_content = result
            elif isinstance(result, StubResult):
                # Apply processors
                for processor in self.processors:
                    result = processor.process(result)
                stub_content = result.stub_content
            else:
                logger.error(
                    f"Unexpected result type from generate_stub: {type(result)}"
                )
                return

            # Write stub
            output_path = (
                self.config.paths.output_dir / file_path.with_suffix(".pyi").name
            )
            output_path.write_text(stub_content)

        except Exception as e:
            logger.error(f"Failed to process {file_path}: {e}")
            if not self.config.runtime.ignore_errors:
                raise

    def generate(self) -> None:
        """Generate stubs according to configuration."""
        try:
            logger.info(
                f"Generating stubs using {self.config.runtime.backend.name} backend"
            )
            logger.info(f"Output directory: {self.config.paths.output_dir}")

            # Create output directory
            self.config.paths.output_dir.mkdir(parents=True, exist_ok=True)

            # Initialize backend
            backend = self._initialize_backend()

            # Process files
            for file_path in self.config.paths.files:
                self._process_file(backend, file_path)

            logger.info("Stub generation completed successfully")

        except Exception as e:
            logger.error(f"Failed to generate stubs: {e}")
            if self.config.runtime.verbose:
                import traceback

                logger.debug(traceback.format_exc())
            if not self.config.runtime.ignore_errors:
                raise

    def generate_for_file(self, file_path: str | Path) -> StubResult:
        """Generate stub for a single file.

        Args:
            file_path: Path to the Python file

        Returns:
            Generated stub result
        """
        # TODO: Implement single file stub generation
        raise NotImplementedError

    def generate_for_module(self, module_name: str) -> StubResult:
        """Generate stub for a module by name.

        Args:
            module_name: Fully qualified module name

        Returns:
            Generated stub result
        """
        # TODO: Implement module stub generation
        raise NotImplementedError


async def generate_stub(
    source_path: PathLike,
    output_path: PathLike | None = None,
    backend: Literal["ast", "mypy"] = "ast",
    config: StubConfig | None = None,
) -> StubResult:
    """Generate a type stub for a Python source file.

    Args:
        source_path: Path to the source file
        output_path: Optional path to write the stub file
        backend: Backend to use for stub generation ("ast" or "mypy")
        config: Optional configuration for stub generation

    Returns:
        StubResult containing the generated stub and metadata

    Raises:
        ValueError: If the specified backend is not supported
    """
    source_path = Path(source_path)
    output_path_obj = Path(output_path) if output_path else None

    # Create default config if none provided
    if config is None:
        config = StubConfig(
            input_path=source_path,
            output_path=output_path_obj,
            backend=backend,  # type: Literal["ast", "mypy"]
            parallel=True,
            max_workers=None,
            infer_types=True,
            preserve_literals=True,
            docstring_type_hints=True,
            line_length=88,
            sort_imports=True,
            add_header=True,
            no_import=False,
            inspect=False,
            doc_dir="",
            ignore_errors=True,
            parse_only=False,
            include_private=False,
            verbose=False,
            quiet=True,
            export_less=False,
            max_docstring_length=150,
            include_type_comments=True,
            infer_property_types=True,
            files=[source_path],
            include_patterns=["*.py"],
            exclude_patterns=["test_*.py", "*_test.py"],
            modules=[],
            packages=[],
            search_paths=[],
            python_version=(sys.version_info.major, sys.version_info.minor),
            interpreter=Path(sys.executable),
        )

    # Convert StubConfig to StubGenConfig for backend
    stub_gen_config = _convert_to_stub_gen_config(config)

    # Initialize backend
    backend_obj: StubBackend
    if backend == "ast":
        backend_obj = ASTBackend(stub_gen_config)
    elif backend == "mypy":
        backend_obj = MypyBackend(stub_gen_config)
    else:
        raise ValueError(f"Unsupported backend: {backend}")

    # Generate stub
    result = await backend_obj.generate_stub(source_path)
    if not isinstance(result, StubResult):
        raise TypeError(f"Expected StubResult, got {type(result)}")

    # Write stub if output path is specified
    if output_path_obj:
        output_path_obj.parent.mkdir(parents=True, exist_ok=True)
        output_path_obj.write_text(result.stub_content)

    return result


# Configure default logging
setup_logging()

# Version information
__version__ = version("twat_coding")
__author__ = "Adam Twardoch"
__license__ = "MIT"

__all__ = [
    "ArgInfo",
    "ASTBackend",
    "Backend",
    "ClassInfo",
    "FunctionInfo",
    "ImportanceLevel",
    "ModuleInfo",
    "MypyBackend",
    "SmartStubGenerator",
    "StubBackend",
    "StubGenConfig",
    "StubResult",
    "generate_stub",
    "RuntimeConfig",
    "StubConfig",
    "ASTError",
    "ConfigError",
    "ErrorCode",
    "MyPyError",
    "StubGenerationError",
]

================
File: src/twat_coding/pystubnik/config.py
================
#!/usr/bin/env -S uv run
"""Configuration system for stub generation."""

import os
import sys
from collections.abc import Sequence
from pathlib import Path
from typing import Any, Literal

from pydantic import BaseModel, Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

from .core.config import PathConfig, StubGenConfig
from .errors import ConfigError, ErrorCode


class FileLocations(BaseModel):
    """File locations for stub generation."""

    source_path: Path = Field(..., description="Path to Python source file")
    input_dir: Path = Field(..., description="Base input directory")
    output_dir: Path = Field(..., description="Base output directory")

    model_config = SettingsConfigDict(
        arbitrary_types_allowed=True,
        validate_assignment=True,
    )

    @field_validator("source_path")
    @classmethod
    def validate_source_path(cls, v: Path) -> Path:
        """Validate source file exists."""
        if not v.exists():
            raise ConfigError(
                "Source file does not exist",
                ErrorCode.CONFIG_VALIDATION_ERROR,
                source=str(v),
            )
        return v

    @field_validator("input_dir")
    @classmethod
    def validate_input_dir(cls, v: Path) -> Path:
        """Validate input directory exists."""
        if not v.exists():
            raise ConfigError(
                "Input directory does not exist",
                ErrorCode.CONFIG_VALIDATION_ERROR,
                source=str(v),
            )
        return v

    @field_validator("output_dir")
    @classmethod
    def validate_output_dir(cls, v: Path) -> Path:
        """Validate and create output directory."""
        try:
            v.mkdir(parents=True, exist_ok=True)
            return v
        except Exception as e:
            raise ConfigError(
                f"Failed to create output directory: {e}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(v),
            ) from e

    @property
    def output_path(self) -> Path:
        """Calculate output path based on input path and bases."""
        try:
            rel_path = self.source_path.relative_to(self.input_dir)
            return self.output_dir / rel_path
        except ValueError as e:
            raise ConfigError(
                f"Source file {self.source_path} is not within input directory {self.input_dir}",
                ErrorCode.CONFIG_VALIDATION_ERROR,
            ) from e

    def check_paths(self) -> None:
        """Validate file locations and create output directories."""
        # Create output directory and verify permissions
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        if not os.access(self.output_path.parent, os.W_OK):
            raise ConfigError(
                f"No write permission: {self.output_path.parent}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(self.output_path.parent),
            )


def _default_stub_gen_config() -> StubGenConfig:
    """Create default StubGenConfig instance.

    Returns:
        Default StubGenConfig instance
    """
    return StubGenConfig(paths=PathConfig())


class StubConfig(BaseModel):
    """Configuration for stub generation."""

    model_config = SettingsConfigDict(
        extra="allow",  # Allow extra fields to handle all parameters
        frozen=True,
        validate_assignment=True,
    )

    # Input/Output settings
    input_path: Path = Field(
        ...,
        description="Path to Python source files or directory",
    )
    output_path: Path | None = Field(
        None,
        description="Path to output directory for stubs",
    )
    files: list[Path] = Field(
        default_factory=list,
        description="List of files to process",
    )
    include_patterns: list[str] = Field(
        default_factory=lambda: ["*.py"],
        description="Glob patterns for files to include",
    )
    exclude_patterns: list[str] = Field(
        default_factory=lambda: ["test_*.py", "*_test.py"],
        description="Glob patterns for files to exclude",
    )

    # Processing settings
    backend: Literal["ast", "mypy"] = Field(
        "ast",
        description="Backend to use for stub generation",
    )
    parallel: bool = Field(
        True,
        description="Enable parallel processing",
    )
    max_workers: int | None = Field(
        None,
        description="Maximum number of worker threads",
        ge=1,
    )

    # Stub generation settings
    stub_gen_config: StubGenConfig = Field(
        default_factory=_default_stub_gen_config,
        description="Configuration for stub generation",
    )

    # Type inference settings
    infer_types: bool = Field(
        True,
        description="Enable type inference",
    )
    preserve_literals: bool = Field(
        False,
        description="Preserve literal values in stubs",
    )
    docstring_type_hints: bool = Field(
        True,
        description="Extract type hints from docstrings",
    )

    # Output settings
    line_length: int = Field(
        88,
        description="Maximum line length for output",
        ge=1,
    )
    sort_imports: bool = Field(
        True,
        description="Sort imports in output",
    )
    add_header: bool = Field(
        True,
        description="Add header to generated files",
    )

    # MyPy-specific settings
    python_version: tuple[int, int] = Field(
        default_factory=lambda: (sys.version_info.major, sys.version_info.minor),
        description="Python version to target",
    )
    no_import: bool = Field(
        False,
        description="Don't import modules, just parse and analyze",
    )
    inspect: bool = Field(
        False,
        description="Import and inspect modules instead of parsing source",
    )
    doc_dir: str = Field(
        "",
        description="Path to .rst documentation directory",
    )
    search_paths: Sequence[str | Path] = Field(
        default_factory=list,
        description="Module search paths",
    )
    interpreter: str | Path = Field(
        default_factory=lambda: sys.executable,
        description="Python interpreter to use",
    )
    ignore_errors: bool = Field(
        True,
        description="Ignore errors during stub generation",
    )
    parse_only: bool = Field(
        False,
        description="Don't do semantic analysis of source",
    )
    include_private: bool = Field(
        False,
        description="Include private objects in stubs",
    )
    modules: Sequence[str] = Field(
        default_factory=list,
        description="List of module names to process",
    )
    packages: Sequence[str] = Field(
        default_factory=list,
        description="List of package names to process",
    )
    verbose: bool = Field(
        False,
        description="Enable verbose output",
    )
    quiet: bool = Field(
        True,
        description="Enable quiet mode",
    )
    export_less: bool = Field(
        False,
        description="Export fewer symbols",
    )
    importance_patterns: dict[str, float] = Field(
        default_factory=dict,
        description="Patterns for importance scoring",
    )
    max_docstring_length: int = Field(
        150,
        description="Maximum docstring length",
    )
    include_type_comments: bool = Field(
        True,
        description="Include type comments",
    )
    infer_property_types: bool = Field(
        True,
        description="Infer property types",
    )

    def get_file_locations(self, source_path: Path) -> FileLocations:
        """Get file locations for a source file.

        Args:
            source_path: Path to source file

        Returns:
            FileLocations object
        """
        return FileLocations(
            source_path=source_path,
            input_dir=self.input_path,
            output_dir=self.output_path or Path("out"),
        )


class RuntimeConfig(BaseSettings):
    """Runtime configuration for the application."""

    model_config = SettingsConfigDict(
        env_prefix="PYSTUBNIK_",
        env_file=".env",
        extra="ignore",
    )

    # Logging settings
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    log_file: Path | None = None

    # Performance settings
    cache_dir: Path = Field(default_factory=lambda: Path(".cache"))
    memory_limit: int | None = None  # In megabytes
    timeout: int | None = None  # In seconds

    # Development settings
    debug: bool = False
    profile: bool = False

    @field_validator("cache_dir", mode="before")
    @classmethod
    def validate_cache_dir(cls, v: Any) -> Path:
        """Validate and create cache directory."""
        try:
            path = Path(v)
            path.mkdir(parents=True, exist_ok=True)
            return path
        except Exception as e:
            raise ConfigError(
                f"Failed to create cache directory: {e}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(v),
            ) from e

================
File: src/twat_coding/pystubnik/errors.py
================
#!/usr/bin/env -S uv run
"""Error handling system for stub generation."""

from enum import Enum
from typing import Any


class ErrorCode(str, Enum):
    """Error codes for stub generation."""

    # AST-related errors (AST*)
    AST_PARSE_ERROR = "AST001"  # Failed to parse Python source
    AST_VISIT_ERROR = "AST002"  # Error during AST node visitation
    AST_TRANSFORM_ERROR = "AST003"  # Error transforming AST to stub

    # Type inference errors (TYPE*)
    TYPE_INFERENCE_ERROR = "TYPE001"  # Failed to infer type
    TYPE_CONFLICT_ERROR = "TYPE002"  # Conflicting type information
    TYPE_VALIDATION_ERROR = "TYPE003"  # Invalid type annotation

    # Configuration errors (CFG*)
    CONFIG_PARSE_ERROR = "CFG001"  # Failed to parse configuration
    CONFIG_VALIDATION_ERROR = "CFG002"  # Invalid configuration
    CONFIG_IO_ERROR = "CFG003"  # Configuration file I/O error

    # Backend errors (BACKEND*)
    BACKEND_ERROR = "BACKEND001"  # Generic backend error
    BACKEND_NOT_FOUND = "BACKEND002"  # Backend not available
    BACKEND_CONFIG_ERROR = "BACKEND003"  # Backend configuration error

    # I/O errors (IO*)
    IO_READ_ERROR = "IO001"  # Failed to read source file
    IO_WRITE_ERROR = "IO002"  # Failed to write stub file
    IO_PERMISSION_ERROR = "IO003"  # Permission denied

    # Processing errors (PROC*)
    PROC_TIMEOUT_ERROR = "PROC001"  # Processing timeout
    PROC_MEMORY_ERROR = "PROC002"  # Out of memory
    PROC_INTERRUPT_ERROR = "PROC003"  # Processing interrupted


class StubGenerationError(Exception):
    """Base class for all stub generation errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
    ) -> None:
        """Initialize the error.

        Args:
            message: Human-readable error message
            code: Error code from ErrorCode enum or string
            details: Additional error details
            source: Source code or file where error occurred
            line_number: Line number where error occurred
        """
        self.code = ErrorCode(code) if isinstance(code, str) else code
        self.details = details or {}
        self.source = source
        self.line_number = line_number
        super().__init__(message)

    def __str__(self) -> str:
        """Return formatted error message."""
        msg = f"[{self.code}] {super().__str__()}"
        if self.source and self.line_number:
            msg = f"{msg}\nAt {self.source}:{self.line_number}"
        if self.details:
            msg = f"{msg}\nDetails: {self.details}"
        return msg


class ASTError(StubGenerationError):
    """AST processing errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.AST_PARSE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
        node_type: str | None = None,
    ) -> None:
        """Initialize AST error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
            node_type: Type of AST node where error occurred
        """
        if node_type:
            details = details or {}
            details["node_type"] = node_type
        super().__init__(message, code, details, source=source, line_number=line_number)


class MyPyError(StubGenerationError):
    """MyPy backend errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.BACKEND_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
        mypy_error_code: str | None = None,
    ) -> None:
        """Initialize MyPy error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
            mypy_error_code: Original MyPy error code
        """
        if mypy_error_code:
            details = details or {}
            details["mypy_error_code"] = mypy_error_code
        super().__init__(message, code, details, source=source, line_number=line_number)


class ConfigError(StubGenerationError):
    """Configuration errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.CONFIG_PARSE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        config_key: str | None = None,
    ) -> None:
        """Initialize configuration error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Configuration file path
            config_key: Key in configuration that caused error
        """
        if config_key:
            details = details or {}
            details["config_key"] = config_key
        super().__init__(message, code, details, source=source)

================
File: src/twat_coding/pystubnik/read_imports.py
================
#!/usr/bin/env python3
"""Extract imports from Python files."""

import ast
from pathlib import Path

import fire


def ast_to_source(node: ast.Import | ast.ImportFrom) -> str:
    """Convert an AST import node to source code.

    Args:
        node: The AST import node

    Returns:
        The source code representation of the import
    """
    if isinstance(node, ast.Import):
        names = [alias.name for alias in node.names]
        return f"import {', '.join(names)}"
    elif isinstance(node, ast.ImportFrom):
        names = [alias.name for alias in node.names]
        module = node.module or ""
        level = "." * node.level
        return f"from {level}{module} import {', '.join(names)}"
    else:
        msg = f"Unexpected node type: {type(node)}"
        raise ValueError(msg)


def extract_imports(source: str) -> list[ast.Import | ast.ImportFrom]:
    """Extract import statements from Python source code.

    Args:
        source: Python source code

    Returns:
        List of import nodes
    """
    tree = ast.parse(source)
    imports: list[ast.Import | ast.ImportFrom] = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Import | ast.ImportFrom):
            imports.append(node)

    return imports


def process_py_file(file_path: str | Path) -> None:
    """Process a Python file to extract imports.

    Args:
        file_path: Path to the Python file
    """
    file_path = Path(file_path)
    source = file_path.read_text()
    imports = extract_imports(source)

    for _node in imports:
        pass


def main() -> None:
    """Main entry point."""
    fire.Fire(process_py_file)


if __name__ == "__main__":
    main()

================
File: src/twat_coding/pystubnik/README.md
================
# pystubnik

A Python package for generating "smart stubs" - an intelligent intermediate representation between full source code and type stubs, optimized for LLM code understanding.

## 1. Overview

Pystubnik creates a "shadow" directory structure that mirrors your Python package, containing smart stubs for all Python files. These smart stubs are designed to be more informative than traditional `.pyi` stub files while being more concise than full source code.

### 1.1. What are Smart Stubs?

Smart stubs are an intermediate representation that includes:
- All function and class signatures with type hints
- All imports (organized and optimized)
- Docstrings (with configurable length limits)
- Important/relevant code sections
- Truncated versions of large data structures and strings
- Simplified function bodies for non-critical code

The verbosity level is automatically adjusted based on the code's importance and complexity.

## 2. Architecture

### 2.1. Backends

#### 2.1.1. AST Backend
- Uses Python's built-in AST module for precise control
- Preserves code structure while reducing verbosity
- Configurable truncation of large literals and sequences
- Maintains type information and docstrings
- Supports Python 3.12+ features (type parameters, etc.)

#### 2.1.2. MyPy Backend
- Leverages mypy's stubgen for type information
- Better type inference capabilities
- Handles special cases (dataclasses, properties)
- Supports type comment extraction

### 2.2. Processors

#### 2.2.1. Import Processor
- Analyzes and organizes imports
- Groups by type (stdlib, third-party, local)
- Handles relative imports
- Detects and removes duplicates

#### 2.2.2. Docstring Processor
- Configurable docstring preservation
- Format detection and conversion
- Type information extraction
- Length-based truncation

#### 2.2.3. Importance Processor
- Scores code elements by importance
- Pattern-based importance detection
- Inheritance-aware scoring
- Configurable filtering

## 3. Usage

```python
from twat_coding.pystubnik import generate_stub, StubGenConfig, PathConfig

# Basic usage
result = generate_stub(
    source_path="path/to/file.py",
    output_path="path/to/output/file.py"
)

# Advanced configuration
config = StubGenConfig(
    paths=PathConfig(
        output_dir="path/to/output",
        doc_dir="path/to/docs",
        search_paths=[],
        modules=[],
        packages=[],
        files=[],
    ),
    processing=ProcessingConfig(
        include_docstrings=True,
        include_private=False,
        include_type_comments=True,
        infer_property_types=True,
        export_less=False,
        importance_patterns={
            r"^test_": 0.5,  # Lower importance for test functions
            r"^main$": 1.0,  # High importance for main functions
        },
    ),
    truncation=TruncationConfig(
        max_sequence_length=4,
        max_string_length=17,
        max_docstring_length=150,
        max_file_size=3_000,
        truncation_marker="...",
    ),
)

result = generate_stub(
    source_path="path/to/file.py",
    output_path="path/to/output/file.py",
    config=config
)

# Process multiple files
from twat_coding.pystubnik import SmartStubGenerator

generator = SmartStubGenerator(
    output_dir="path/to/output",
    include_docstrings=True,
    include_private=False,
    verbose=True,
)

generator.generate()  # Process all configured files
```

## 4. Configuration

Smart stub generation can be customized with various settings:

### 4.1. Path Configuration
- `output_dir`: Directory for generated stubs
- `doc_dir`: Directory containing documentation
- `search_paths`: Module search paths
- `modules`: Module names to process
- `packages`: Package names to process
- `files`: Specific files to process

### 4.2. Processing Configuration
- `include_docstrings`: Whether to include docstrings
- `include_private`: Whether to include private symbols
- `include_type_comments`: Whether to include type comments
- `infer_property_types`: Whether to infer property types
- `export_less`: Whether to minimize exports
- `importance_patterns`: Regex patterns for importance scoring

### 4.3. Truncation Configuration
- `max_sequence_length`: Maximum items in sequences
- `max_string_length`: Maximum length for strings
- `max_docstring_length`: Maximum length for docstrings
- `max_file_size`: File size threshold for truncation
- `truncation_marker`: Marker for truncated content

## 5. Installation

```bash
# Install with all dependencies
pip install twat-coding[pystubnik]

# Install minimal version
pip install twat-coding
```

## 6. Use Cases

1. **LLM Code Understanding**
   - Prepare large codebases for LLM analysis
   - Reduce token usage while maintaining critical information
   - Improve LLM's understanding of code structure

2. **Code Documentation**
   - Generate readable, concise code documentation
   - Maintain type information and signatures
   - Preserve important implementation details

3. **Code Analysis**
   - Quick understanding of large codebases
   - Dependency analysis
   - API surface exploration

4. **Type Stub Generation**
   - Generate enhanced type stubs
   - Better support for documentation tools
   - IDE integration

## 7. Contributing

Contributions are welcome! Please see our contributing guidelines for more information.

## 8. License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: src/twat_coding/__init__.py
================
"""twat-coding package."""

from .__version__ import __version__, version, version_tuple

__all__ = ["__version__", "version", "version_tuple"]

================
File: src/twat_coding/twat_coding.py
================
#!/usr/bin/env python3
"""twat_coding:

Created by Adam Twardoch
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Any

__version__ = "0.1.0"

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class Config:
    """Configuration settings for twat_coding."""

    name: str
    value: str | int | float
    options: dict[str, Any] | None = None


def process_data(
    data: list[Any], config: Config | None = None, *, debug: bool = False
) -> dict[str, Any]:
    """Process the input data according to configuration.

    Args:
        data: Input data to process
        config: Optional configuration settings
        debug: Enable debug mode

    Returns:
        Processed data as a dictionary

    Raises:
        ValueError: If input data is invalid
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")

    if not data:
        msg = "Input data cannot be empty"
        raise ValueError(msg)

    # TODO: Implement data processing logic
    result: dict[str, Any] = {}
    return result


def main() -> None:
    """Main entry point for twat_coding."""
    try:
        # Example usage
        config = Config(name="default", value="test", options={"key": "value"})
        result = process_data([], config=config)
        logger.info("Processing completed: %s", result)

    except Exception as e:
        logger.exception("An error occurred: %s", str(e))
        raise


if __name__ == "__main__":
    main()

================
File: tests/test_package.py
================
"""Test package functionality."""

from pathlib import Path

import pytest

from twat_coding.pystubnik.backends.ast_backend import ASTBackend
from twat_coding.pystubnik.core.config import PathConfig, StubGenConfig


@pytest.mark.asyncio
async def test_ast_backend(tmp_path: Path) -> None:
    """Test AST stub generation."""
    test_file = tmp_path / "test.py"
    test_file.write_text("""
def hello(name: str) -> str:
    \"\"\"Important function.\"\"\"
    return "Hello"
""")
    config = StubGenConfig(paths=PathConfig(output_dir=tmp_path, files=[test_file]))
    backend = ASTBackend(config)
    result = await backend.generate_stub(test_file)
    assert "def hello(name: str) -> str" in result
    assert "Important function" in result


@pytest.mark.asyncio
async def test_docstring_preservation(tmp_path: Path) -> None:
    """Test docstring handling based on importance."""
    test_file = tmp_path / "important.py"
    test_file.write_text("""
def critical_function():
    \"\"\"This is a very important function that should keep its docstring.\"\"\"
    pass

def minor_function():
    \"\"\"This is a less important function that might lose its docstring.\"\"\"
    pass
""")
    config = StubGenConfig(paths=PathConfig(output_dir=tmp_path, files=[test_file]))
    backend = ASTBackend(config)
    result = await backend.generate_stub(test_file)
    assert "This is a very important function" in result


@pytest.mark.asyncio
async def test_type_hints(tmp_path: Path) -> None:
    """Test type hint preservation."""
    test_file = tmp_path / "types.py"
    test_file.write_text("""
from typing import List, Dict, Optional

def process_data(items: List[str], config: Optional[Dict[str, int]] = None) -> bool:
    return True
""")
    config = StubGenConfig(paths=PathConfig(output_dir=tmp_path, files=[test_file]))
    backend = ASTBackend(config)
    result = await backend.generate_stub(test_file)

    # Remove all spaces to handle different formatting styles
    result_no_spaces = result.replace(" ", "")
    expected_no_spaces = "config:Optional[Dict[str,int]]"

    # Check each part of the type hint separately
    assert "def process_data" in result
    assert "items:List[str]" in result_no_spaces
    assert expected_no_spaces in result_no_spaces
    assert "->bool" in result_no_spaces

================
File: .gitignore
================
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]

================
File: cleanup.py
================
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [
#   "ruff>=0.9.6",
#   "pytest>=8.3.4",
#   "mypy>=1.15.0",
# ]
# ///
# this_file: cleanup.py

"""
Cleanup tool for managing repository tasks and maintaining code quality.

This script provides a comprehensive set of commands for repository maintenance:

When to use each command:

- `cleanup.py status`: Use this FIRST when starting work to check the current state
  of the repository. It shows file structure, git status, and runs all code quality
  checks. Run this before making any changes to ensure you're starting from a clean state.

- `cleanup.py venv`: Run this when setting up the project for the first time or if
  your virtual environment is corrupted/missing. Creates a new virtual environment
  using uv.

- `cleanup.py install`: Use after `venv` or when dependencies have changed. Installs
  the package and all development dependencies in editable mode.

- `cleanup.py update`: Run this when you've made changes and want to commit them.
  It will:
  1. Show current status (like `status` command)
  2. Stage and commit any changes with a generic message
  Use this for routine maintenance commits.

- `cleanup.py push`: Run this after `update` when you want to push your committed
  changes to the remote repository.

Workflow Example:
1. Start work: `cleanup.py status`
2. Make changes to code
3. Commit changes: `cleanup.py update`
4. Push to remote: `cleanup.py push`

The script maintains a CLEANUP.log file that records all operations with timestamps.
It also includes content from README.md at the start and TODO.md at the end of logs
for context.

Required Files:
- LOG.md: Project changelog
- README.md: Project documentation
- TODO.md: Pending tasks and future plans
"""

import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import NoReturn

# Configuration
IGNORE_PATTERNS = [
    ".git",
    ".venv",
    "__pycache__",
    "*.pyc",
    "dist",
    "build",
    "*.egg-info",
]
REQUIRED_FILES = ["LOG.md", ".cursor/rules/0project.mdc", "TODO.md"]
LOG_FILE = Path("CLEANUP.log")

# Ensure we're working from the script's directory
os.chdir(Path(__file__).parent)


def new() -> None:
    """Remove existing log file."""
    if LOG_FILE.exists():
        LOG_FILE.unlink()


def prefix() -> None:
    """Write README.md content to log file."""
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)


def suffix() -> None:
    """Write TODO.md content to log file."""
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
        log_message(content)


def log_message(message: str) -> None:
    """Log a message to file and console with timestamp."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"{timestamp} - {message}\n"
    with LOG_FILE.open("a") as f:
        f.write(log_line)


def print_log() -> None:
    """Print the contents of the CLEANUP.log file."""
    if LOG_FILE.exists():
        print(LOG_FILE.read_text())
    else:
        print("Log file does not exist")


def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
    """Run a shell command and return the result."""
    try:
        result = subprocess.run(cmd, check=check, capture_output=True, text=True)
        if result.stdout:
            log_message(result.stdout)
        return result
    except subprocess.CalledProcessError as e:
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        if check:
            raise
        return subprocess.CompletedProcess(cmd, 1, "", str(e))


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in the system."""
    try:
        subprocess.run(["which", cmd], check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError:
        return False


class Cleanup:
    """Main cleanup tool class."""

    def __init__(self) -> None:
        self.workspace = Path.cwd()

    def _print_header(self, message: str) -> None:
        """Print a section header."""
        log_message(f"\n=== {message} ===")

    def _check_required_files(self) -> bool:
        """Check if all required files exist."""
        missing = False
        for file in REQUIRED_FILES:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
                missing = True
        return not missing

    def _generate_tree(self) -> None:
        """Generate and display tree structure of the project."""
        if not check_command_exists("tree"):
            log_message("Warning: 'tree' command not found. Skipping tree generation.")
            return None

        try:
            # Create/overwrite the file with YAML frontmatter
            rules_dir = Path(".cursor/rules")
            rules_dir.mkdir(parents=True, exist_ok=True)
            # Get tree output
            tree_result = run_command(
                ["tree", "-a", "-I", ".git", "--gitignore", "-n", "-h", "-I", "*_cache"]
            )
            tree_text = tree_result.stdout
            # Write frontmatter and tree output to file
            with open(rules_dir / "filetree.mdc", "w") as f:
                f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
                f.write(tree_text)

            # Log the contents
            log_message("\nProject structure:")
            log_message(tree_text)

        except Exception as e:
            log_message(f"Failed to generate tree: {e}")
        return None

    def _git_status(self) -> bool:
        """Check git status and return True if there are changes."""
        result = run_command(["git", "status", "--porcelain"], check=False)
        return bool(result.stdout.strip())

    def _venv(self) -> None:
        """Create and activate virtual environment using uv."""
        log_message("Setting up virtual environment")
        try:
            run_command(["uv", "venv"])
            # Activate the virtual environment
            venv_path = self.workspace / ".venv" / "bin" / "activate"
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                os.environ["PATH"] = (
                    f"{self.workspace / '.venv' / 'bin'}{os.pathsep}{os.environ['PATH']}"
                )
                log_message("Virtual environment created and activated")
            else:
                log_message("Virtual environment created but activation failed")
        except Exception as e:
            log_message(f"Failed to create virtual environment: {e}")

    def _install(self) -> None:
        """Install package in development mode with all extras."""
        log_message("Installing package with all extras")
        try:
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
        except Exception as e:
            log_message(f"Failed to install package: {e}")

    def _run_checks(self) -> None:
        """Run code quality checks using ruff and pytest."""
        log_message("Running code quality checks")

        try:
            # Run ruff checks
            log_message(">>> Running code fixes...")
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "check",
                    "--fix",
                    "--unsafe-fixes",
                    "src",
                    "tests",
                ],
                check=False,
            )
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "format",
                    "--respect-gitignore",
                    "src",
                    "tests",
                ],
                check=False,
            )

            # Run type checks
            log_message(">>>Running type checks...")
            run_command(["python", "-m", "mypy", "src", "tests"], check=False)

            # Run tests
            log_message(">>> Running tests...")
            run_command(["python", "-m", "pytest", "tests"], check=False)

            log_message("All checks completed")
        except Exception as e:
            log_message(f"Failed during checks: {e}")

    def status(self) -> None:
        """Show current repository status: tree structure, git status, and run checks."""
        prefix()  # Add README.md content at start
        self._print_header("Current Status")

        # Check required files
        self._check_required_files()

        # Show tree structure
        self._generate_tree()

        # Show git status
        result = run_command(["git", "status"], check=False)
        log_message(result.stdout)

        # Run additional checks
        self._print_header("Environment Status")
        self._venv()
        self._install()
        self._run_checks()

        suffix()  # Add TODO.md content at end

    def venv(self) -> None:
        """Create and activate virtual environment."""
        self._print_header("Virtual Environment Setup")
        self._venv()

    def install(self) -> None:
        """Install package with all extras."""
        self._print_header("Package Installation")
        self._install()

    def update(self) -> None:
        """Show status and commit any changes if needed."""
        # First show current status
        self.status()

        # Then handle git changes if any
        if self._git_status():
            log_message("Changes detected in repository")
            try:
                # Add all changes
                run_command(["git", "add", "."])
                # Commit changes
                commit_msg = "Update repository files"
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
            except Exception as e:
                log_message(f"Failed to commit changes: {e}")
        else:
            log_message("No changes to commit")

    def push(self) -> None:
        """Push changes to remote repository."""
        self._print_header("Pushing Changes")
        try:
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
        except Exception as e:
            log_message(f"Failed to push changes: {e}")


def print_usage() -> None:
    """Print usage information."""
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")


def main() -> NoReturn:
    """Main entry point."""
    new()  # Clear log file

    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)

    command = sys.argv[1]
    cleanup = Cleanup()

    try:
        if command == "status":
            cleanup.status()
        elif command == "venv":
            cleanup.venv()
        elif command == "install":
            cleanup.install()
        elif command == "update":
            cleanup.update()
        elif command == "push":
            cleanup.push()
        else:
            print_usage()
            sys.exit(1)
    except Exception as e:
        log_message(f"Error: {e}")
        sys.exit(1)

    print_log()  # Print the log contents at the end
    sys.exit(0)


if __name__ == "__main__":
    main()

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: MANIFEST.in
================
# This MANIFEST.in file ensures that all necessary files are included in the package distribution.
recursive-include src/twat_coding/data *
include src/twat_coding/py.typed

================
File: mypy.ini
================
[mypy]
python_version = 3.12
ignore_missing_imports = True
disallow_untyped_defs = True
warn_return_any = True
warn_unused_configs = True
check_untyped_defs = True
disallow_incomplete_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True

[mypy-tests.*]
disallow_untyped_defs = False

================
File: package.toml
================
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows

================
File: pyproject.toml
================
# this_file: pyproject.toml
# this_project: twat-coding

[project]
name = "twat_coding"
dynamic = ["version"]
description = "A Python package for generating type stubs"
authors = [
    { name = "Adam Twardoch", email = "adam@twardoch.com" }
]
dependencies = [
    "pydantic>=2.6.1",
    "pydantic-settings>=2.1.0",
    "loguru>=0.7.2",
    "rich>=13.7.0",
    "click>=8.1.7",
    "psutil>=5.9.8",
    "memory-profiler>=0.61.0",
    "docstring-parser>=0.15",
    "mypy>=1.8.0",
    "black>=24.1.1",
    "isort>=5.13.2",
    # File importance analysis dependencies
    "networkx>=3.2.1",     # For import graph analysis
    "radon>=6.0.1",       # For code complexity metrics
    "coverage>=7.4.1",    # For test coverage analysis
    "pydocstyle>=6.3.0", # For docstring quality checks
    "importlab>=0.8",    # For import graph building
    "toml>=0.10.2",      # For pyproject.toml parsing
    "types-toml>=0.10.8.7",    # Type stubs for toml
    "mypy-extensions>=1.0.0",  # Additional type system features
]
requires-python = ">=3.12"
readme = "README.md"
license = { text = "Apache-2.0" }
keywords = [
    "twat",
    "coding",
    "development"
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]

[project.optional-dependencies]
test = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.23.5",
    "pytest-benchmark>=4.0.0",
    "pytest-golden>=0.2.2",
    "pytest-memray>=1.5.0",
]
dev = [
    "black>=24.1.1",
    "ruff>=0.2.1",
    "mypy>=1.8.0",
    "pre-commit>=3.6.0",
    "python-semantic-release>=8.7.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
target-version = "py312"
line-length = 88
fix = true
unsafe-fixes = true
output-format = "github"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
]
ignore = []

[tool.ruff.lint.isort]
known-first-party = ["twat_coding"]
combine-as-imports = true

[tool.black]
line-length = 88
target-version = ["py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.pytest.ini_options]
minversion = "8.0"
addopts = "-ra -q --cov=twat_coding"
testpaths = ["tests"]

[project.scripts]
twat-coding = "twat_coding.__main__:main"

[project.urls]
Homepage = "https://github.com/twardoch/twat-coding"
Documentation = "https://github.com/twardoch/twat-coding#readme"
Issues = "https://github.com/twardoch/twat-coding/issues"
Source = "https://github.com/twardoch/twat-coding"

[tool.coverage.paths]
twat_coding = ["src/twat_coding", "*/twat-coding/src/twat_coding"]
tests = ["tests", "*/twat-coding/tests"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]
ignore_errors = true
omit = [
    "tests/*",
    "setup.py",
    "src/twat_coding/__about__.py",
]

[tool.hatch.build.targets.wheel]
packages = ["src/twat_coding"]
artifacts = [
    "src/twat_coding/py.typed"
]

[tool.hatch.envs.default]
dependencies = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov {args:tests}"
cov-report = ["- coverage combine", "coverage report"]
cov = ["test-cov", "cov-report"]
lint = ["ruff check src/twat_coding tests", "ruff format --respect-gitignore src/twat_coding tests"]
fix = ["ruff check  --fix --unsafe-fixes src/twat_coding tests", "ruff format --respect-gitignore src/twat_coding tests"]

[tool.hatch.envs.lint]
dependencies = [
    "black>=24.1.1",
    "mypy>=1.8.0",
    "ruff>=0.2.1",
]

[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_coding tests}"
style = ["ruff check {args:.}", "ruff format --check {args:.}"]
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
all = ["style", "typing"]

[tool.hatch.envs.test]
dependencies = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
]

[tool.hatch.version]
path = "src/twat_coding/__version__.py"

[tool.pytest-benchmark]
warmup = true

[project.entry-points."twat.plugins"]
coding = "twat_coding"

================
File: README.md
================
# twat-coding

A collection of coding-related tools and utilities, designed to enhance code understanding and analysis.

## Packages

### pystubnik

A sophisticated tool for generating "smart stubs" - an intelligent intermediate representation between full source code and type stubs, optimized for LLM code understanding. Smart stubs preserve essential code structure, types, and documentation while reducing verbosity, making large codebases more digestible for LLMs.

Key features:
- Multiple backends (AST and MyPy) for comprehensive stub generation
- Smart import analysis and organization
- Configurable docstring processing and formatting
- Importance-based code filtering
- Support for Python 3.12+ features

[Learn more about pystubnik](src/twat_coding/pystubnik/README.md)

## Installation

```bash
# Install the base package
pip install twat-coding

# Install with pystubnik support
pip install twat-coding[pystubnik]
```

## Development

This project uses modern Python features and follows best practices:
- Type hints and dataclasses for robust data structures
- Protocol-based interfaces for flexibility
- Comprehensive error handling and logging
- Parallel processing for performance

## Contributing

Contributions are welcome! Please see our contributing guidelines for more information.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: TODO.md
================
---
this_file: TODO.md
---

# TODO

## 1. Immediate Priorities

- [!] **Fix Critical Type Errors**
  - *Problem*: Multiple type errors in core files affecting functionality
  - *Details*:
    1. In `docstring.py`: Name-defined errors for `key_info.annotation` and `value_info.annotation`
    2. In `__init__.py`: StubConfig initialization missing required arguments
    3. In `__init__.py`: Type mismatch between StubConfig and StubGenConfig in backend initialization
  - *Solution*:
    1. Fix dictionary type handling in `docstring.py`
    2. Update StubConfig initialization with all required parameters
    3. Implement proper type conversion between StubConfig and StubGenConfig

- [!] **Improve Test Coverage**
  - *Current*: 37% (target: 50%)
  - *Zero Coverage Modules*:
    1. `stub_generation.py` (0%)
    2. `type_inference.py` (0%)
    3. `read_imports.py` (0%)
    4. `twat_coding.py` (0%)
  - *Low Coverage Modules*:
    1. `imports.py` (15%)
    2. `core/utils.py` (19%)
    3. `file_importance.py` (21%)
    4. `type_system.py` (28%)

- [!] **Integrate File Importance**
  - *Status*: Core functionality ready, needs integration
  - *Next Steps*:
    1. Update `StubResult` to store importance metadata
    2. Modify `SignatureExtractor` to use importance scores
    3. Implement importance-based docstring preservation
    4. Add tests for importance-based stub generation

## 2. Future Tasks

- [ ] **Documentation Updates**
  - Update README.md with clear usage examples
  - Add docstrings to key classes and functions
  - Create example files demonstrating features

- [ ] **Code Cleanup**
  - Refactor complex functions in `ast_backend.py`
  - Simplify configuration system
  - Remove redundant code in processors

- [ ] **Performance Optimization**
  - Profile stub generation performance
  - Optimize memory usage in AST processing
  - Improve concurrent processing

## 3. Completed Tasks

- [x] **Core AST Backend Implementation**
  - Basic stub generation functionality
  - AST transformation logic
  - Error handling

- [x] **Project Structure**
  - Basic project layout
  - Module organization
  - Package configuration

- [x] **Type System Improvements**
  - Created `shared_types.py` for common types
  - Defined `TruncationConfig` as a shared type
  - Fixed type compatibility in `ast_utils.py`

## 4. Notes

- Focus on fixing type errors first as they affect core functionality
- Then improve test coverage to ensure stability
- Complete file importance integration to enable smart stub generation
- Run `./cleanup.py status` to check progress
- Use `uv pip` instead of `pip` for package management
- Ignore minor linting issues (line length) for now

Before you do any changes or if I say "cleanup", run the `./cleanup.py install && source .venv/bin/activate && ./cleanup.py update | cat` script in the main folder. Analyze the results, describe recent changes @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `./cleanup.py update | cat` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done, and `- [!]` if they're NEXT TODO.

Don't use `pip`, use `uv pip`. Ignore minor problems like "line too long", FOCUS ON MAJOR PROBLEMS!

---

Hey, junior dev! This `TODO.md` is your step-by-step guide to get `pystubnik` ready for its 1.0 release—a rock-solid tool for generating smart stubs. We're keeping it simple: one backend (AST), smart file importance, basic tests, and clear docs. Every task below is broken down so you can nail it. If you get stuck, run `./cleanup.py status`, check `CLEANUP.log`, and ask for help. Let's make this awesome together!

## 5. Core Functionality

- [x] **Lock Down the AST Backend**
  - *Status*: It's in `src/twat_coding/pystubnik/backends/ast_backend.py` and works!
  - *Why*: This is our stub-generating powerhouse for 1.0—no MyPy or hybrid yet (but keep them around)

- [!] **Fully Integrate File Importance**
  - *What*: Use importance scores to control stub detail (e.g., keep docstrings for important files).
  - *Why*: This is the "smart" in smart stubs—more detail where it matters.
  - *How*:
    1. Open `src/twat_coding/pystubnik/core/types.py`.
    2. Update `StubResult` to store metadata:
       ```python
       @dataclass
       class StubResult:
           source_path: Path
           stub_content: str
           imports: list[ImportInfo]
           errors: list[str]
           importance_score: float = 0.0
           metadata: dict[str, Any] = field(default_factory=dict)  # Add this
       ```
    3. Open `src/twat_coding/pystubnik/backends/ast_backend.py`.
    4. In `SignatureExtractor.__init__`, add importance:
       ```python
       def __init__(self, config: StubGenConfig, file_size: int = 0, importance_score: float = 1.0):
           super().__init__()
           self.config = config
           self.file_size = file_size
           self.importance_score = importance_score  # Add this
       ```
    5. Update `_preserve_docstring` to use importance:
       ```python
       def _preserve_docstring(self, body: list[ast.stmt]) -> list[ast.stmt]:
           if not body:
               return []
           if self.file_size > self.config.truncation.max_file_size and self.importance_score < 0.7:
               return []  # Skip docstrings for big, low-importance files
           match body[0]:
               case ast.Expr(value=ast.Constant(value=str() as docstring)):
                   if len(docstring) > self.config.truncation.max_docstring_length and self.importance_score < 0.9:
                       return []  # Skip long docstrings unless very important
                   return [body[0]]
               case _:
                   return []
       ```
    6. In `ASTBackend.generate_stub`, pass the score:
       ```python
       async def generate_stub(self, source_path: Path) -> str:
           # ... existing code ...
           # After getting file_score elsewhere or calculating it:
           file_score = 0.5  # Placeholder; we'll get this from ImportanceProcessor
           tree = await self._run_in_executor(
               functools.partial(ast.parse, source, filename=str(source_path))
           )
           attach_parents(tree)
           transformer = SignatureExtractor(self.config.stub_gen_config, len(source), file_score)
           transformed = transformer.visit(tree)
           stub_content = ast.unparse(transformed)
           # ... rest of the method ...
       ```
    7. Open `src/twat_coding/pystubnik/processors/importance.py`.
    8. Update `process` to set the score:
       ```python
       def process(self, stub_result: StubResult) -> StubResult:
           package_dir = str(stub_result.source_path.parent)
           self._file_scores = prioritize_files(package_dir, self.config.file_importance)
           file_score = self._file_scores.get(str(stub_result.source_path), 0.5)
           stub_result.importance_score = file_score
           stub_result.metadata["file_score"] = file_score
           # Process individual symbols
           tree = ast.parse(stub_result.stub_content)
           for node in ast.walk(tree):
               if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                   docstring = ast.get_docstring(node)
                   score = self.calculate_importance(
                       name=node.name,
                       docstring=docstring,
                       is_public=not node.name.startswith('_'),
                       is_special=node.name.startswith('__') and node.name.endswith('__'),
                       extra_info={"file_path": stub_result.source_path}
                   )
                   stub_result.metadata[f"{node.name}_score"] = score
                   if score < 0.7 and isinstance(node, ast.FunctionDef):
                       node.body = [ast.Expr(value=ast.Constant(value=Ellipsis))]
           stub_result.stub_content = ast.unparse(tree)
           return stub_result
       ```
    9. Test it:
       - Run `./cleanup.py install && source .venv/bin/activate`.
       - Run `python -m twat_coding.pystubnik generate --files src/twat_coding/pystubnik/__init__.py`.
       - Check `out/__init__.pyi`—important functions should keep docstrings, others should simplify.

- [ ] **Simplify Configuration**
  - *What*: Trim `StubGenConfig` to essentials for 1.0.
  - *Why*: Less complexity for users and you!
  - *How*:
    1. Open `src/twat_coding/pystubnik/core/config.py`.
    2. Replace `StubGenConfig` with:
       ```python
       @dataclass(frozen=True)
       class StubGenConfig:
           output_dir: Path = Path("out")
           files: Sequence[Path] = field(default_factory=list)
           include_docstrings: bool = True
           max_docstring_length: int = 150
           ignore_errors: bool = True
       ```
    3. Update `SmartStubGenerator.__init__` in `__init__.py`:
       ```python
       def __init__(
           self,
           output_dir: str | Path = "out",
           files: Sequence[str | Path] = (),
           include_docstrings: bool = True,
           max_docstring_length: int = 150,
           ignore_errors: bool = True,
       ):
           self.config = StubGenConfig(
               output_dir=Path(output_dir),
               files=[Path(f) for f in files],
               include_docstrings=include_docstrings,
               max_docstring_length=max_docstring_length,
               ignore_errors=ignore_errors,
           )
           # ... rest of init ...
       ```
    4. Fix any import errors in other files (e.g., `ast_backend.py`).

## 6. Testing

- [!] **Add Core Tests**
  - *What*: Test stub generation and importance logic.
  - *Why*: We need 50% coverage for trust—start here.
  - *How*:
    1. Replace `tests/test_package.py` with:
       ```python
       import pytest
       from twat_coding.pystubnik import SmartStubGenerator, StubResult
       from twat_coding.pystubnik.backends.ast_backend import ASTBackend
       from twat_coding.pystubnik.processors.importance import ImportanceProcessor
       from pathlib import Path

       def test_ast_backend(tmp_path):
           """Test AST stub generation."""
           test_file = tmp_path / "test.py"
           test_file.write_text('''"""
           Important file
           """
           def hello(name: str) -> str:
               return "Hello"
           ''')
           config = StubGenConfig(output_dir=tmp_path, files=[test_file])
           backend = ASTBackend(config)
           result = backend.generate_stub(test_file).result()  # Async call
           stub_file = tmp_path / "test.pyi"
           assert stub_file.exists()
           content = stub_file.read_text()
           assert "def hello(name: str) -> str" in content
           assert '"""Important file"""' in content  # Docstring preserved

       def test_importance_processor(tmp_path):
           """Test importance scoring affects stubs."""
           test_file = tmp_path / "low.py"
           test_file.write_text('''def minor():
               """Minor function"""
               pass
           ''')
           config = StubGenConfig(output_dir=tmp_path, files=[test_file])
           generator = SmartStubGenerator(output_dir=tmp_path, files=[str(test_file)])
           generator.generate()
           stub_file = tmp_path / "low.pyi"
           content = stub_file.read_text()
           assert "def minor(): ..." in content  # Low importance, no docstring

       def test_cli(tmp_path):
           """Test CLI end-to-end."""
           test_file = tmp_path / "cli.py"
           test_file.write_text("def run(): pass")
           import subprocess
           subprocess.run([
               "python", "-m", "twat_coding.pystubnik", "generate",
               "--files", str(test_file), "--output-dir", str(tmp_path)
           ], check=True)
           assert (tmp_path / "cli.pyi").exists()
       ```
    2. Run `./cleanup.py install && source .venv/bin/activate && pytest tests/`.
    3. Fix failures (e.g., add `asyncio.run()` around async calls if needed).

- [ ] **Boost Coverage to 50%**
  - *What*: Add tests for key modules.
  - *Why*: Ensures stuff doesn't break.
  - *How*:
    1. Run `pytest --cov=src/twat_coding tests/`.
    2. Create `tests/test_processors.py`:
       ```python
       def test_import_processor(tmp_path):
           from twat_coding.pystubnik.processors.imports import ImportProcessor
           processor = ImportProcessor()
           result = StubResult(tmp_path / "test.py", "import os\n", [], [])
           processed = processor.process(result)
           assert "import os" in processed.stub_content
       ```
    3. Add more tests for `docstring.py`, `ast_utils.py` based on coverage gaps.
    4. Rerun until coverage hits 50%.

## 7. Documentation

- [!] **Revamp README.md**
  - *What*: Make it user-ready with usage and example.
  - *Why*: First impression for adopters.
  - *How*:
    1. Open `README.md`.
    2. Replace "Usage" section with:
       ```markdown
       ## 8. Usage

       Generate smart stubs with a simple command:

       ```bash
       python -m twat_coding.pystubnik generate --files my_script.py --output-dir stubs
       ```

       Or in Python:

       ```python
       from twat_coding.pystubnik import SmartStubGenerator

       generator = SmartStubGenerator(
           output_dir="stubs",
           files=["my_script.py"],
           include_docstrings=True
       )
       generator.generate()
       ```

       ### 8.1. Example

       **Input** (`example.py`):
       ```python
       def greet(name: str) -> str:
           """Say hello to someone important."""
           return f"Hello, {name}"
       ```

       **Output** (`stubs/example.pyi`):
       ```python
       def greet(name: str) -> str:
           """Say hello to someone important."""
           ...
       ```
       ```
    3. Save and check with `cat README.md`.

- [ ] **Add Example File**
  - *What*: A real file to demo.
  - *Why*: Users can try it themselves.
  - *How*:
    1. Create `examples/greet.py`:
       ```python
       def greet(name: str) -> str:
           """Say hello to someone important."""
           return f"Hello, {name}"
       ```
    2. Test it: `python -m twat_coding.pystubnik generate --files examples/greet.py`.

- [ ] **Update CLI Help**
  - *What*: Add clear CLI options.
  - *Why*: Makes it user-friendly.
  - *How*:
    1. Open `src/twat_coding/pystubnik/__init__.py`.
    2. Add a CLI entry point:
       ```python
       import click

       @click.command()
       @click.option("--files", multiple=True, type=click.Path(exists=True), help="Files to process")
       @click.option("--output-dir", default="out", type=click.Path(), help="Output directory")
       @click.option("--include-docstrings", is_flag=True, default=True, help="Keep docstrings")
       def generate(files, output_dir, include_docstrings):
           """Generate smart stubs for Python files."""
           generator = SmartStubGenerator(
               output_dir=output_dir,
               files=files,
               include_docstrings=include_docstrings
           )
           generator.generate()

       if __name__ == "__main__":
           generate()
       ```
    3. Test: `python -m twat_coding.pystubnik --help`.

## 9. Code Quality

- [!] **Fix Type Errors**
  - *What*: Clear all `mypy` issues.
  - *Why*: Clean code = fewer bugs.
  - *How*:
    1. Run `./cleanup.py install && source .venv/bin/activate && mypy src/`.
    2. Fix errors:
       - `type_system.py`: Replace `issubclass` with:
         ```python
         if isinstance(resolved, type) and hasattr(resolved, "__mro__") and issubclass(resolved, Protocol):
         ```
       - Follow old `TODO.md` section 2 for others.
    3. Rerun until no errors.

- [ ] **Refactor Complex Functions**
  - *What*: Simplify `truncate_literal` and `calculate_importance`.
  - *Why*: Easier for you to maintain.
  - *How*:
    1. In `ast_backend.py`, split `truncate_literal`:
       ```python
       def truncate_string(self, s: str) -> str:
           return s if len(s) <= self.config.truncation.max_string_length else f"{s[:self.config.truncation.max_string_length]}..."

       def truncate_literal(self, node: ast.AST) -> ast.AST:
           if isinstance(node, ast.Constant) and isinstance(node.value, str):
               return ast.Constant(value=self.truncate_string(node.value))
           # ... other cases ...
       ```
    2. In `importance.py`, break down `calculate_importance`:
       ```python
       def _score_by_name(self, name: str) -> float:
           score = 1.0
           for pattern, weight in self.config.patterns.items():
               if re.search(pattern, name):
                   score *= weight
           return score

       def calculate_importance(self, name: str, docstring: str | None = None, ...):
           file_score = self._get_file_score(extra_info)
           pattern_score = self._score_by_name(name)
           # ... combine scores ...
       ```

- [x] **Remove Redundant Scripts**
  - *What*: Delete old standalone files DONE
  - *Why*: Keeps the codebase clean.

## 10. Release Preparation

- [ ] **Finalize 1.0**
  - *What*: Version it, log it, ship it.
  - *Why*: Makes it official!
  - *How*:
    1. In `src/twat_coding/__version__.py`, set:
       ```python
       version = "1.0.0"
       ```
    2. Run `./cleanup.py update && ./cleanup.py push`.

## 11. Next Actions

- [!] Integrate file importance fully (step 1.2).
- [!] Add core tests (step 2.1).
- [!] Revamp README.md (step 3.1).
- [!] Fix type errors (step 4.1).

## 12. Notes

- Focus on AST only—MyPy's for later.
- Test every change with `./cleanup.py status`.
- You're doing great—let's ship this!



================================================================
End of Codebase
================================================================
