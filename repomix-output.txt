This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: _private, .specstory
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
    quality.mdc
.github/
  workflows/
    push.yml
    release.yml
cursor/
  rules/
    0project.mdc
src/
  twat_coding/
    pystubnik/
      backends/
        __init__.py
        ast_backend.py
        base.py
        mypy_backend.py
      core/
        config.py
        types.py
        utils.py
      processors/
        __init__.py
        docstring.py
        file_importance.py
        importance.py
        imports.py
        stub_generation.py
        type_inference.py
      types/
        docstring.py
        type_system.py
      utils/
        ast_utils.py
        display.py
        memory.py
      __init__.py
      config.py
      errors.py
      make_stubs_ast.py
      make_stubs_mypy.py
      pypacky2.sh
      read_imports.py
      README.md
    __init__.py
    twat_coding.py
tests/
  test_package.py
.gitignore
.pre-commit-config.yaml
cleanup.py
LICENSE
MANIFEST.in
mypy.ini
package.toml
pyproject.toml
README.md
TODO.md

================================================================
Files
================================================================

================
File: .cursor/rules/0project.mdc
================
---
description: About this project
globs: 
---
---
name: About this project
key: 0project
---

`twat-coding` is a Python toolkit focused on code analysis and transformation. Its main package `pystubnik` generates "smart stubs" - a hybrid between full source code and type stubs that helps LLMs understand large codebases efficiently. Smart stubs preserve essential code structure (signatures, types, imports) while intelligently reducing verbosity based on code importance.

## 1. Overview

Pystubnik creates a "shadow" directory structure that mirrors your Python package, containing smart stubs for all Python files. These smart stubs are designed to be more informative than traditional `.pyi` stub files while being more concise than full source code.

### 1.1. What are Smart Stubs?

Smart stubs are an intermediate representation that includes:
- All function and class signatures with type hints
- All imports (organized and optimized)
- Docstrings (with configurable length limits)
- Important/relevant code sections
- Truncated versions of large data structures and strings
- Simplified function bodies for non-critical code

The verbosity level is automatically adjusted based on the code's importance and complexity.

## 2. Architecture

### 2.1. Backends

#### 2.1.1. AST Backend
- Uses Python's built-in AST module for precise control
- Preserves code structure while reducing verbosity
- Configurable truncation of large literals and sequences
- Maintains type information and docstrings
- Supports Python 3.12+ features (type parameters, etc.)

#### 2.1.2. MyPy Backend
- Leverages mypy's stubgen for type information
- Better type inference capabilities
- Handles special cases (dataclasses, properties)
- Supports type comment extraction

### 2.2. Processors

#### 2.2.1. Import Processor
- Analyzes and organizes imports
- Groups by type (stdlib, third-party, local)
- Handles relative imports
- Detects and removes duplicates

#### 2.2.2. Docstring Processor
- Configurable docstring preservation
- Format detection and conversion
- Type information extraction
- Length-based truncation

#### 2.2.3. Importance Processor
- Scores code elements by importance
- Pattern-based importance detection
- Inheritance-aware scoring
- Configurable filtering

================
File: .cursor/rules/cleanup.mdc
================
---
description: Run `cleanup.py` script before and after changes
globs: 
---
Before you do any changes or if I say "cleanup", run the `./cleanup.py install && source .venv/bin/activate && ./cleanup.py update | cat` script in the main folder. Analyze the results, describe recent changes @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `./cleanup.py update | cat` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done, and `- [!]` if they're NEXT TODO. 

Don't use `pip`, use `uv pip`.

================
File: .cursor/rules/filetree.mdc
================
---
description: File tree of the project
globs: 
---
[ 960]  .
├── [  64]  .benchmarks
├── [  96]  .cursor
│   └── [ 224]  rules
│       ├── [2.2K]  0project.mdc
│       ├── [ 632]  cleanup.mdc
│       ├── [3.8K]  filetree.mdc
│       └── [2.0K]  quality.mdc
├── [  96]  .github
│   └── [ 128]  workflows
│       ├── [2.7K]  push.yml
│       └── [1.4K]  release.yml
├── [3.5K]  .gitignore
├── [ 532]  .pre-commit-config.yaml
├── [  96]  .specstory
│   └── [ 448]  history
│       ├── [2.0K]  .what-is-this.md
│       ├── [ 63K]  adjusting-todo-priorities-from-cleanup-status.md
│       ├── [ 79K]  cleanup-script-execution-and-todo-management.md
│       ├── [333K]  command-execution-and-todo-update.md
│       ├── [ 248]  detailed-overview-of-python-stubbing-tools.md
│       ├── [ 25K]  integrating-importance-analysis-modules.md
│       ├── [3.0K]  overview-of-python-tools-and-their-functions.md
│       ├── [ 58K]  project-directory-structure-for-python-package.md
│       ├── [ 64K]  reviewing-and-tracking-todo-progress.md
│       ├── [ 56K]  task-management-from-todo-md.md
│       ├── [4.9K]  task-organization-and-cleanup-process.md
│       └── [165K]  todo-list-cleanup-and-review.md
├── [1.0K]  LICENSE
├── [ 173]  MANIFEST.in
├── [1.4K]  README.md
├── [ 12K]  TODO.md
├── [ 12K]  cleanup.py
├── [  96]  cursor
│   └── [  96]  rules
│       └── [  31]  0project.mdc
├── [  96]  dist
│   └── [   1]  .gitkeep
├── [ 305]  mypy.ini
├── [ 426]  package.toml
├── [4.8K]  pyproject.toml
├── [215K]  repomix-output.txt
├── [ 128]  src
│   └── [ 256]  twat_coding
│       ├── [ 144]  __init__.py
│       ├── [ 480]  pystubnik
│       │   ├── [5.3K]  README.md
│       │   ├── [ 14K]  __init__.py
│       │   ├── [ 192]  backends
│       │   │   ├── [1.3K]  __init__.py
│       │   │   ├── [ 12K]  ast_backend.py
│       │   │   ├── [2.1K]  base.py
│       │   │   └── [1.1K]  mypy_backend.py
│       │   ├── [9.5K]  config.py
│       │   ├── [ 160]  core
│       │   │   ├── [4.9K]  config.py
│       │   │   ├── [3.9K]  types.py
│       │   │   └── [8.3K]  utils.py
│       │   ├── [5.1K]  errors.py
│       │   ├── [ 14K]  make_stubs_ast.py
│       │   ├── [ 11K]  make_stubs_mypy.py
│       │   ├── [ 288]  processors
│       │   │   ├── [1.2K]  __init__.py
│       │   │   ├── [5.0K]  docstring.py
│       │   │   ├── [ 16K]  file_importance.py
│       │   │   ├── [6.4K]  importance.py
│       │   │   ├── [6.6K]  imports.py
│       │   │   ├── [ 11K]  stub_generation.py
│       │   │   └── [7.4K]  type_inference.py
│       │   ├── [2.0K]  pypacky2.sh
│       │   ├── [1.6K]  read_imports.py
│       │   ├── [ 128]  types
│       │   │   ├── [8.4K]  docstring.py
│       │   │   └── [8.2K]  type_system.py
│       │   └── [ 160]  utils
│       │       ├── [5.3K]  ast_utils.py
│       │       ├── [1.8K]  display.py
│       │       └── [5.2K]  memory.py
│       └── [1.6K]  twat_coding.py
└── [ 128]  tests
    └── [ 165]  test_package.py

20 directories, 61 files

================
File: .cursor/rules/quality.mdc
================
---
description: Quality
globs: 
---
- **Verify Information**: Always verify information before presenting it. Do not make assumptions or speculate without clear evidence.
- **No Apologies**: Never use apologies.
- **No Whitespace Suggestions**: Don't suggest whitespace changes.
- **No Inventions**: Don't invent major changes other than what's explicitly requested.
- **No Unnecessary Confirmations**: Don't ask for confirmation of information already provided in the context.
- **Preserve Existing Code**: Don't remove unrelated code or functionalities. Pay attention to preserving existing structures.
- **No Implementation Checks**: Don't ask the user to verify implementations that are visible in the provided context.
- **No Unnecessary Updates**: Don't suggest updates or changes to files when there are no actual modifications needed.
- **No Current Implementation**: Don't show or discuss the current implementation unless specifically requested.
- **Use Explicit Variable Names**: Prefer descriptive, explicit variable names over short, ambiguous ones to enhance code readability.
- **Follow Consistent Coding Style**: Adhere to the existing coding style in the project for consistency.
- **Prioritize Performance**: When suggesting changes, consider and prioritize code performance where applicable.
- **Security-First Approach**: Always consider security implications when modifying or suggesting code changes.
- **Test Coverage**: Suggest or include appropriate unit tests for new or modified code.
- **Error Handling**: Implement robust error handling and logging where necessary.
- **Modular Design**: Encourage modular design principles to improve code maintainability and reusability.
- **Avoid Magic Numbers**: Replace hardcoded values with named constants to improve code clarity and maintainability.
- **Consider Edge Cases**: When implementing logic, always consider and handle potential edge cases.
- **Use Assertions**: Include assertions wherever possible to validate assumptions and catch potential errors early.

================
File: .github/workflows/push.yml
================
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_coding --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5

================
File: .github/workflows/release.yml
================
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-coding
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

================
File: cursor/rules/0project.mdc
================
---
description: 
globs: 
---

================
File: src/twat_coding/pystubnik/backends/__init__.py
================
"""Backend interface and registry for stub generation.

This module provides the abstract base class for stub generation backends
and a registry to manage different backend implementations.
"""

from abc import abstractmethod
from pathlib import Path
from typing import Protocol

from pystubnik.core.types import StubResult


class StubBackend(Protocol):
    """Protocol defining the interface for stub generation backends."""

    @abstractmethod
    def generate_stub(
        self, source_path: Path, output_path: Path | None = None
    ) -> StubResult:
        """Generate stub for the given source file.

        Args:
            source_path: Path to the source file
            output_path: Optional path to write the stub file

        Returns:
            StubResult containing the generated stub and metadata
        """
        ...


_backends: dict[str, type[StubBackend]] = {}


def register_backend(name: str, backend: type[StubBackend]) -> None:
    """Register a new backend implementation."""
    _backends[name] = backend


def get_backend(name: str) -> type[StubBackend]:
    """Get a registered backend by name."""
    if name not in _backends:
        msg = f"Backend '{name}' not found"
        raise KeyError(msg)
    return _backends[name]


def list_backends() -> list[str]:
    """List all registered backend names."""
    return list(_backends.keys())

================
File: src/twat_coding/pystubnik/backends/ast_backend.py
================
#!/usr/bin/env -S uv run
"""AST-based stub generation backend.

This module implements stub generation using Python's ast module,
based on the original make_stubs_ast.py implementation.
"""

import ast
import asyncio
import functools
import hashlib
import weakref
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from pathlib import Path
from typing import Any, ClassVar

from loguru import logger
from pystubnik.backends import StubBackend
from pystubnik.core.config import StubGenConfig, TruncationConfig

from ..config import StubConfig
from ..errors import ASTError, ErrorCode
from ..utils.ast_utils import attach_parents, truncate_literal
from ..utils.display import print_progress
from ..utils.memory import MemoryMonitor, stream_process_ast


@dataclass
class ASTCacheEntry:
    """Cache entry for parsed AST nodes."""

    node: ast.AST
    source_hash: str
    access_time: float


class SignatureExtractor(ast.NodeTransformer):
    """Transform AST to preserve signatures while truncating implementation details.

    This transformer preserves:
    - imports
    - docstrings (subject to length constraints)
    - function & method signatures
    - class-level assignments
    But replaces function bodies with an ellipsis.
    """

    def __init__(self, config: StubGenConfig, file_size: int = 0):
        super().__init__()
        self.config = config
        self.file_size = file_size

    def _preserve_docstring(self, body: list[ast.stmt]) -> list[ast.stmt]:
        """If the first statement is a docstring, keep it if it meets size constraints.

        Args:
            body: List of statements to check for docstring

        Returns:
            List containing the docstring statement if it should be preserved,
            otherwise an empty list
        """
        if not body:
            return []

        # Skip all docstrings if file is too large
        if self.file_size > self.config.truncation.max_file_size:
            return []

        match body[0]:
            case ast.Expr(value=ast.Constant(value=str() as docstring)):
                # Skip if docstring is too long
                if len(docstring) > self.config.truncation.max_docstring_length:
                    return []
                return [body[0]]
            case _:
                return []

    def _make_ellipsis_expr(self, node: ast.AST, indent: int = 0) -> ast.Expr:
        """Create an ellipsis node with the same location offsets.

        Args:
            node: Node to copy location from
            indent: Additional indentation to add

        Returns:
            Ellipsis expression node
        """
        return ast.Expr(
            value=ast.Constant(value=Ellipsis),
            lineno=getattr(node, "lineno", 1),
            col_offset=getattr(node, "col_offset", 0) + indent,
        )

    def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.FunctionDef:
        """Keep function signature and docstring, replace body with ellipsis."""
        preserved_doc = self._preserve_docstring(node.body)
        ellipsis_body = [self._make_ellipsis_expr(node, indent=4)]
        return ast.FunctionDef(
            name=node.name,
            args=node.args,
            body=preserved_doc + ellipsis_body,
            decorator_list=node.decorator_list,
            returns=node.returns,
            type_params=[],  # For Python 3.12+
            lineno=node.lineno,
            col_offset=node.col_offset,
        )

    def visit_ClassDef(self, node: ast.ClassDef) -> ast.ClassDef:
        """Preserve class structure but transform method bodies."""
        preserved_doc = self._preserve_docstring(node.body)
        remainder = node.body[len(preserved_doc) :]
        new_body: list[ast.stmt] = []

        for item in remainder:
            match item:
                case ast.FunctionDef():
                    new_body.append(self.visit_FunctionDef(item))
                case _:
                    # Visit child nodes to truncate large literals
                    new_body.append(self.visit(item))

        return ast.ClassDef(
            name=node.name,
            bases=node.bases,
            keywords=node.keywords,
            body=preserved_doc + new_body,
            decorator_list=node.decorator_list,
            type_params=[],  # For Python 3.12+
            lineno=node.lineno,
            col_offset=node.col_offset,
        )

    def visit_Module(self, node: ast.Module) -> ast.Module:
        """Process top-level statements."""
        body: list[ast.stmt] = []
        for item in node.body:
            # Skip empty or purely whitespace expressions
            is_empty_expr = (
                isinstance(item, ast.Expr)
                and isinstance(item.value, ast.Constant)
                and (not item.value.value or str(item.value.value).isspace())
            )
            if is_empty_expr:
                continue
            body.append(self.visit(item))

        return ast.Module(body=body, type_ignores=[])

    def generic_visit(self, node: ast.AST) -> ast.AST:
        """Recurse into all child nodes, then apply literal truncation."""
        new_node = super().generic_visit(node)
        return truncate_literal(new_node, self.config.truncation)


class ASTBackend(StubBackend):
    """AST-based stub generation backend with advanced concurrency."""

    # Class-level LRU cache for parsed AST nodes
    _ast_cache: ClassVar[dict[Path, ASTCacheEntry]] = {}
    _ast_cache_lock: ClassVar[asyncio.Lock] = asyncio.Lock()
    _max_cache_size: ClassVar[int] = 100

    def __init__(self, config: StubConfig) -> None:
        """Initialize the AST backend.

        Args:
            config: Configuration for stub generation
        """
        super().__init__(config)
        self._executor = ThreadPoolExecutor(
            max_workers=config.max_workers if config.max_workers else None
        )
        self._node_registry: weakref.WeakValueDictionary[int, ast.AST] = (
            weakref.WeakValueDictionary()
        )
        self._memory_monitor = MemoryMonitor()

    async def generate_stub(self, source_path: Path) -> str:
        """Generate a stub for a Python source file using AST parsing.

        Args:
            source_path: Path to the source file

        Returns:
            Generated stub content

        Raises:
            ASTError: If AST parsing or processing fails
        """
        try:
            # Get file locations
            locations = self.config.get_file_locations(source_path)
            locations.check_paths()

            # Start memory monitoring
            self._memory_monitor.start()

            # Read and parse source file
            source = await self._run_in_executor(source_path.read_text)
            source_hash = hashlib.sha256(source.encode()).hexdigest()

            # Try to get from cache
            async with self._ast_cache_lock:
                if source_path in self._ast_cache:
                    entry = self._ast_cache[source_path]
                    if entry.source_hash == source_hash:
                        logger.debug(f"AST cache hit for {source_path}")
                        return await self._process_ast(entry.node, source_path)

            # Parse file
            tree = await self._run_in_executor(
                functools.partial(ast.parse, source, filename=str(source_path))
            )

            # Attach parent references
            attach_parents(tree)

            # Update cache
            async with self._ast_cache_lock:
                self._ast_cache[source_path] = ASTCacheEntry(
                    node=tree,
                    source_hash=source_hash,
                    access_time=asyncio.get_event_loop().time(),
                )
                if len(self._ast_cache) > self._max_cache_size:
                    # Remove oldest entry
                    oldest = min(
                        self._ast_cache.items(),
                        key=lambda x: x[1].access_time,
                    )
                    del self._ast_cache[oldest[0]]

            # Process AST
            stub_content = await self._process_ast(tree, source_path)

            # Write output
            output_path = locations.output_path
            await self._run_in_executor(output_path.write_text, stub_content)

            return stub_content

        except Exception as e:
            raise ASTError(
                f"Failed to generate stub for {source_path}: {e}",
                ErrorCode.AST_PARSE_ERROR,
                source=str(source_path),
            ) from e
        finally:
            # Stop memory monitoring and log stats
            self._memory_monitor.stop()
            peak_mb = self._memory_monitor.peak_memory / 1024 / 1024
            logger.debug(f"Peak memory usage: {peak_mb:.1f}MB")
            self._memory_monitor.clear_stats()

    async def process_directory(self, directory: Path) -> dict[Path, str]:
        """Process a directory recursively.

        Args:
            directory: Directory to process

        Returns:
            Dictionary mapping output paths to stub contents

        Raises:
            ASTError: If directory processing fails
        """
        try:
            # Find all Python files matching patterns
            python_files: list[Path] = []
            for pattern in self.config.include_patterns:
                python_files.extend(directory.rglob(pattern))

            # Filter out excluded files
            for pattern in self.config.exclude_patterns:
                python_files = [f for f in python_files if not f.match(pattern)]

            # Process files with progress reporting
            results: dict[Path, str] = {}
            total = len(python_files)
            for i, path in enumerate(python_files, 1):
                try:
                    print_progress("Processing files", i, total)
                    stub = await self.generate_stub(path)
                    results[path] = stub
                except Exception as e:
                    logger.error(f"Failed to process {path}: {e}")
                    if not self.config.ignore_errors:
                        raise

            return results

        except Exception as e:
            raise ASTError(
                f"Failed to process directory {directory}: {e}",
                ErrorCode.AST_PARSE_ERROR,
                source=str(directory),
            ) from e

    def cleanup(self) -> None:
        """Clean up resources."""
        self._executor.shutdown(wait=True)
        self._ast_cache.clear()
        self._node_registry.clear()
        self._memory_monitor.stop()

    async def _process_ast(self, tree: ast.AST, source_path: Path) -> str:
        """Process an AST to generate a stub.

        Args:
            tree: AST to process
            source_path: Source file path for error reporting

        Returns:
            Generated stub content

        Raises:
            ASTError: If processing fails
        """
        try:
            # Create truncation config
            trunc_config = TruncationConfig(
                max_sequence_length=4,  # TODO: Make configurable
                max_string_length=17,
                max_docstring_length=150,
                max_file_size=3_000,
                truncation_marker="...",
            )

            # Process AST nodes in chunks to optimize memory usage
            stub_parts = []
            async for nodes in stream_process_ast(tree):
                # Process each chunk of nodes
                for node in nodes:
                    # Apply truncation
                    truncated = truncate_literal(node, trunc_config)
                    # Convert to string
                    stub_parts.append(ast.unparse(truncated))

            # Combine processed parts
            return "\n".join(stub_parts)

        except Exception as e:
            raise ASTError(
                f"Failed to process AST for {source_path}: {e}",
                ErrorCode.AST_TRANSFORM_ERROR,
                source=str(source_path),
            ) from e

    async def _run_in_executor(self, func: Any, *args: Any) -> Any:
        """Run a function in the thread pool executor.

        Args:
            func: Function to run
            *args: Arguments to pass to the function

        Returns:
            Function result
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self._executor, func, *args)

================
File: src/twat_coding/pystubnik/backends/base.py
================
#!/usr/bin/env -S uv run
"""Base interface for stub generation backends."""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import Any

from ..config import StubConfig


class StubBackend(ABC):
    """Abstract base class for stub generation backends."""

    def __init__(self, config: StubConfig) -> None:
        """Initialize the backend.

        Args:
            config: Configuration for stub generation
        """
        self.config = config

    @abstractmethod
    async def generate_stub(self, source_path: Path) -> str:
        """Generate a stub for a Python source file.

        Args:
            source_path: Path to the source file

        Returns:
            Generated stub content as a string

        Raises:
            StubGenerationError: If stub generation fails
        """
        raise NotImplementedError

    @abstractmethod
    async def process_module(self, module_name: str) -> str:
        """Process a module by its import name.

        Args:
            module_name: Fully qualified module name

        Returns:
            Generated stub content as a string

        Raises:
            StubGenerationError: If module processing fails
        """
        raise NotImplementedError

    @abstractmethod
    async def process_package(self, package_path: Path) -> dict[Path, str]:
        """Process a package directory recursively.

        Args:
            package_path: Path to the package directory

        Returns:
            Dictionary mapping output paths to stub contents

        Raises:
            StubGenerationError: If package processing fails
        """
        raise NotImplementedError

    def cleanup(self) -> None:
        """Clean up any resources used by the backend.

        This method should be called when the backend is no longer needed.
        """
        pass

    def __enter__(self) -> "StubBackend":
        """Enter the context manager."""
        return self

    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Exit the context manager."""
        self.cleanup()

================
File: src/twat_coding/pystubnik/backends/mypy_backend.py
================
"""MyPy-based stub generation backend.

This module implements stub generation using MyPy's stubgen functionality,
based on the original make_stubs_mypy.py implementation.
"""

from pathlib import Path

from pystubnik.backends import StubBackend
from pystubnik.core.config import PathConfig, StubGenConfig
from pystubnik.core.types import StubResult


class MypyBackend(StubBackend):
    """MyPy-based stub generation backend."""

    def __init__(self, config: StubGenConfig | None = None):
        self.config = config or StubGenConfig(paths=PathConfig())

    def generate_stub(
        self, source_path: Path, output_path: Path | None = None
    ) -> StubResult:
        """Generate stub for the given source file using MyPy's stubgen.

        Args:
            source_path: Path to the source file
            output_path: Optional path to write the stub file

        Returns:
            StubResult containing the generated stub and metadata
        """
        # TODO: Port functionality from make_stubs_mypy.py
        msg = "MyPy backend not yet implemented"
        raise NotImplementedError(msg)

================
File: src/twat_coding/pystubnik/core/config.py
================
#!/usr/bin/env python3
"""Configuration for pystubnik stub generation."""

import sys
from collections.abc import Mapping, Sequence
from dataclasses import dataclass, field
from enum import Enum, auto
from pathlib import Path
from typing import Any


class Backend(Enum):
    """Available stub generation backends."""

    AST = auto()  # Use Python's AST for precise control
    MYPY = auto()  # Use MyPy's stubgen for better type inference
    HYBRID = auto()  # Use both and merge results


class ImportanceLevel(Enum):
    """Importance levels for code elements."""

    CRITICAL = 1.5  # Must preserve exactly
    HIGH = 1.2  # Should preserve most details
    NORMAL = 1.0  # Standard preservation
    LOW = 0.8  # Minimal preservation
    IGNORE = 0.0  # Can be omitted


@dataclass(frozen=True)
class TruncationConfig:
    """Configuration for code truncation."""

    max_sequence_length: int = 4  # For lists, dicts, sets, tuples
    max_string_length: int = 17  # For strings except docstrings
    max_docstring_length: int = 150  # Default max length for docstrings
    max_file_size: int = 3_000  # Default max file size before removing all docstrings
    truncation_marker: str = "..."


def _default_importance_keywords() -> set[str]:
    """Get default importance keywords.

    Returns:
        Set of default importance keywords
    """
    return {"important", "critical", "essential", "main", "key"}


@dataclass(frozen=True)
class ProcessingConfig:
    """Configuration for stub processing."""

    include_docstrings: bool = True
    include_private: bool = False
    include_type_comments: bool = True
    infer_property_types: bool = True
    export_less: bool = False
    importance_patterns: Mapping[str, float] = field(default_factory=dict)
    importance_keywords: set[str] = field(default_factory=_default_importance_keywords)


@dataclass(frozen=True)
class PathConfig:
    """Configuration for file paths and search."""

    output_dir: Path = Path("out")
    doc_dir: Path | None = None
    search_paths: Sequence[Path] = field(default_factory=list)
    modules: Sequence[str] = field(default_factory=list)
    packages: Sequence[str] = field(default_factory=list)
    files: Sequence[Path] = field(default_factory=list)


@dataclass(frozen=True)
class RuntimeConfig:
    """Configuration for runtime behavior."""

    backend: Backend = Backend.HYBRID
    python_version: tuple[int, int] = field(
        default_factory=lambda: sys.version_info[:2]
    )
    interpreter: Path = field(default_factory=lambda: Path(sys.executable))
    no_import: bool = False
    inspect: bool = False
    parse_only: bool = False
    ignore_errors: bool = True
    verbose: bool = False
    quiet: bool = True
    parallel: bool = True
    max_workers: int | None = None

    @classmethod
    def create(
        cls,
        *,
        python_version: tuple[int, int] | None = None,
        interpreter: Path | str | None = None,
        **kwargs: Any,
    ) -> "RuntimeConfig":
        """Create a RuntimeConfig with optional version and interpreter.

        Args:
            python_version: Optional Python version tuple
            interpreter: Optional interpreter path
            **kwargs: Additional configuration options

        Returns:
            RuntimeConfig instance
        """
        if python_version is None:
            python_version = sys.version_info[:2]

        if interpreter is None:
            interpreter = Path(sys.executable)
        elif isinstance(interpreter, str):
            interpreter = Path(interpreter)

        return cls(
            python_version=python_version,
            interpreter=interpreter,
            **kwargs,
        )


@dataclass(frozen=True)
class StubGenConfig:
    """Main configuration for stub generation."""

    paths: PathConfig
    runtime: RuntimeConfig = field(default_factory=RuntimeConfig)
    processing: ProcessingConfig = field(default_factory=ProcessingConfig)
    truncation: TruncationConfig = field(default_factory=TruncationConfig)

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "StubGenConfig":
        """Create configuration from a dictionary."""
        paths = PathConfig(**data.get("paths", {}))
        runtime = RuntimeConfig(**data.get("runtime", {}))
        processing = ProcessingConfig(**data.get("processing", {}))
        truncation = TruncationConfig(**data.get("truncation", {}))
        return cls(
            paths=paths,
            runtime=runtime,
            processing=processing,
            truncation=truncation,
        )

    def to_dict(self) -> dict[str, Any]:
        """Convert configuration to a dictionary."""
        return {
            "paths": {
                k: str(v) if isinstance(v, Path) else v
                for k, v in self.paths.__dict__.items()
            },
            "runtime": self.runtime.__dict__,
            "processing": self.processing.__dict__,
            "truncation": self.truncation.__dict__,
        }

================
File: src/twat_coding/pystubnik/core/types.py
================
#!/usr/bin/env python3
"""Core type definitions for pystubnik."""

from collections.abc import Sequence
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Protocol, Union


@dataclass(frozen=True)
class ArgInfo:
    """Information about a function argument."""

    name: str
    type: str | None = None
    default: bool = False
    default_value: Any | None = None
    is_kwonly: bool = False
    is_posonly: bool = False
    is_variadic: bool = False


@dataclass(frozen=True)
class FunctionInfo:
    """Information about a function or method."""

    name: str
    args: Sequence[ArgInfo]
    return_type: str | None = None
    docstring: str | None = None
    decorators: Sequence[str] = ()
    is_async: bool = False
    is_generator: bool = False
    is_abstract: bool = False
    is_property: bool = False
    is_classmethod: bool = False
    is_staticmethod: bool = False
    importance_score: float = 1.0


@dataclass(frozen=True)
class ClassInfo:
    """Information about a class."""

    name: str
    bases: Sequence[str] = ()
    docstring: str | None = None
    decorators: Sequence[str] = ()
    methods: Sequence[FunctionInfo] = ()
    properties: Sequence[FunctionInfo] = ()
    class_vars: dict[str, str] = field(default_factory=dict)  # name -> type
    instance_vars: dict[str, str] = field(default_factory=dict)  # name -> type
    is_abstract: bool = False
    importance_score: float = 1.0


@dataclass(frozen=True)
class ModuleInfo:
    """Information about a module."""

    name: str
    path: Path | None = None
    docstring: str | None = None
    imports: Sequence[str] = ()
    functions: Sequence[FunctionInfo] = ()
    classes: Sequence[ClassInfo] = ()
    variables: dict[str, str] = field(default_factory=dict)  # name -> type
    all_names: Sequence[str] | None = None
    importance_score: float = 1.0


class StubBackend(Protocol):
    """Protocol for stub generation backends."""

    def generate_module_info(self, module_path: Path) -> ModuleInfo:
        """Generate module information from a source file."""
        ...

    def generate_stub(self, module_info: ModuleInfo) -> str:
        """Generate stub content from module information."""
        ...


class ImportProcessor(Protocol):
    """Protocol for import processing."""

    def process_imports(self, source: str) -> Sequence[str]:
        """Extract and process imports from source code."""
        ...

    def sort_imports(self, imports: Sequence[str]) -> Sequence[str]:
        """Sort imports in a standard way."""
        ...


class DocstringProcessor(Protocol):
    """Protocol for docstring processing."""

    def process_docstring(
        self, docstring: str | None, context: dict[str, Any]
    ) -> tuple[str | None, dict[str, Any]]:
        """Process a docstring and extract type information."""
        ...


class ImportanceScorer(Protocol):
    """Protocol for importance scoring."""

    def calculate_score(
        self,
        name: str,
        docstring: str | None = None,
        context: dict[str, Any] | None = None,
    ) -> float:
        """Calculate importance score for a symbol."""
        ...


class ImportType(Enum):
    """Types of imports that can be found in Python code."""

    STANDARD_LIB = "stdlib"
    THIRD_PARTY = "third_party"
    LOCAL = "local"
    RELATIVE = "relative"


@dataclass
class ImportInfo:
    """Information about an imported module or name."""

    module_name: str
    import_type: ImportType
    imported_names: list[str]
    is_from_import: bool = False
    relative_level: int = 0


@dataclass
class StubResult:
    """Result of stub generation for a single file."""

    source_path: Path
    stub_content: str
    imports: list[ImportInfo]
    errors: list[str]
    importance_score: float = 0.0


# Type aliases
PathLike = Union[str, Path]
ImportMap = dict[str, ImportInfo]
StubMap = dict[PathLike, StubResult]

================
File: src/twat_coding/pystubnik/core/utils.py
================
#!/usr/bin/env python3
"""Utility functions for pystubnik."""

import ast
import os
import re
import sys
from collections.abc import Callable, Iterable, Sequence
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import TypeVar

from loguru import logger

T = TypeVar("T")
U = TypeVar("U")


def process_parallel(
    items: Iterable[T],
    process_func: Callable[[T], U],
    max_workers: int | None = None,
    desc: str = "",
) -> list[U]:
    """Process items in parallel using ThreadPoolExecutor.

    Args:
        items: Items to process
        process_func: Function to process each item
        max_workers: Maximum number of worker threads
        desc: Description for progress reporting

    Returns:
        List of processed results
    """
    if max_workers is None:
        max_workers = os.cpu_count() or 1

    results = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = {executor.submit(process_func, item): item for item in items}
        for future in as_completed(futures):
            item = futures[future]
            try:
                result = future.result()
                results.append(result)
                if desc:
                    logger.debug(f"{desc}: Processed {item}")
            except Exception as e:
                logger.error(f"Failed to process {item}: {e}")
                if not isinstance(e, KeyboardInterrupt):
                    logger.debug(f"Error details: {type(e).__name__}: {e}")
                else:
                    raise
    return results


def find_python_files(
    path: Path | str,
    exclude_patterns: Sequence[str] = (".*", "*_test.py", "test_*.py"),
) -> list[Path]:
    """Find Python files in directory recursively.

    Args:
        path: Directory to search
        exclude_patterns: Glob patterns to exclude

    Returns:
        List of Python file paths
    """
    path = Path(path)
    if not path.exists():
        msg = f"Path does not exist: {path}"
        raise FileNotFoundError(msg)

    def is_excluded(p: Path) -> bool:
        return any(p.match(pattern) for pattern in exclude_patterns)

    if path.is_file():
        return [path] if path.suffix == ".py" and not is_excluded(path) else []

    result = []
    for root, _, files in os.walk(path):
        root_path = Path(root)
        if is_excluded(root_path):
            continue
        for file in files:
            file_path = root_path / file
            if file_path.suffix == ".py" and not is_excluded(file_path):
                result.append(file_path)
    return sorted(result)


def normalize_docstring(docstring: str | None) -> str | None:
    """Normalize a docstring by fixing indentation and removing redundant whitespace.

    Args:
        docstring: Raw docstring

    Returns:
        Normalized docstring or None if input is None
    """
    if not docstring:
        return None

    # Remove common leading whitespace from every line
    lines = docstring.expandtabs().splitlines()

    # Find minimum indentation (first line doesn't count)
    indent = sys.maxsize
    for line in lines[1:]:
        stripped = line.lstrip()
        if stripped:
            indent = min(indent, len(line) - len(stripped))

    # Remove indentation (but don't strip first line)
    trimmed = [lines[0].strip()]
    if indent < sys.maxsize:
        for line in lines[1:]:
            trimmed.append(line[indent:].rstrip())

    # Strip trailing empty lines
    while trimmed and not trimmed[-1]:
        trimmed.pop()

    # Return normalized docstring
    return "\n".join(trimmed)


def get_qualified_name(node: ast.AST) -> str:
    """Get qualified name from an AST node.

    Args:
        node: AST node (typically Name or Attribute)

    Returns:
        Qualified name as string
    """
    if isinstance(node, ast.Name):
        return node.id
    elif isinstance(node, ast.Attribute):
        return f"{get_qualified_name(node.value)}.{node.attr}"
    return ""


def parse_type_string(type_str: str) -> str:
    """Parse and normalize a type string.

    Args:
        type_str: Type string to parse

    Returns:
        Normalized type string
    """
    # Remove redundant spaces
    type_str = re.sub(r"\s+", " ", type_str.strip())

    # Handle union types (both new and old syntax)
    type_str = re.sub(r"Union\[(.*?)\]", r"(\1)", type_str)
    type_str = re.sub(r"\s*\|\s*", " | ", type_str)

    # Handle optional types
    type_str = re.sub(r"Optional\[(.*?)\]", r"\1 | None", type_str)

    # Normalize nested brackets
    depth = 0
    result = []
    for char in type_str:
        if char == "[":
            if depth > 0:
                result.append(" ")
            depth += 1
        elif char == "]":
            depth -= 1
        elif char == "," and depth > 0:
            result.append(", ")
            continue
        result.append(char)

    return "".join(result)


class ImportTracker:
    """Track and manage imports in a module."""

    def __init__(self) -> None:
        self.imports: dict[str, set[str]] = {}  # module -> names
        self.import_froms: dict[
            str, dict[str, str | None]
        ] = {}  # module -> {name -> alias}
        self.explicit_imports: set[str] = set()  # Explicitly requested imports

    def add_import(self, module: str, name: str | None = None) -> None:
        """Add an import."""
        if name:
            self.imports.setdefault(module, set()).add(name)
        else:
            self.imports.setdefault(module, set())
        self.explicit_imports.add(module)

    def add_import_from(
        self, module: str, names: Sequence[tuple[str, str | None]]
    ) -> None:
        """Add a from-import."""
        self.import_froms.setdefault(module, {}).update(dict(names))

    def get_import_lines(self) -> list[str]:
        """Get sorted import lines."""
        lines = []

        # Regular imports
        for module in sorted(self.imports):
            names = sorted(self.imports[module])
            if names:
                items = ", ".join(names)
                lines.append(f"from {module} import {items}")
            else:
                lines.append(f"import {module}")

        # From-imports
        for module in sorted(self.import_froms):
            name_map = self.import_froms[module]
            if name_map:
                items = ", ".join(
                    f"{name} as {alias}" if alias else name
                    for name, alias in sorted(name_map.items())
                )
                lines.append(f"from {module} import {items}")

        return lines


def setup_logging(level: str = "INFO") -> None:
    """Configure logging for the package."""
    logger.remove()  # Remove default handler
    logger.add(
        lambda msg: print(msg),
        level=level,
        format=(
            "<green>{time:HH:mm:ss}</green> | "
            "<level>{level: <8}</level> | "
            "<cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - "
            "<level>{message}</level>"
        ),
    )


def read_source_file(path: str | Path) -> tuple[str, str | None]:
    """Read a Python source file and detect its encoding.

    Args:
        path: Path to the source file

    Returns:
        Tuple of (source_code, encoding)
    """
    path = Path(path)
    try:
        with path.open("rb") as f:
            source = f.read()

        # Try to detect encoding from first two lines
        encoding = "utf-8"  # default
        for line in source.split(b"\n")[:2]:
            if line.startswith(b"#") and b"coding:" in line:
                encoding = line.split(b"coding:")[-1].strip().decode("ascii")
                break

        return source.decode(encoding), encoding
    except Exception as e:
        logger.error(f"Failed to read {path}: {e}")
        return "", None


def parse_source(source: str) -> ast.AST | None:
    """Parse Python source code into an AST.

    Args:
        source: Python source code string

    Returns:
        AST if parsing successful, None otherwise
    """
    try:
        return ast.parse(source)
    except SyntaxError as e:
        logger.error(f"Failed to parse source: {e}")
        return None


def normalize_path(path: str | Path) -> Path:
    """Normalize a path to an absolute Path object.

    Args:
        path: Path-like object to normalize

    Returns:
        Normalized absolute Path
    """
    return Path(path).resolve()

================
File: src/twat_coding/pystubnik/processors/__init__.py
================
"""Processor interface and registry for stub generation.

This module provides the base classes and interfaces for different
processors that can be used during stub generation.
"""

from abc import abstractmethod
from typing import Protocol

from pystubnik.core.types import StubResult


class Processor(Protocol):
    """Protocol defining the interface for stub processors."""

    @abstractmethod
    def process(self, stub_result: StubResult) -> StubResult:
        """Process a stub generation result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result
        """
        ...


_processors: dict[str, type[Processor]] = {}


def register_processor(name: str, processor: type[Processor]) -> None:
    """Register a new processor implementation."""
    _processors[name] = processor


def get_processor(name: str) -> type[Processor]:
    """Get a registered processor by name."""
    if name not in _processors:
        msg = f"Processor '{name}' not found"
        raise KeyError(msg)
    return _processors[name]


def list_processors() -> list[str]:
    """List all registered processor names."""
    return list(_processors.keys())

================
File: src/twat_coding/pystubnik/processors/docstring.py
================
"""Docstring processing functionality."""

from dataclasses import dataclass
from typing import Literal

from docstring_parser import parse
from docstring_parser.common import DocstringStyle
from loguru import logger

from ..core.types import (
    ClassInfo,
    FunctionInfo,
    ModuleInfo,
    StubResult,
)
from ..errors import StubGenerationError
from ..types.type_system import TypeInfo
from . import Processor


class TypeInferenceError(StubGenerationError):
    """Error raised when type inference fails."""

    def __init__(self, message: str, details: dict[str, str] | None = None) -> None:
        super().__init__(message, "TYPE001", details)


@dataclass
class DocstringResult:
    """Result of docstring processing."""

    docstring: str | None
    signatures: list[FunctionInfo]
    param_types: dict[str, TypeInfo]
    return_type: TypeInfo | None
    yield_type: TypeInfo | None
    raises: list[tuple[TypeInfo, str]]  # (exception_type, description)


class DocstringProcessor(Processor):
    """Processes docstrings to extract type information and signatures."""

    def __init__(
        self,
        style: Literal["google", "numpy", "rest"] = "google",
        max_length: int | None = None,
        preserve_sections: list[str] | None = None,
    ) -> None:
        """Initialize docstring processor.

        Args:
            style: Docstring style to use for parsing
            max_length: Maximum length for docstrings before truncation
            preserve_sections: List of section names to always preserve
        """
        self.style = DocstringStyle.GOOGLE
        if style == "numpy":
            self.style = DocstringStyle.NUMPYDOC
        elif style == "rest":
            self.style = DocstringStyle.REST

        self.max_length = max_length
        self.preserve_sections = preserve_sections or [
            "Args",
            "Returns",
            "Yields",
            "Raises",
        ]

    def process(self, stub_result: StubResult) -> StubResult:
        """Process docstrings in a stub result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result
        """
        logger.debug(f"Processing docstrings for {stub_result.source_path}")

        # Process module docstring
        module_info = self._process_module_docstring(stub_result)

        # Process function docstrings
        for function in module_info.functions:
            self._process_function_docstring(function)

        # Process class docstrings
        for class_info in module_info.classes:
            self._process_class_docstring(class_info)

            # Process method docstrings
            for method in class_info.methods:
                self._process_function_docstring(method)

            # Process property docstrings
            for prop in class_info.properties:
                self._process_function_docstring(prop)

        return stub_result

    def _process_module_docstring(self, stub_result: StubResult) -> ModuleInfo:
        """Process the module-level docstring.

        Args:
            stub_result: The stub generation result

        Returns:
            The processed module info
        """
        logger.debug("Processing module docstring")
        # TODO: Implement module docstring processing
        return ModuleInfo(name="", path=stub_result.source_path)

    def _process_function_docstring(self, function: FunctionInfo) -> None:
        """Process a function or method docstring.

        Args:
            function: The function info to process
        """
        if not function.docstring:
            return

        logger.debug(f"Processing docstring for function {function.name}")

        try:
            docstring = parse(function.docstring, style=self.style)

            # Extract parameter types
            for param in docstring.params:
                if param.type_name:
                    arg = next(
                        (a for a in function.args if a.name == param.arg_name), None
                    )
                    if arg:
                        # Update arg type if found in docstring
                        object.__setattr__(arg, "type", param.type_name)

            # Extract return type
            if docstring.returns and docstring.returns.type_name:
                object.__setattr__(function, "return_type", docstring.returns.type_name)

        except Exception as e:
            logger.warning(f"Failed to parse docstring for {function.name}: {e}")

    def _process_class_docstring(self, class_info: ClassInfo) -> None:
        """Process a class docstring.

        Args:
            class_info: The class info to process
        """
        if not class_info.docstring:
            return

        logger.debug(f"Processing docstring for class {class_info.name}")

        try:
            parse(class_info.docstring, style=self.style)
            # TODO: Extract class-level type information

        except Exception as e:
            logger.warning(f"Failed to parse docstring for {class_info.name}: {e}")

================
File: src/twat_coding/pystubnik/processors/file_importance.py
================
"""Enhanced file importance analysis for Python packages.

This module provides tools to analyze and rank Python files based on multiple metrics:
- Dependency centrality (PageRank)
- Code complexity
- Test coverage
- Documentation quality
"""

import argparse
import ast
import importlib.util
import json
import os
from dataclasses import dataclass, field
from typing import Any, TypeVar, cast

import networkx as nx
import toml
from coverage import Coverage
from pydocstyle import check as pydocstyle_check
from radon.complexity import cc_visit

# Type variables for better type safety
T = TypeVar("T")


def _cast_or_default(value: Any, default: T) -> T:
    """Cast a value to the type of the default or return the default if casting fails."""
    try:
        if isinstance(value, type(default)):
            return cast(T, value)
        return default
    except Exception:
        return default


# Optional dependency error messages
COVERAGE_MISSING = "coverage package not installed. Coverage analysis will be disabled."
PYDOCSTYLE_MISSING = (
    "pydocstyle package not installed. Documentation quality analysis will be disabled."
)
RADON_MISSING = (
    "radon package not installed. Code complexity analysis will be disabled."
)


def _check_optional_dependencies() -> dict[str, bool]:
    """Check which optional dependencies are available.

    Returns:
        Dictionary mapping dependency names to their availability status
    """
    available = {
        "coverage": True,
        "pydocstyle": True,
        "radon": True,
    }

    if not importlib.util.find_spec("coverage"):
        available["coverage"] = False
        print(COVERAGE_MISSING)

    if not importlib.util.find_spec("pydocstyle"):
        available["pydocstyle"] = False
        print(PYDOCSTYLE_MISSING)

    if not importlib.util.find_spec("radon"):
        available["radon"] = False
        print(RADON_MISSING)

    return available


# Check optional dependencies at module import time
AVAILABLE_DEPS = _check_optional_dependencies()


@dataclass
class FileImportanceConfig:
    """Configuration for file importance analysis."""

    exclude_dirs: list[str] = field(default_factory=list)
    centrality: str = "pagerank"  # One of: pagerank, betweenness, eigenvector
    coverage_data: str | None = None
    weights: dict[str, float] = field(
        default_factory=lambda: {
            "centrality": 0.4,
            "complexity": 0.3,
            "coverage": 0.15,
            "doc_quality": 0.15,
        }
    )


def find_py_files(package_dir: str, exclude_dirs: list[str] | None = None) -> list[str]:
    """Find all .py files in the package directory, excluding specified directories.

    Args:
        package_dir: Root directory to search
        exclude_dirs: List of directory names to exclude

    Returns:
        List of Python file paths
    """
    py_files = []
    exclude_dirs = exclude_dirs or []
    for root, _, files in os.walk(package_dir):
        if any(exclude in root for exclude in exclude_dirs):
            continue
        for file in files:
            if file.endswith(".py"):
                py_files.append(os.path.join(root, file))
    return py_files


def is_entry_point(file_path: str) -> bool:
    """Check if the file contains if __name__ == '__main__'."""
    try:
        with open(file_path, encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=file_path)
        for node in ast.walk(tree):
            if (
                isinstance(node, ast.If)
                and isinstance(node.test, ast.Compare)
                and isinstance(node.test.left, ast.Name)
                and node.test.left.id == "__name__"
                and len(node.test.ops) == 1
                and isinstance(node.test.ops[0], ast.Eq)
                and len(node.test.comparators) == 1
                and isinstance(node.test.comparators[0], ast.Constant)
                and node.test.comparators[0].value == "__main__"
            ):
                return True
    except Exception:
        return False
    return False


def get_additional_entry_points(package_dir: str) -> list[str]:
    """Parse pyproject.toml for additional entry points like console scripts."""
    entry_points = []
    pyproject_path = os.path.join(package_dir, "pyproject.toml")
    if os.path.exists(pyproject_path):
        try:
            with open(pyproject_path) as f:
                data = toml.load(f)
                scripts = data.get("project", {}).get("scripts", {})
                for script in scripts.values():
                    if os.path.exists(script) and script.endswith(".py"):
                        entry_points.append(script)
        except Exception:
            pass
    return entry_points


def _parse_import_node(
    source_file: str, node: ast.Import | ast.ImportFrom, py_files: list[str]
) -> list[tuple[str, str]]:
    """Parse a single import node and return edges for the import graph.

    Args:
        source_file: Path to the source file containing the import
        node: The AST import node to parse
        py_files: List of all Python files in the package

    Returns:
        List of (source, target) tuples representing import edges
    """
    edges = []
    if isinstance(node, ast.Import):
        for name in node.names:
            imported = name.name.split(".")[0]
            for target in py_files:
                if os.path.basename(target) == f"{imported}.py":
                    edges.append((source_file, target))
    elif isinstance(node, ast.ImportFrom) and node.module:
        imported = node.module.split(".")[0]
        for target in py_files:
            if os.path.basename(target) == f"{imported}.py":
                edges.append((source_file, target))
    return edges


def _parse_imports_from_file(file: str, py_files: list[str]) -> list[tuple[str, str]]:
    """Parse imports from a Python file and return edges for the import graph.

    Args:
        file: Path to the Python file to parse
        py_files: List of all Python files in the package

    Returns:
        List of (source, target) tuples representing import edges
    """
    edges = []
    try:
        with open(file, encoding="utf-8") as f:
            tree = ast.parse(f.read(), filename=file)

        for node in ast.walk(tree):
            if isinstance(node, ast.Import | ast.ImportFrom):
                edges.extend(_parse_import_node(file, node, py_files))
    except Exception:
        pass
    return edges


def build_import_graph(package_dir: str, py_files: list[str]) -> nx.DiGraph:
    """Build the import graph using importlab and convert to networkx.

    Args:
        package_dir: Root directory of the Python package
        py_files: List of Python files to analyze

    Returns:
        A networkx DiGraph representing the import dependencies
    """
    # Create networkx graph
    G = nx.DiGraph()
    for file in py_files:
        G.add_node(file)

    # Parse imports from each file and add edges
    for file in py_files:
        edges = _parse_imports_from_file(file, py_files)
        G.add_edges_from(edges)

    return G


def calculate_complexity(file_path: str) -> float:
    """Calculate average cyclomatic complexity using radon."""
    if not AVAILABLE_DEPS["radon"]:
        return 0.0

    try:
        with open(file_path, encoding="utf-8") as f:
            code = f.read()
        complexities = cc_visit(code)
        if not complexities:
            return 0.0
        total_complexity = sum(
            _cast_or_default(c.complexity, 0.0) for c in complexities
        )
        return total_complexity / len(complexities)
    except Exception as e:
        print(f"Warning: Failed to calculate complexity for {file_path}: {e}")
        return 0.0


def calculate_coverage(file_path: str, coverage_data: str | None) -> float:
    """Get test coverage percentage for the file.

    Args:
        file_path: Path to the Python file
        coverage_data: Path to coverage data file

    Returns:
        Coverage percentage between 0 and 100
    """
    if not AVAILABLE_DEPS["coverage"]:
        return 0.0

    if not coverage_data or not os.path.exists(coverage_data):
        return 0.0
    try:
        cov = Coverage(data_file=coverage_data)
        cov.load()
        analysis = cov._analyze(file_path)
        total_lines = _cast_or_default(analysis.numbers.n_statements, 0)
        covered_lines = _cast_or_default(analysis.numbers.n_executed, 0)
        return (covered_lines / total_lines * 100) if total_lines > 0 else 0.0
    except Exception as e:
        print(f"Warning: Failed to calculate coverage for {file_path}: {e}")
        return 0.0


def calculate_doc_quality(file_path: str) -> float:
    """Assess documentation quality using pydocstyle."""
    if not AVAILABLE_DEPS["pydocstyle"]:
        return 0.0

    try:
        violations = list(pydocstyle_check([file_path], ignore=["D100", "D101"]))
        return max(0.0, 1.0 - (len(violations) / 10))
    except Exception as e:
        print(f"Warning: Failed to assess documentation quality for {file_path}: {e}")
        return 0.0


def _calculate_centrality(
    G: nx.DiGraph,
    py_files: list[str],
    centrality_measure: str,
    entry_points: dict[str, bool],
) -> dict[str, float]:
    """Calculate centrality scores for files in the import graph.

    Args:
        G: The import dependency graph
        py_files: List of Python files
        centrality_measure: Type of centrality to calculate
        entry_points: Dict mapping files to their entry point status

    Returns:
        Dictionary mapping files to their centrality scores
    """
    personalization_dict = {
        file: 1.0 if entry_points[file] else 0.0 for file in py_files
    }

    if centrality_measure == "pagerank":
        # If all values are 0, use uniform personalization
        if sum(personalization_dict.values()) == 0:
            personalization_dict = {file: 1.0 for file in py_files}
        result = nx.pagerank(G, personalization=personalization_dict)
        return {k: float(v) for k, v in result.items()}
    elif centrality_measure == "betweenness":
        result = nx.betweenness_centrality(G)
        return {k: float(v) for k, v in result.items()}
    elif centrality_measure == "eigenvector":
        try:
            result = nx.eigenvector_centrality(G, max_iter=500)
            return {k: float(v) for k, v in result.items()}
        except nx.PowerIterationFailedConvergence:
            return {file: 0.0 for file in py_files}
    else:
        print(
            f"Warning: Unsupported centrality measure '{centrality_measure}'. "
            "Using PageRank."
        )
        result = nx.pagerank(G)
        return {k: float(v) for k, v in result.items()}


def _print_results(
    sorted_files: list[tuple[str, float]],
    package_dir: str,
    entry_points: dict[str, bool],
    centrality: dict[str, float],
    complexity_scores: dict[str, float],
    coverage_scores: dict[str, float],
    doc_scores: dict[str, float],
) -> None:
    """Print analysis results in a formatted table.

    Args:
        sorted_files: List of (file, score) tuples sorted by score
        package_dir: Root directory of the package
        entry_points: Dict mapping files to their entry point status
        centrality: Dict mapping files to their centrality scores
        complexity_scores: Dict mapping files to their complexity scores
        coverage_scores: Dict mapping files to their coverage scores
        doc_scores: Dict mapping files to their documentation quality scores
    """
    header = (
        f"{'File Path':<50} {'Composite':<10} {'Centrality':<10} "
        f"{'Complexity':<10} {'Coverage':<10} {'Doc':<10} {'Entry':<6} {'Init'}"
    )
    print(header)
    print("-" * len(header))

    for file, score in sorted_files:
        is_entry = "Yes" if entry_points.get(file, False) else "No"
        is_init = "Yes" if os.path.basename(file) == "__init__.py" else "No"
        rel_path = os.path.relpath(file, package_dir)

        # Split long line into multiple lines for readability
        print(
            f"{rel_path:<50} {score:<10.3f} "
            f"{centrality.get(file, 0):<10.3f} "
            f"{complexity_scores.get(file, 0):<10.1f} "
            f"{coverage_scores.get(file, 0):<10.1f} "
            f"{doc_scores.get(file, 0):<10.2f} "
            f"{is_entry:<6} {is_init}"
        )


def prioritize_files(
    package_dir: str, config: FileImportanceConfig
) -> dict[str, float]:
    """Analyze and prioritize .py files in the package based on multiple metrics.

    Args:
        package_dir: Root directory of the Python package
        config: Configuration for importance analysis

    Returns:
        Dictionary mapping file paths to their composite importance scores
    """
    # Step 1: Collect .py files
    py_files = find_py_files(package_dir, config.exclude_dirs)
    if not py_files:
        print("No Python files found in the package directory.")
        return {}

    # Step 2: Build import graph and identify entry points
    G = build_import_graph(package_dir, py_files)
    entry_points = {file: is_entry_point(file) for file in py_files}
    additional_eps = get_additional_entry_points(package_dir)
    for ep in additional_eps:
        if ep in py_files:
            entry_points[ep] = True

    # Step 3: Calculate metrics
    centrality = _calculate_centrality(G, py_files, config.centrality, entry_points)
    complexity_scores = {file: calculate_complexity(file) for file in py_files}
    coverage_scores = {
        file: calculate_coverage(file, config.coverage_data) for file in py_files
    }
    doc_scores = {file: calculate_doc_quality(file) for file in py_files}

    # Step 4: Normalize complexity
    max_complexity = max(complexity_scores.values(), default=1.0)
    complexity_normalized = {
        file: score / max_complexity for file, score in complexity_scores.items()
    }

    # Step 5: Compute composite scores
    weights = config.weights
    composite_scores: dict[str, float] = {}
    for file in py_files:
        score = (
            weights["centrality"] * centrality.get(file, 0.0)
            + weights["complexity"] * complexity_normalized.get(file, 0.0)
            + weights["coverage"] * (coverage_scores.get(file, 0.0) / 100)
            + weights["doc_quality"] * doc_scores.get(file, 0.0)
        )
        composite_scores[file] = score

    # Step 6: Sort and display results
    sorted_files = sorted(composite_scores.items(), key=lambda x: x[1], reverse=True)
    _print_results(
        sorted_files,
        package_dir,
        entry_points,
        centrality,
        complexity_scores,
        coverage_scores,
        doc_scores,
    )

    return composite_scores


def load_config(config_file: str | None) -> dict[str, Any]:
    """Load configuration from a JSON file."""
    if config_file and os.path.exists(config_file):
        try:
            with open(config_file) as f:
                config = json.load(f)
                return cast(dict[str, Any], config)
        except Exception as e:
            print(
                "Warning: Failed to load config file "
                f"'{config_file}': {e}. Using defaults."
            )
    return {}


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=(
            "Prioritize .py files in a Python package based on multiple metrics."
        )
    )
    parser.add_argument("package_dir", help="Path to the package directory")
    parser.add_argument("--config", help="Path to configuration JSON file")
    parser.add_argument("--coverage-data", help="Path to coverage.py data file")
    parser.add_argument("--exclude-dirs", nargs="+", help="Directories to exclude")
    parser.add_argument(
        "--centrality",
        choices=["pagerank", "betweenness", "eigenvector"],
        default="pagerank",
        help="Centrality measure to use",
    )
    args = parser.parse_args()

    config = FileImportanceConfig()
    user_config = load_config(args.config)

    if user_config:
        config.weights.update(user_config.get("weights", {}))
        config.exclude_dirs.extend(user_config.get("exclude_dirs", []))
        if "centrality" in user_config:
            config.centrality = user_config["centrality"]

    if args.coverage_data:
        config.coverage_data = args.coverage_data
    if args.exclude_dirs:
        config.exclude_dirs.extend(args.exclude_dirs)
    if args.centrality:
        config.centrality = args.centrality

    prioritize_files(args.package_dir, config)

================
File: src/twat_coding/pystubnik/processors/importance.py
================
#!/usr/bin/env -S uv run
"""Importance scoring for symbols and code elements."""

import re
from dataclasses import dataclass, field
from typing import Any

from loguru import logger

from ..core.types import StubResult
from . import Processor
from .file_importance import FileImportanceConfig, prioritize_files


@dataclass
class ImportanceConfig:
    """Configuration for importance scoring."""

    patterns: dict[str, float] = field(default_factory=dict)
    keywords: set[str] = field(
        default_factory=lambda: {
            "important",
            "critical",
            "essential",
            "main",
            "key",
        }
    )
    min_score: float = 0.5
    docstring_weight: float = 0.01  # Per word in docstring
    public_weight: float = 1.1
    special_weight: float = 1.2

    # File-level importance settings
    file_importance: FileImportanceConfig = field(default_factory=FileImportanceConfig)


class ImportanceProcessor(Processor):
    """Calculate importance scores for code elements."""

    def __init__(self, config: ImportanceConfig | None = None) -> None:
        """Initialize the importance processor.

        Args:
            config: Configuration for importance scoring
        """
        self.config = config or ImportanceConfig()
        self._file_scores: dict[str, float] = {}

    def process(self, stub_result: StubResult) -> StubResult:
        """Process a stub result to calculate importance scores.

        This method combines both symbol-level and file-level importance scoring:
        1. First, it calculates file-level importance if not already done
        2. Then, it adjusts symbol scores based on their containing file's importance
        3. Finally, it applies symbol-specific scoring rules

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result with importance scores
        """
        # Calculate file-level importance if needed
        if not self._file_scores:
            try:
                package_dir = str(stub_result.source_path.parent)
                prioritize_files(package_dir, self.config.file_importance)
            except Exception as e:
                logger.warning(f"Failed to calculate file importance: {e}")

        # Get file importance score
        file_score = self._file_scores.get(str(stub_result.source_path), 0.5)

        # Adjust stub result's importance score
        stub_result.importance_score = file_score

        # Process each symbol in the stub result
        # TODO: Traverse AST to find and score individual symbols
        # For now, we'll just use the file score as a base

        return stub_result

    def _get_file_score(self, extra_info: dict[str, Any] | None) -> float:
        """Get the file-level base score."""
        if extra_info and "file_path" in extra_info:
            file_path = extra_info["file_path"]
            return self._file_scores.get(str(file_path), 1.0)
        return 1.0

    def _calculate_pattern_score(self, name: str) -> float:
        """Calculate score based on importance patterns."""
        score = 1.0
        for pattern, weight in self.config.patterns.items():
            if re.search(pattern, name):
                score *= weight
        return score

    def _calculate_docstring_score(self, docstring: str | None) -> float:
        """Calculate score based on docstring quality."""
        if not docstring:
            return 1.0

        score = 1.0
        word_count = len(docstring.split())
        score += word_count * self.config.docstring_weight

        # Check for importance keywords
        for keyword in self.config.keywords:
            if keyword.lower() in docstring.lower():
                score *= 1.1  # Small boost for each importance keyword

        return score

    def _calculate_visibility_score(self, is_public: bool, is_special: bool) -> float:
        """Calculate score based on symbol visibility."""
        score = 1.0
        if is_public:
            score *= self.config.public_weight
        if is_special:
            score *= self.config.special_weight
        return score

    def calculate_importance(
        self,
        name: str,
        docstring: str | None = None,
        is_public: bool = True,
        is_special: bool = False,
        extra_info: dict[str, Any] | None = None,
    ) -> float:
        """Calculate importance score for a symbol.

        Args:
            name: Symbol name
            docstring: Associated docstring
            is_public: Whether the symbol is public
            is_special: Whether it's a special method
            extra_info: Additional information for scoring

        Returns:
            Importance score between 0 and 1
        """
        try:
            # Calculate component scores
            file_score = self._get_file_score(extra_info)
            pattern_score = self._calculate_pattern_score(name)
            docstring_score = self._calculate_docstring_score(docstring)
            visibility_score = self._calculate_visibility_score(is_public, is_special)

            # Combine scores
            final_score = (
                file_score * pattern_score * docstring_score * visibility_score
            )

            # Ensure score is between 0 and 1
            return max(min(final_score, 1.0), 0.0)

        except Exception as e:
            logger.warning(f"Error calculating importance for {name}: {e}")
            return self.config.min_score

    def should_include(
        self,
        name: str,
        score: float | None = None,
        docstring: str | None = None,
        is_public: bool = True,
        is_special: bool = False,
        extra_info: dict[str, Any] | None = None,
    ) -> bool:
        """Determine if a symbol should be included based on importance.

        Args:
            name: Symbol name
            score: Pre-calculated importance score
            docstring: Associated docstring
            is_public: Whether the symbol is public
            is_special: Whether it's a special method
            extra_info: Additional information for scoring

        Returns:
            True if the symbol should be included
        """
        if score is None:
            score = self.calculate_importance(
                name,
                docstring,
                is_public,
                is_special,
                extra_info,
            )
        return score >= self.config.min_score

================
File: src/twat_coding/pystubnik/processors/imports.py
================
"""Import processor for stub generation."""

import ast
from pathlib import Path

from pystubnik.core.types import ImportInfo, ImportType, StubResult
from pystubnik.processors import Processor


class ImportProcessor(Processor):
    """Processor for analyzing and organizing imports in stub generation."""

    def __init__(self) -> None:
        self.stdlib_modules: set[str] = self._get_stdlib_modules()

    def process(self, stub_result: StubResult) -> StubResult:
        """Process imports in the stub result.

        Args:
            stub_result: The stub generation result to process

        Returns:
            The processed stub result with organized imports
        """
        tree = ast.parse(stub_result.stub_content)
        imports = self._analyze_imports(tree)

        # Group imports by type
        grouped_imports = self._group_imports(imports)

        # Generate new import section
        new_imports = self._format_imports(grouped_imports)

        # Replace imports in stub content
        stub_result.stub_content = self._replace_imports(tree, new_imports)
        stub_result.imports = list(imports.values())

        return stub_result

    def _get_stdlib_modules(self) -> set[str]:
        """Get a set of standard library module names."""
        import sysconfig

        stdlib_path = sysconfig.get_path("stdlib")
        if not stdlib_path:
            return set()

        stdlib_modules = set()
        for path in Path(stdlib_path).glob("**/*.py"):
            module_name = path.stem
            if module_name != "__init__":
                stdlib_modules.add(module_name)

        return stdlib_modules

    def _analyze_imports(self, tree: ast.AST) -> dict[str, ImportInfo]:
        """Analyze imports in an AST.

        Args:
            tree: The AST to analyze

        Returns:
            Dictionary mapping module names to ImportInfo
        """
        imports: dict[str, ImportInfo] = {}

        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for name in node.names:
                    module_name = name.name
                    import_type = self._get_import_type(module_name)
                    imports[module_name] = ImportInfo(
                        module_name=module_name,
                        import_type=import_type,
                        imported_names=[name.asname or name.name],
                    )
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                import_type = self._get_import_type(module)
                if module not in imports:
                    imports[module] = ImportInfo(
                        module_name=module,
                        import_type=import_type,
                        imported_names=[],
                        is_from_import=True,
                        relative_level=node.level,
                    )
                for name in node.names:
                    imports[module].imported_names.append(name.asname or name.name)

        return imports

    def _get_import_type(self, module_name: str) -> ImportType:
        """Determine the type of an import.

        Args:
            module_name: Name of the module

        Returns:
            The ImportType of the module
        """
        if not module_name:
            return ImportType.RELATIVE

        base_module = module_name.split(".")[0]
        if base_module in self.stdlib_modules:
            return ImportType.STANDARD_LIB
        elif "." in module_name:
            return ImportType.LOCAL
        else:
            return ImportType.THIRD_PARTY

    def _group_imports(
        self, imports: dict[str, ImportInfo]
    ) -> dict[ImportType, list[ImportInfo]]:
        """Group imports by their type.

        Args:
            imports: Dictionary of imports to group

        Returns:
            Dictionary mapping import types to lists of imports
        """
        grouped: dict[ImportType, list[ImportInfo]] = {
            ImportType.STANDARD_LIB: [],
            ImportType.THIRD_PARTY: [],
            ImportType.LOCAL: [],
            ImportType.RELATIVE: [],
        }

        for import_info in imports.values():
            grouped[import_info.import_type].append(import_info)

        # Sort each group
        for group in grouped.values():
            group.sort(key=lambda x: x.module_name)

        return grouped

    def _format_imports(
        self, grouped_imports: dict[ImportType, list[ImportInfo]]
    ) -> str:
        """Format grouped imports into a string.

        Args:
            grouped_imports: Dictionary of grouped imports

        Returns:
            Formatted import section as a string
        """
        sections = []

        for import_type in ImportType:
            imports = grouped_imports[import_type]
            if not imports:
                continue

            section = []
            for import_info in imports:
                if import_info.is_from_import:
                    relative = "." * import_info.relative_level
                    names = ", ".join(sorted(import_info.imported_names))
                    section.append(
                        f"from {relative}{import_info.module_name} import {names}"
                    )
                else:
                    section.append(f"import {import_info.module_name}")

            if section:
                sections.append("\n".join(sorted(section)))

        return "\n\n".join(sections) + "\n\n"

    def _replace_imports(self, tree: ast.AST, new_imports: str) -> str:
        """Replace imports in the AST with new formatted imports.

        Args:
            tree: The AST to modify
            new_imports: The new import section

        Returns:
            Modified source code as a string
        """
        # Find the last import node
        last_import = None
        for node in ast.walk(tree):
            if isinstance(node, ast.Import | ast.ImportFrom):
                last_import = node

        if last_import:
            # Get the source up to the first import and after the last import
            source_lines = ast.unparse(tree).split("\n")
            first_import_line = min(
                node.lineno
                for node in ast.walk(tree)
                if isinstance(node, ast.Import | ast.ImportFrom)
            )
            last_import_line = max(
                node.end_lineno or node.lineno
                for node in ast.walk(tree)
                if isinstance(node, ast.Import | ast.ImportFrom)
            )

            before = "\n".join(source_lines[: first_import_line - 1])
            after = "\n".join(source_lines[last_import_line:])

            return f"{before}\n{new_imports}{after}"

        return ast.unparse(tree)

================
File: src/twat_coding/pystubnik/processors/stub_generation.py
================
#!/usr/bin/env -S uv run
"""Stub generation processor for creating type stub files."""

import ast
from dataclasses import dataclass, field
from pathlib import Path
from typing import ClassVar, cast

from ..errors import ErrorCode, StubGenerationError
from ..types.type_system import TypeRegistry
from ..utils.ast_utils import TruncationConfig, attach_parents


@dataclass
class StubConfig:
    """Configuration for stub generation."""

    # Output settings
    line_length: int = 88
    sort_imports: bool = True
    add_header: bool = True
    header_template: str = (
        "# Generated by pystubnik\n# Do not edit this file directly\n\n"
    )

    # Content settings
    include_docstrings: bool = True
    include_private: bool = False
    include_type_comments: bool = True
    infer_property_types: bool = True
    export_less: bool = False

    # Truncation settings
    truncation: TruncationConfig = field(default_factory=TruncationConfig)


class StubGenerator:
    """Generate type stubs from Python source code."""

    # Common import patterns to preserve
    ESSENTIAL_IMPORTS: ClassVar[set[str]] = {
        "typing",
        "dataclasses",
        "enum",
        "abc",
        "contextlib",
        "pathlib",
        "collections.abc",
    }

    def __init__(
        self,
        config: StubConfig | None = None,
        type_registry: TypeRegistry | None = None,
    ) -> None:
        """Initialize the stub generator.

        Args:
            config: Configuration for stub generation
            type_registry: Registry for type resolution
        """
        self.config = config or StubConfig()
        self.type_registry = type_registry or TypeRegistry()

    def generate_stub(self, source_path: Path, tree: ast.AST | None = None) -> str:
        """Generate a type stub for a Python source file.

        Args:
            source_path: Path to the source file
            tree: Optional pre-parsed AST

        Returns:
            Generated stub content

        Raises:
            StubGenerationError: If stub generation fails
        """
        try:
            # Parse source if not provided
            if tree is None:
                with source_path.open() as f:
                    tree = ast.parse(f.read(), filename=str(source_path))

            # Attach parent references for better context
            attach_parents(tree)

            # Process the AST
            if not isinstance(tree, ast.Module):
                raise StubGenerationError(
                    "Expected Module AST node",
                    ErrorCode.AST_PARSE_ERROR,
                    source=str(source_path),
                )
            processed = self._process_module(tree)

            # Generate stub content
            return self._generate_content(processed)

        except Exception as e:
            raise StubGenerationError(
                f"Failed to generate stub for {source_path}: {e}",
                ErrorCode.AST_TRANSFORM_ERROR,
                source=str(source_path),
            ) from e

    def _process_module(self, node: ast.Module) -> ast.Module:
        """Process a module AST for stub generation.

        Args:
            node: Module AST to process

        Returns:
            Processed module AST
        """
        # Create a new module for the stub
        stub = ast.Module(body=[], type_ignores=[])

        # Process imports and definitions
        imports = self._collect_imports(node)
        definitions = self._collect_definitions(node)

        # Build the stub body
        stub.body = self._build_stub_body(imports, definitions)

        return stub

    def _collect_imports(self, node: ast.Module) -> list[ast.Import | ast.ImportFrom]:
        """Collect and process import statements.

        Args:
            node: Module AST to process

        Returns:
            List of processed import statements
        """
        imports = [
            child
            for child in node.body
            if isinstance(child, ast.Import | ast.ImportFrom)
            and self._should_keep_import(child)
        ]

        if self.config.sort_imports and imports:
            # Sort imports by module name
            imports_with_keys = [
                (self._import_sort_key(cast(ast.Import | ast.ImportFrom, imp)), imp)
                for imp in imports
            ]
            imports_with_keys.sort(key=lambda x: x[0])
            imports = [imp for _, imp in imports_with_keys]

        return imports

    def _collect_definitions(self, node: ast.Module) -> list[ast.stmt]:
        """Collect and process module definitions.

        Args:
            node: Module AST to process

        Returns:
            List of processed definitions
        """
        definitions = []
        for child in node.body:
            if not isinstance(child, ast.Import | ast.ImportFrom):
                if processed := self._process_node(child):
                    if isinstance(processed, ast.stmt):
                        definitions.append(processed)
        return definitions

    def _build_stub_body(
        self,
        imports: list[ast.Import | ast.ImportFrom],
        definitions: list[ast.stmt],
    ) -> list[ast.stmt]:
        """Build the final stub body from components.

        Args:
            imports: List of processed import statements
            definitions: List of processed definitions

        Returns:
            Complete list of statements for the stub body
        """
        body: list[ast.stmt] = []

        # Add header if configured
        if self.config.add_header:
            header = ast.Expr(value=ast.Constant(value=self.config.header_template))
            body.append(header)

        # Add imports
        body.extend(imports)

        # Add separator if needed
        if imports and definitions:
            body.append(ast.Expr(value=ast.Constant(value="")))

        # Add definitions
        body.extend(definitions)

        return body

    def _process_node(self, node: ast.AST) -> ast.stmt | None:
        """Process a single AST node for stub generation.

        Args:
            node: AST node to process

        Returns:
            Processed node or None if it should be excluded
        """
        match node:
            case ast.ClassDef():
                return self._process_class(node)
            case ast.FunctionDef() | ast.AsyncFunctionDef():
                return self._process_function(node)
            case ast.AnnAssign() | ast.Assign():
                return self._process_assignment(node)
            case _:
                return None

    def _process_class(self, node: ast.ClassDef) -> ast.ClassDef | None:
        """Process a class definition for stub generation.

        Args:
            node: Class definition to process

        Returns:
            Processed class definition or None if it should be excluded
        """
        if not self.config.include_private and node.name.startswith("_"):
            return None

        # Create new class with same name and bases
        stub_class = ast.ClassDef(
            name=node.name,
            bases=node.bases,
            keywords=node.keywords,
            body=[],
            decorator_list=node.decorator_list,
            type_params=[],  # For Python 3.12+
        )

        # Preserve docstring if configured
        if self.config.include_docstrings:
            docstring = ast.get_docstring(node)
            if docstring:
                stub_class.body.append(ast.Expr(value=ast.Constant(value=docstring)))

        # Process class body
        for child in node.body:
            if processed := self._process_node(child):
                stub_class.body.append(processed)

        return stub_class

    def _process_function(
        self, node: ast.FunctionDef | ast.AsyncFunctionDef
    ) -> ast.FunctionDef | ast.AsyncFunctionDef | None:
        """Process a function definition for stub generation.

        Args:
            node: Function definition to process

        Returns:
            Processed function definition or None if it should be excluded
        """
        if not self.config.include_private and node.name.startswith("_"):
            return None

        # Create new function with same signature
        stub_func = type(node)(
            name=node.name,
            args=node.args,
            returns=node.returns,
            type_params=[],  # For Python 3.12+
            body=[ast.Expr(value=ast.Constant(value=...))],  # Use ellipsis for body
            decorator_list=node.decorator_list,
        )

        # Preserve docstring if configured
        if self.config.include_docstrings:
            docstring = ast.get_docstring(node)
            if docstring:
                stub_func.body.insert(0, ast.Expr(value=ast.Constant(value=docstring)))

        return stub_func

    def _process_assignment(self, node: ast.AnnAssign | ast.Assign) -> ast.stmt | None:
        """Process an assignment for stub generation.

        Args:
            node: Assignment to process

        Returns:
            Processed assignment or None if it should be excluded
        """
        match node:
            case ast.AnnAssign():
                # Preserve annotated assignments
                return node
            case ast.Assign(
                targets=[ast.Name() as name_node], value=ast.Constant() as value
            ):
                # Only keep module-level assignments of constants
                if not name_node.id.startswith("_") or name_node.id.isupper():
                    return ast.AnnAssign(
                        target=name_node,
                        annotation=ast.Name(id=type(value.value).__name__),
                        value=value,
                        simple=1,
                    )
        return None

    def _should_keep_import(self, node: ast.Import | ast.ImportFrom) -> bool:
        """Check if an import should be kept in the stub.

        Args:
            node: Import node to check

        Returns:
            True if the import should be kept
        """
        if isinstance(node, ast.ImportFrom):
            return node.module in self.ESSENTIAL_IMPORTS or any(
                name.name.isupper() for name in node.names
            )
        return any(
            name.name in self.ESSENTIAL_IMPORTS
            or name.name.split(".")[0] in self.ESSENTIAL_IMPORTS
            for name in node.names
        )

    def _import_sort_key(self, node: ast.Import | ast.ImportFrom) -> tuple[int, str]:
        """Get sort key for import statements.

        Args:
            node: Import node to sort

        Returns:
            Tuple of (import type, module name) for sorting
        """
        if isinstance(node, ast.ImportFrom):
            return (1, node.module or "")
        return (0, node.names[0].name)

    def _generate_content(self, node: ast.Module) -> str:
        """Generate stub content from processed AST.

        Args:
            node: Processed AST

        Returns:
            Generated stub content
        """
        return ast.unparse(node)

================
File: src/twat_coding/pystubnik/processors/type_inference.py
================
#!/usr/bin/env -S uv run
"""Type inference processor for stub generation."""

import ast
import re
from typing import Any, ClassVar

from loguru import logger

from ..types.type_system import TypeInferenceError, TypeInfo, TypeRegistry


class TypeInferenceProcessor:
    """Processor for inferring types from code analysis."""

    # Common type patterns in variable names
    TYPE_PATTERNS: ClassVar[dict[re.Pattern[str], type]] = {
        re.compile(r"_str$|_string$"): str,
        re.compile(r"_int$|_count$|_index$"): int,
        re.compile(r"_float$|_ratio$|_rate$"): float,
        re.compile(r"_bool$|_flag$|is_|has_|can_"): bool,
        re.compile(r"_list$|_array$"): list,
        re.compile(r"_dict$|_map$"): dict,
        re.compile(r"_set$"): set,
        re.compile(r"_tuple$"): tuple,
    }

    def __init__(
        self,
        type_registry: TypeRegistry | None = None,
        confidence_threshold: float = 0.5,
        infer_from_usage: bool = True,
        infer_from_assignments: bool = True,
        infer_from_returns: bool = True,
        infer_from_defaults: bool = True,
    ) -> None:
        """Initialize the type inference processor.

        Args:
            type_registry: Registry for type resolution
            confidence_threshold: Minimum confidence for inferred types
            infer_from_usage: Whether to infer types from usage patterns
            infer_from_assignments: Whether to infer types from assignments
            infer_from_returns: Whether to infer types from return statements
            infer_from_defaults: Whether to infer types from default values
        """
        self.type_registry = type_registry or TypeRegistry()
        self.confidence_threshold = confidence_threshold
        self.infer_from_usage = infer_from_usage
        self.infer_from_assignments = infer_from_assignments
        self.infer_from_returns = infer_from_returns
        self.infer_from_defaults = infer_from_defaults

    def infer_types(self, node: ast.AST) -> dict[str, TypeInfo]:
        """Infer types for variables and expressions in an AST.

        Args:
            node: AST node to analyze

        Returns:
            Dictionary mapping names to inferred type information

        Raises:
            TypeInferenceError: If type inference fails
        """
        try:
            inferred_types: dict[str, TypeInfo] = {}

            # Collect type information from different sources
            if self.infer_from_assignments:
                self._infer_from_assignments(node, inferred_types)
            if self.infer_from_usage:
                self._infer_from_usage(node, inferred_types)
            if self.infer_from_returns:
                self._infer_from_returns(node, inferred_types)

            # Filter by confidence threshold
            return {
                name: type_info
                for name, type_info in inferred_types.items()
                if type_info.confidence >= self.confidence_threshold
            }

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to infer types: {e}",
                details={"node_type": type(node).__name__},
            ) from e

    def _infer_from_assignments(
        self, node: ast.AST, types: dict[str, TypeInfo]
    ) -> None:
        """Infer types from assignment statements.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        for child in ast.walk(node):
            match child:
                case ast.AnnAssign(target=ast.Name(id=name), annotation=annotation):
                    # Handle explicitly annotated assignments
                    try:
                        type_info = self.type_registry.resolve_type(
                            annotation, f"annotation:{name}"
                        )
                        types[name] = type_info
                    except Exception as e:
                        logger.warning(
                            f"Failed to resolve type annotation for {name}: {e}"
                        )

                case ast.Assign(targets=[ast.Name(id=name)], value=value):
                    # Infer from assigned value
                    if self.infer_from_defaults and isinstance(value, ast.Constant):
                        type_info = TypeInfo(
                            annotation=type(value.value),
                            source="default",
                            confidence=0.7,
                            metadata={"value": value.value},
                        )
                        types[name] = type_info

                    # Infer from variable name patterns
                    for pattern, typ in self.TYPE_PATTERNS.items():
                        if pattern.search(name):
                            types[name] = TypeInfo(
                                annotation=typ,
                                source="pattern",
                                confidence=0.6,
                                metadata={"pattern": pattern.pattern},
                            )

    def _infer_from_usage(self, node: ast.AST, types: dict[str, TypeInfo]) -> None:
        """Infer types from usage patterns.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        # Track attribute access
        for child in ast.walk(node):
            if isinstance(child, ast.Attribute):
                if isinstance(child.value, ast.Name):
                    name = child.value.id
                    # Record attribute access for potential type inference
                    if name not in types:
                        types[name] = TypeInfo(
                            annotation=Any,
                            source="usage",
                            confidence=0.3,
                            metadata={"attributes": {child.attr}},
                        )
                    else:
                        attrs = types[name].metadata.get("attributes", set())
                        attrs.add(child.attr)
                        types[name].metadata["attributes"] = attrs

    def _infer_from_returns(self, node: ast.AST, types: dict[str, TypeInfo]) -> None:
        """Infer return types from return statements.

        Args:
            node: AST node to analyze
            types: Dictionary to update with inferred types
        """
        for child in ast.walk(node):
            if isinstance(child, ast.FunctionDef | ast.AsyncFunctionDef):
                return_types = set()
                for return_node in ast.walk(child):
                    if isinstance(return_node, ast.Return) and return_node.value:
                        if isinstance(return_node.value, ast.Constant):
                            return_types.add(type(return_node.value.value))
                        elif isinstance(return_node.value, ast.Name):
                            name = return_node.value.id
                            if name in types:
                                return_types.add(types[name].annotation)

                if return_types:
                    types[child.name] = TypeInfo(
                        annotation=next(iter(return_types))
                        if len(return_types) == 1
                        else Any,
                        source="returns",
                        confidence=0.5,
                        metadata={"return_types": list(return_types)},
                    )

================
File: src/twat_coding/pystubnik/types/docstring.py
================
#!/usr/bin/env -S uv run
"""Docstring type extraction and processing."""

import ast
import re
from dataclasses import dataclass
from typing import Any, ClassVar, cast

from docstring_parser import parse as parse_docstring
from loguru import logger

from ..types.type_system import TypeInferenceError, TypeInfo, TypeRegistry


@dataclass
class DocstringTypeInfo:
    """Type information extracted from docstrings."""

    param_types: dict[str, TypeInfo]
    return_type: TypeInfo | None
    yield_type: TypeInfo | None
    raises: list[tuple[TypeInfo, str]]  # (exception_type, description)


class DocstringTypeExtractor:
    """Extract type information from docstrings."""

    # Common type mappings for docstring type names
    TYPE_MAPPINGS: ClassVar[dict[str, Any]] = {
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
        "list": list,
        "dict": dict,
        "tuple": tuple,
        "set": set,
        "None": type(None),
        "any": Any,
        # Add more mappings as needed
    }

    def __init__(self, type_registry: TypeRegistry) -> None:
        """Initialize the docstring type extractor.

        Args:
            type_registry: Type registry for resolving types
        """
        self.type_registry = type_registry
        self._param_pattern = re.compile(
            r":param\s+(\w+)\s*:\s*(?:\(([^)]+)\))?\s*([^\n]+)"
        )
        self._type_pattern = re.compile(r":type\s+(\w+)\s*:\s*([^\n]+)")
        self._rtype_pattern = re.compile(r":rtype:\s*([^\n]+)")
        self._returns_pattern = re.compile(r":returns?:\s*([^\n]+)")
        self._yields_pattern = re.compile(r":yields?:\s*([^\n]+)")
        self._raises_pattern = re.compile(r":raises?\s+([^:]+):\s*([^\n]+)")

    def _extract_param_types(self, doc: Any) -> dict[str, TypeInfo]:
        """Extract parameter types from docstring."""
        param_types = {}
        for param in doc.params:
            if param.type_name:
                try:
                    type_info = self._parse_type_string(param.type_name)
                    param_types[param.arg_name] = type_info
                except TypeInferenceError as e:
                    logger.warning(
                        f"Failed to parse type for parameter {param.arg_name}: {e}"
                    )
        return param_types

    def _extract_return_type(self, doc: Any) -> TypeInfo | None:
        """Extract return type from docstring."""
        if doc.returns and doc.returns.type_name:
            try:
                return self._parse_type_string(doc.returns.type_name)
            except TypeInferenceError as e:
                logger.warning(f"Failed to parse return type: {e}")
        return None

    def _extract_yield_type(self, doc: Any) -> TypeInfo | None:
        """Extract yield type from docstring."""
        if hasattr(doc, "yields") and doc.yields and doc.yields.type_name:
            try:
                return self._parse_type_string(doc.yields.type_name)
            except TypeInferenceError as e:
                logger.warning(f"Failed to parse yield type: {e}")
        return None

    def _extract_raises(self, doc: Any) -> list[tuple[TypeInfo, str]]:
        """Extract raises information from docstring."""
        raises = []
        for raises_section in doc.raises:
            if raises_section.type_name:
                try:
                    exc_type = self._parse_type_string(raises_section.type_name)
                    raises.append((exc_type, raises_section.description or ""))
                except TypeInferenceError as e:
                    logger.warning(
                        f"Failed to parse exception type {raises_section.type_name}: {e}"
                    )
        return raises

    def extract_types(
        self,
        node: ast.AsyncFunctionDef | ast.FunctionDef | ast.ClassDef | ast.Module,
    ) -> DocstringTypeInfo | None:
        """Extract type information from an AST node's docstring.

        Args:
            node: AST node to process

        Returns:
            Extracted type information or None if no docstring found

        Raises:
            TypeInferenceError: If docstring parsing fails
        """
        docstring = ast.get_docstring(node)
        if not docstring:
            return None

        # Parse docstring
        doc = parse_docstring(docstring)

        return DocstringTypeInfo(
            param_types=self._extract_param_types(doc),
            return_type=self._extract_return_type(doc),
            yield_type=self._extract_yield_type(doc),
            raises=self._extract_raises(doc),
        )

    def _parse_type_string(self, type_str: str) -> TypeInfo:
        """Parse a type string from a docstring.

        Args:
            type_str: Type string to parse

        Returns:
            Parsed type information

        Raises:
            TypeInferenceError: If type string parsing fails
        """
        try:
            # Clean up type string
            type_str = type_str.strip()

            # Handle simple types
            if type_str in self.TYPE_MAPPINGS:
                return TypeInfo(
                    annotation=self.TYPE_MAPPINGS[type_str],
                    source="docstring",
                    confidence=0.8,
                    metadata={"original": type_str},
                )

            # Handle union types (e.g., "str or None")
            if " or " in type_str:
                types = [
                    self._parse_type_string(t.strip()) for t in type_str.split(" or ")
                ]
                if not types:
                    raise TypeInferenceError("Empty union type")
                if len(types) == 1:
                    return types[0]
                return TypeInfo(
                    annotation=tuple(t.annotation for t in types)[0]
                    | tuple(t.annotation for t in types)[1:],
                    source="docstring",
                    confidence=0.7,
                    metadata={"original": type_str, "union_types": types},
                )

            # Handle generic types (e.g., "List[str]")
            match = re.match(r"(\w+)\[(.*)\]", type_str)
            if match:
                container, content = match.groups()
                if container.lower() in ("list", "set", "tuple"):
                    elem_type = self._parse_type_string(content)
                    container_type = self.TYPE_MAPPINGS.get(container.lower(), list)
                    return TypeInfo(
                        annotation=container_type[elem_type.annotation],  # type: ignore
                        source="docstring",
                        confidence=0.7,
                        metadata={
                            "original": type_str,
                            "container": container,
                            "element_type": elem_type,
                        },
                    )
                elif container.lower() == "dict":
                    key_type, value_type = map(str.strip, content.split(","))
                    key_info = self._parse_type_string(key_type)
                    value_info = self._parse_type_string(value_type)
                    return TypeInfo(
                        annotation=dict[
                            cast(type, key_info.annotation),
                            cast(type, value_info.annotation),
                        ],
                        source="docstring",
                        confidence=0.7,
                        metadata={
                            "original": type_str,
                            "key_type": key_info,
                            "value_type": value_info,
                        },
                    )

            # Try to resolve as a type alias or fall back to Any
            if type_str in self.type_registry._type_aliases:
                return TypeInfo(
                    annotation=self.type_registry._type_aliases[type_str],
                    source="docstring",
                    confidence=0.6,
                    metadata={"original": type_str, "is_alias": True},
                )

            # If all else fails, return Any with low confidence
            return TypeInfo(
                annotation=Any,
                source="docstring",
                confidence=0.1,
                metadata={"original": type_str, "unresolved": True},
            )

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to parse type string '{type_str}': {e}"
            ) from e

================
File: src/twat_coding/pystubnik/types/type_system.py
================
#!/usr/bin/env -S uv run
"""Type system implementation with support for advanced type features."""

import ast
from dataclasses import dataclass
from typing import (
    Annotated,
    Any,
    Protocol,
    TypeVar,
    get_args,
    get_origin,
    get_type_hints,
    runtime_checkable,
)

from ..errors import ErrorCode, StubGenerationError


class TypeInferenceError(StubGenerationError):
    """Error during type inference."""

    def __init__(
        self,
        message: str,
        code: ErrorCode = ErrorCode.TYPE_INFERENCE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
    ) -> None:
        """Initialize type inference error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
        """
        super().__init__(message, code, details, source=source, line_number=line_number)


@dataclass
class TypeInfo:
    """Information about a type annotation."""

    annotation: Any  # The type annotation itself
    source: str  # Where the type came from (annotation, docstring, inference)
    confidence: float  # Confidence score (0-1)
    metadata: dict[str, Any]  # Additional type metadata


@runtime_checkable
class TypeProtocol(Protocol):
    """Protocol for types that can be used in type hints."""

    __name__: str


class TypeRegistry:
    """Registry for type information and aliases."""

    def __init__(self) -> None:
        """Initialize the type registry."""
        self._type_aliases: dict[str, Any] = {}
        self._type_vars: dict[str, TypeVar] = {}
        self._protocols: dict[str, type[TypeProtocol]] = {}
        self._type_cache: dict[tuple[Any, str], TypeInfo] = {}

    def register_alias(self, name: str, target: Any) -> None:
        """Register a type alias.

        Args:
            name: Alias name
            target: Target type
        """
        self._type_aliases[name] = target

    def register_type_var(self, name: str, type_var: TypeVar) -> None:
        """Register a type variable.

        Args:
            name: Type variable name
            type_var: Type variable
        """
        self._type_vars[name] = type_var

    def register_protocol(self, protocol_class: type[TypeProtocol]) -> None:
        """Register a Protocol class.

        Args:
            protocol_class: Protocol class to register
        """
        self._protocols[protocol_class.__name__] = protocol_class

    def resolve_type(self, type_hint: Any, context: str = "") -> TypeInfo:
        """Resolve and normalize a type hint.

        Args:
            type_hint: Type hint to resolve
            context: Context for caching

        Returns:
            Resolved type information

        Raises:
            TypeInferenceError: If type resolution fails
        """
        cache_key = (type_hint, context)
        if cache_key in self._type_cache:
            return self._type_cache[cache_key]

        try:
            # Handle type aliases
            if isinstance(type_hint, str) and type_hint in self._type_aliases:
                resolved = self._type_aliases[type_hint]
            else:
                resolved = type_hint

            # Handle Annotated types
            origin = get_origin(resolved)
            if origin is Annotated:
                args = get_args(resolved)
                if not args:
                    raise TypeInferenceError("Empty Annotated type")
                base_type, *metadata = args
                type_info = TypeInfo(
                    annotation=base_type,
                    source="annotation",
                    confidence=1.0,
                    metadata={"annotations": metadata},
                )
            # Handle TypeVar
            elif isinstance(resolved, TypeVar):
                type_info = TypeInfo(
                    annotation=resolved,
                    source="typevar",
                    confidence=1.0,
                    metadata={
                        "name": resolved.__name__,
                        "bound": resolved.__bound__,
                        "constraints": resolved.__constraints__,
                    },
                )
            # Handle Protocol
            elif (
                isinstance(resolved, type)
                and issubclass(resolved, Protocol)
                and resolved is not Protocol
            ):
                type_info = TypeInfo(
                    annotation=resolved,
                    source="protocol",
                    confidence=1.0,
                    metadata={
                        "attributes": {
                            name: get_type_hints(resolved)[name]
                            for name in dir(resolved)
                            if not name.startswith("_")
                        }
                    },
                )
            # Handle generic types
            elif origin is not None:
                args = get_args(resolved)
                type_info = TypeInfo(
                    annotation=resolved,
                    source="generic",
                    confidence=1.0,
                    metadata={
                        "origin": origin,
                        "args": [self.resolve_type(arg).annotation for arg in args],
                    },
                )
            # Handle simple types
            else:
                type_info = TypeInfo(
                    annotation=resolved,
                    source="direct",
                    confidence=1.0,
                    metadata={},
                )

            self._type_cache[cache_key] = type_info
            return type_info

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to resolve type {type_hint}: {e}",
                details={"context": context},
            ) from e

    def merge_types(
        self,
        types: list[TypeInfo],
        *,
        prefer_explicit: bool = True,
    ) -> TypeInfo:
        """Merge multiple type information entries.

        Args:
            types: List of type information to merge
            prefer_explicit: Whether to prefer explicit annotations over inferred ones

        Returns:
            Merged type information

        Raises:
            TypeInferenceError: If type merging fails
        """
        if not types:
            raise TypeInferenceError("No types to merge")
        if len(types) == 1:
            return types[0]

        try:
            # Sort by confidence and source preference
            sorted_types = sorted(
                types,
                key=lambda t: (
                    t.confidence,
                    1 if t.source == "annotation" and prefer_explicit else 0,
                ),
                reverse=True,
            )

            # Use the highest confidence type as base
            base = sorted_types[0]

            # Merge metadata from all types
            merged_metadata = {}
            for type_info in sorted_types:
                merged_metadata.update(type_info.metadata)

            return TypeInfo(
                annotation=base.annotation,
                source=base.source,
                confidence=base.confidence,
                metadata=merged_metadata,
            )

        except Exception as e:
            raise TypeInferenceError(
                f"Failed to merge types: {e}",
                details={"types": [str(t.annotation) for t in types]},
            ) from e


def extract_type_from_docstring(docstring: str) -> TypeInfo | None:
    """Extract type information from a docstring.

    Args:
        docstring: Docstring to parse

    Returns:
        Extracted type information or None if no type found
    """
    # TODO: Implement docstring type extraction
    # This will be implemented in a separate commit
    return None


def infer_type_from_usage(node: ast.AST) -> TypeInfo | None:
    """Infer type information from usage patterns.

    Args:
        node: AST node to analyze

    Returns:
        Inferred type information or None if inference not possible
    """
    # TODO: Implement type inference from usage
    # This will be implemented in a separate commit
    return None

================
File: src/twat_coding/pystubnik/utils/ast_utils.py
================
#!/usr/bin/env -S uv run
"""AST manipulation utilities."""

import ast
from dataclasses import dataclass
from typing import cast
from weakref import WeakKeyDictionary

# Global dict to store parent references
_parent_refs: WeakKeyDictionary[ast.AST, ast.AST] = WeakKeyDictionary()


@dataclass(frozen=True)
class TruncationConfig:
    """Configuration for AST literal truncation."""

    max_sequence_length: int = 4  # For lists, dicts, sets, tuples
    max_string_length: int = 17  # For strings except docstrings
    max_docstring_length: int = 150  # Default max length for docstrings
    max_file_size: int = 3_000  # Default max file size before removing all docstrings
    truncation_marker: str = "..."


def _get_parent(node: ast.AST) -> ast.AST | None:
    """Get parent node if it exists.

    Args:
        node: AST node

    Returns:
        Parent node if it exists, None otherwise
    """
    return _parent_refs.get(node)


def _truncate_string(s: str, config: TruncationConfig) -> str:
    """Truncate a string value according to config.

    Args:
        s: String to truncate
        config: Truncation configuration

    Returns:
        Truncated string
    """
    if len(s) <= config.max_string_length:
        return s
    return f"{s[: config.max_string_length]}{config.truncation_marker}"


def _truncate_bytes(b: bytes, config: TruncationConfig) -> bytes:
    """Truncate a bytes value according to config.

    Args:
        b: Bytes to truncate
        config: Truncation configuration

    Returns:
        Truncated bytes
    """
    if len(b) <= config.max_string_length:
        return b
    return b[: config.max_string_length] + b"..."


def _truncate_sequence(
    node: ast.List | ast.Set | ast.Tuple, config: TruncationConfig
) -> ast.AST:
    """Truncate a sequence (list, set, tuple) according to config.

    Args:
        node: AST node representing the sequence
        config: Truncation configuration

    Returns:
        Truncated AST node
    """
    truncated_elts = []
    for i, e in enumerate(node.elts):
        if i < config.max_sequence_length:
            truncated_elts.append(cast(ast.expr, truncate_literal(e, config)))
        else:
            truncated_elts.append(ast.Constant(value=config.truncation_marker))
            break
    return type(node)(elts=truncated_elts)


def _truncate_dict(node: ast.Dict, config: TruncationConfig) -> ast.Dict:
    """Truncate a dictionary according to config.

    Args:
        node: AST node representing the dictionary
        config: Truncation configuration

    Returns:
        Truncated AST node
    """
    pairs = []
    for i, (k, v) in enumerate(zip(node.keys, node.values, strict=False)):
        if i < config.max_sequence_length:
            new_v = cast(ast.expr, truncate_literal(v, config))
            pairs.append((k, new_v))
        else:
            pairs.append(
                (
                    ast.Constant(value="..."),
                    ast.Constant(value="..."),
                )
            )
            break
    return ast.Dict(
        keys=[k for k, _ in pairs],
        values=[cast(ast.expr, v) for _, v in pairs],
    )


def truncate_literal(node: ast.AST, config: TruncationConfig) -> ast.AST:
    """Truncate literal values to keep them small in the generated shadow file.

    Args:
        node: AST node to truncate
        config: Truncation configuration

    Returns:
        Truncated AST node
    """
    match node:
        case ast.Constant(value=str() as s):
            # Skip docstrings (handled separately in _preserve_docstring)
            parent = _get_parent(node)
            parent_parent = _get_parent(parent) if parent else None
            if isinstance(parent, ast.Expr) and isinstance(
                parent_parent,
                ast.Module | ast.ClassDef | ast.FunctionDef,
            ):
                return node
            return ast.Constant(value=_truncate_string(s, config))

        case ast.Constant(value=bytes() as b):
            return ast.Constant(value=_truncate_bytes(b, config))

        case ast.List() | ast.Set() | ast.Tuple():
            node_cast = cast(ast.List | ast.Set | ast.Tuple, node)
            return _truncate_sequence(node_cast, config)

        case ast.Dict():
            return _truncate_dict(cast(ast.Dict, node), config)

        case _:
            return node


def attach_parents(node: ast.AST) -> None:
    """Attach parent references to all nodes in an AST.

    Args:
        node: Root AST node
    """
    for parent in ast.walk(node):
        for child in ast.iter_child_nodes(parent):
            _parent_refs[child] = parent


def get_docstring(
    node: ast.AsyncFunctionDef | ast.FunctionDef | ast.ClassDef | ast.Module,
) -> str | None:
    """Get docstring from an AST node.

    Args:
        node: AST node to extract docstring from (must be a module, class, or function)

    Returns:
        Docstring if found, None otherwise
    """
    docstring = ast.get_docstring(node)
    if docstring:
        return docstring.strip()
    return None


def is_empty_expr(node: ast.AST) -> bool:
    """Check if a node is an empty or whitespace-only expression.

    Args:
        node: AST node to check

    Returns:
        True if node is an empty expression
    """
    return (
        isinstance(node, ast.Expr)
        and isinstance(node.value, ast.Constant)
        and (not node.value.value or str(node.value.value).isspace())
    )

================
File: src/twat_coding/pystubnik/utils/display.py
================
#!/usr/bin/env -S uv run
"""Display utilities for file trees and progress indicators."""

from pathlib import Path
from typing import Any

from rich.console import Console
from rich.tree import Tree

console = Console()


def print_file_tree(paths: list[Path]) -> None:
    """Print a tree representation of file paths.

    Args:
        paths: List of paths to display
    """
    tree: dict[str, Any] = {}
    for path in sorted(paths):
        add_to_tree(tree, path.parts)

    root = Tree("📁 Project")
    build_rich_tree(root, tree)
    console.print(root)


def add_to_tree(tree: dict[str, Any], components: list[str] | tuple[str, ...]) -> None:
    """Add a path to the tree structure.

    Args:
        tree: Tree dictionary to update
        components: Path components
    """
    current = tree
    for component in components:
        if component not in current:
            current[component] = {}
        current = current[component]


def build_rich_tree(tree: Tree, data: dict[str, Any], prefix: str = "") -> None:
    """Build a rich Tree from the tree dictionary.

    Args:
        tree: Rich Tree to update
        data: Tree dictionary
        prefix: Current path prefix
    """
    for name, subtree in sorted(data.items()):
        icon = "📁" if subtree else "📄"
        branch = tree.add(f"{icon} {name}")
        if subtree:
            build_rich_tree(branch, subtree, f"{prefix}/{name}" if prefix else name)


def print_progress(message: str, current: int, total: int) -> None:
    """Print progress information.

    Args:
        message: Progress message
        current: Current progress value
        total: Total progress value
    """
    percentage = (current / total) * 100 if total > 0 else 0
    console.print(f"{message}: [{current}/{total}] {percentage:.1f}%")

================
File: src/twat_coding/pystubnik/utils/memory.py
================
#!/usr/bin/env -S uv run
"""Memory monitoring utilities."""

import ast
import asyncio
import gc
import os
import threading
from collections.abc import AsyncGenerator, Generator
from contextlib import contextmanager
from dataclasses import dataclass
from typing import Any

from loguru import logger

try:
    import psutil
    from memory_profiler import profile as memory_profile

    HAS_MEMORY_TOOLS = True
except ImportError:
    HAS_MEMORY_TOOLS = False
    psutil = None
    memory_profile = None


@dataclass
class MemoryStats:
    """Memory usage statistics."""

    rss: int  # Resident Set Size in bytes
    vms: int  # Virtual Memory Size in bytes
    shared: int  # Shared memory in bytes
    text: int  # Text segment memory in bytes
    data: int  # Data segment memory in bytes
    lib: int  # Library memory in bytes
    dirty: int  # Dirty pages in bytes
    peak_rss: int  # Peak RSS memory in bytes


class MemoryMonitor:
    """Monitor memory usage of the process."""

    def __init__(self, interval: float = 1.0) -> None:
        """Initialize the memory monitor.

        Args:
            interval: Monitoring interval in seconds
        """
        if not HAS_MEMORY_TOOLS:
            logger.warning(
                "psutil and memory_profiler not available, memory monitoring disabled"
            )
            return

        self.interval = interval
        self._process = psutil.Process(os.getpid())
        self._stop_event = threading.Event()
        self._stats: list[MemoryStats] = []
        self._monitor_thread: threading.Thread | None = None

    def start(self) -> None:
        """Start monitoring memory usage."""
        if not HAS_MEMORY_TOOLS:
            return

        if self._monitor_thread is not None:
            return

        def _monitor() -> None:
            while not self._stop_event.is_set():
                try:
                    meminfo = self._process.memory_info()
                    stats = MemoryStats(
                        rss=meminfo.rss,
                        vms=meminfo.vms,
                        shared=meminfo.shared,
                        text=meminfo.text,
                        data=meminfo.data,
                        lib=meminfo.lib,
                        dirty=meminfo.dirty,
                        peak_rss=self._process.memory_info().rss,
                    )
                    self._stats.append(stats)
                    logger.debug(
                        f"Memory usage: RSS={stats.rss / 1024 / 1024:.1f}MB, "
                        f"VMS={stats.vms / 1024 / 1024:.1f}MB"
                    )
                except Exception as e:
                    logger.error(f"Failed to collect memory stats: {e}")
                self._stop_event.wait(self.interval)

        self._monitor_thread = threading.Thread(target=_monitor, daemon=True)
        self._monitor_thread.start()

    def stop(self) -> None:
        """Stop monitoring memory usage."""
        if not HAS_MEMORY_TOOLS or self._monitor_thread is None:
            return
        self._stop_event.set()
        self._monitor_thread.join()
        self._monitor_thread = None

    @property
    def stats(self) -> list[MemoryStats]:
        """Get collected memory statistics."""
        return self._stats.copy()

    @property
    def peak_memory(self) -> int:
        """Get peak memory usage in bytes."""
        if not self._stats:
            return 0
        return max(stat.peak_rss for stat in self._stats)

    def clear_stats(self) -> None:
        """Clear collected statistics."""
        self._stats.clear()


@contextmanager
def memory_monitor(interval: float = 1.0) -> Generator[MemoryMonitor, None, None]:
    """Context manager for memory monitoring.

    Args:
        interval: Monitoring interval in seconds

    Yields:
        Memory monitor instance
    """
    monitor = MemoryMonitor(interval)
    monitor.start()
    try:
        yield monitor
    finally:
        monitor.stop()


async def stream_process_ast(
    node: ast.AST,
    chunk_size: int = 1000,
    gc_interval: int = 10,
) -> AsyncGenerator[list[ast.AST], None]:
    """Process AST nodes in chunks to reduce memory usage.

    Args:
        node: AST node to process
        chunk_size: Number of nodes to process in each chunk
        gc_interval: Number of chunks between garbage collection

    Yields:
        Lists of processed AST nodes
    """
    nodes = list(ast.walk(node))
    chunks = [nodes[i : i + chunk_size] for i in range(0, len(nodes), chunk_size)]

    for i, chunk in enumerate(chunks):
        # Process nodes in the chunk
        processed = []
        for node in chunk:
            # Add your node processing logic here
            processed.append(node)

        # Run garbage collection periodically
        if i > 0 and i % gc_interval == 0:
            gc.collect()

        # Let other tasks run
        await asyncio.sleep(0)

        yield processed


def profile_memory(func: Any) -> Any:
    """Decorator to profile memory usage of a function.

    Args:
        func: Function to profile

    Returns:
        Profiled function
    """
    if not HAS_MEMORY_TOOLS:
        logger.warning("memory_profiler not available, profiling disabled")
        return func
    return memory_profile(func)

================
File: src/twat_coding/pystubnik/__init__.py
================
#!/usr/bin/env python3
"""Smart stub generation for Python code.

This package provides tools for generating high-quality type stubs for Python code,
with support for multiple backends and intelligent processing of imports,
docstrings, and code importance.
"""

from collections.abc import Mapping, Sequence
from importlib.metadata import version
from pathlib import Path
from typing import Protocol

from loguru import logger

from .backends import StubBackend
from .backends.ast_backend import ASTBackend
from .backends.mypy_backend import MypyBackend
from .config import StubConfig
from .core.config import (
    Backend,
    ImportanceLevel,
    PathConfig,
    ProcessingConfig,
    RuntimeConfig,
    StubGenConfig,
    TruncationConfig,
)
from .core.types import (
    ArgInfo,
    ClassInfo,
    FunctionInfo,
    ModuleInfo,
    PathLike,
    StubResult,
)
from .core.utils import setup_logging
from .errors import ASTError, ConfigError, ErrorCode, MyPyError, StubGenerationError
from .processors.docstring import DocstringProcessor
from .processors.importance import ImportanceConfig, ImportanceProcessor
from .processors.imports import ImportProcessor


def _convert_to_stub_config(config: StubGenConfig) -> StubConfig:
    """Convert StubGenConfig to StubConfig.

    Args:
        config: Source configuration

    Returns:
        Converted configuration
    """
    return StubConfig(
        input_path=config.paths.files[0] if config.paths.files else Path("."),
        output_path=config.paths.output_dir,
        backend="ast",  # Default to AST backend
        parallel=config.runtime.parallel,
        max_workers=config.runtime.max_workers,
        infer_types=config.processing.infer_property_types,
        preserve_literals=True,  # We handle this in the AST backend
        docstring_type_hints=config.processing.include_docstrings,
        python_version=config.runtime.python_version,
        no_import=config.runtime.no_import,
        inspect=config.runtime.inspect,
        doc_dir=str(config.paths.doc_dir) if config.paths.doc_dir else "",
        search_paths=config.paths.search_paths,
        interpreter=config.runtime.interpreter,
        ignore_errors=config.runtime.ignore_errors,
        parse_only=config.runtime.parse_only,
        include_private=config.processing.include_private,
        modules=config.paths.modules,
        packages=config.paths.packages,
        files=config.paths.files,
        verbose=config.runtime.verbose,
        quiet=config.runtime.quiet,
        export_less=config.processing.export_less,
        importance_patterns=dict(config.processing.importance_patterns),
        max_docstring_length=config.truncation.max_docstring_length,
        include_type_comments=config.processing.include_type_comments,
        infer_property_types=config.processing.infer_property_types,
        line_length=88,  # Default line length
        sort_imports=True,  # Default to sorting imports
        add_header=True,  # Default to adding header
    )


class Processor(Protocol):
    """Protocol for stub processors."""

    def process(self, stub: StubResult) -> StubResult:
        """Process a stub result.

        Args:
            stub: Input stub result

        Returns:
            Processed stub result
        """
        ...


class SmartStubGenerator:
    """Main interface for generating smart stubs."""

    def __init__(
        self,
        *,
        # Path configuration
        output_dir: str | Path = "out",
        doc_dir: str | Path | None = None,
        search_paths: Sequence[str | Path] = (),
        modules: Sequence[str] = (),
        packages: Sequence[str] = (),
        files: Sequence[str | Path] = (),
        # Runtime configuration
        backend: Backend | str = Backend.HYBRID,
        python_version: tuple[int, int] | None = None,
        interpreter: str | Path | None = None,
        no_import: bool = False,
        inspect: bool = False,
        parse_only: bool = False,
        ignore_errors: bool = True,
        verbose: bool = False,
        quiet: bool = True,
        parallel: bool = True,
        max_workers: int | None = None,
        # Processing configuration
        include_docstrings: bool = True,
        include_private: bool = False,
        include_type_comments: bool = True,
        infer_property_types: bool = True,
        export_less: bool = False,
        importance_patterns: Mapping[str, float] | None = None,
        importance_keywords: set[str] | None = None,
        # Truncation configuration
        max_sequence_length: int | None = None,
        max_string_length: int | None = None,
        max_docstring_length: int | None = None,
        max_file_size: int | None = None,
        truncation_marker: str | None = None,
    ):
        """Initialize the stub generator with configuration.

        Args:
            output_dir: Directory to write stubs to
            doc_dir: Directory containing .rst documentation
            search_paths: Module search paths
            modules: Module names to process
            packages: Package names to process recursively
            files: Specific files to process
            backend: Stub generation backend (AST, MYPY, or HYBRID)
            python_version: Python version to target
            interpreter: Python interpreter to use
            no_import: Don't import modules
            inspect: Use runtime inspection
            parse_only: Only parse, no semantic analysis
            ignore_errors: Continue on errors
            verbose: Show detailed output
            quiet: Minimal output
            parallel: Use parallel processing
            max_workers: Maximum worker threads
            include_docstrings: Include docstrings in stubs
            include_private: Include private symbols
            include_type_comments: Include type comments
            infer_property_types: Try to infer property types
            export_less: Don't export imported names
            importance_patterns: Patterns for importance scoring
            importance_keywords: Keywords indicating importance
            max_sequence_length: Maximum sequence length
            max_string_length: Maximum string length
            max_docstring_length: Maximum docstring length
            max_file_size: Maximum file size
            truncation_marker: Marker for truncated content
        """
        # Convert backend string to enum
        if isinstance(backend, str):
            backend = Backend[backend.upper()]

        # Create configuration objects
        self.config = StubGenConfig(
            paths=PathConfig(
                output_dir=Path(output_dir),
                doc_dir=Path(doc_dir) if doc_dir else None,
                search_paths=[Path(p) for p in search_paths],
                modules=list(modules),
                packages=list(packages),
                files=[Path(f) for f in files],
            ),
            runtime=RuntimeConfig.create(
                backend=backend,
                python_version=python_version,
                interpreter=interpreter,
                no_import=no_import,
                inspect=inspect,
                parse_only=parse_only,
                ignore_errors=ignore_errors,
                verbose=verbose,
                quiet=quiet,
                parallel=parallel,
                max_workers=max_workers,
            ),
            processing=ProcessingConfig(
                include_docstrings=include_docstrings,
                include_private=include_private,
                include_type_comments=include_type_comments,
                infer_property_types=infer_property_types,
                export_less=export_less,
                importance_patterns=dict(importance_patterns or {}),
                importance_keywords=set(importance_keywords or set()),
            ),
            truncation=TruncationConfig(
                max_sequence_length=max_sequence_length or 4,
                max_string_length=max_string_length or 17,
                max_docstring_length=max_docstring_length or 150,
                max_file_size=max_file_size or 3_000,
                truncation_marker=truncation_marker or "...",
            ),
        )

        # Configure logging
        logger.remove()
        logger.add(
            lambda msg: print(msg, end=""),
            format="<blue>{time:HH:mm:ss}</blue> | {message}",
            level="DEBUG" if verbose else "INFO",
            catch=True,
        )

        # Initialize processors
        self.processors: list[Processor] = [
            DocstringProcessor(
                style="google",
                max_length=self.config.truncation.max_docstring_length,
                preserve_sections=["Args", "Returns", "Yields", "Raises"],
            ),
            ImportanceProcessor(
                config=ImportanceConfig(
                    patterns=dict(self.config.processing.importance_patterns),
                    keywords=self.config.processing.importance_keywords,
                )
            ),
            ImportProcessor(),
        ]

    def _initialize_backend(self) -> StubBackend:
        """Initialize the appropriate backend based on configuration.

        Returns:
            Initialized backend
        """
        match self.config.runtime.backend:
            case Backend.AST:
                return ASTBackend(_convert_to_stub_config(self.config))
            case Backend.MYPY:
                return MypyBackend(_convert_to_stub_config(self.config))
            case Backend.HYBRID:
                # TODO: Implement hybrid backend
                logger.warning("Hybrid backend not implemented, falling back to AST")
                return ASTBackend(_convert_to_stub_config(self.config))
            case _:
                raise ValueError(f"Unknown backend: {self.config.runtime.backend}")

    def _process_file(self, backend: StubBackend, file_path: Path) -> None:
        """Process a single file.

        Args:
            backend: Backend to use for stub generation
            file_path: Path to the file to process
        """
        try:
            # Generate stub
            result = backend.generate_stub(file_path)
            if isinstance(result, StubResult):
                # Apply processors
                for processor in self.processors:
                    result = processor.process(result)

                # Write stub
                output_path = (
                    self.config.paths.output_dir / file_path.with_suffix(".pyi").name
                )
                output_path.write_text(result.stub_content)
            else:
                logger.error(f"Invalid result type for {file_path}")

        except Exception as e:
            logger.error(f"Failed to process {file_path}: {e}")
            if not self.config.runtime.ignore_errors:
                raise

    def generate(self) -> None:
        """Generate stubs according to configuration."""
        try:
            logger.info(
                f"Generating stubs using {self.config.runtime.backend.name} backend"
            )
            logger.info(f"Output directory: {self.config.paths.output_dir}")

            # Create output directory
            self.config.paths.output_dir.mkdir(parents=True, exist_ok=True)

            # Initialize backend
            backend = self._initialize_backend()

            # Process files
            for file_path in self.config.paths.files:
                self._process_file(backend, file_path)

            logger.info("Stub generation completed successfully")

        except Exception as e:
            logger.error(f"Failed to generate stubs: {e}")
            if self.config.runtime.verbose:
                import traceback

                logger.debug(traceback.format_exc())
            if not self.config.runtime.ignore_errors:
                raise

    def generate_for_file(self, file_path: str | Path) -> StubResult:
        """Generate stub for a single file.

        Args:
            file_path: Path to the Python file

        Returns:
            Generated stub result
        """
        # TODO: Implement single file stub generation
        raise NotImplementedError

    def generate_for_module(self, module_name: str) -> StubResult:
        """Generate stub for a module by name.

        Args:
            module_name: Fully qualified module name

        Returns:
            Generated stub result
        """
        # TODO: Implement module stub generation
        raise NotImplementedError


async def generate_stub(
    source_path: PathLike,
    output_path: PathLike | None = None,
    backend: str = "ast",
    config: StubGenConfig | None = None,
) -> StubResult:
    """Generate a type stub for a Python source file.

    Args:
        source_path: Path to the source file
        output_path: Optional path to write the stub file
        backend: Backend to use for stub generation ("ast" or "mypy")
        config: Optional configuration for stub generation

    Returns:
        StubResult containing the generated stub and metadata

    Raises:
        ValueError: If the specified backend is not supported
    """
    source_path = Path(source_path)
    output_path_obj = Path(output_path) if output_path else None

    config = config or StubGenConfig(paths=PathConfig())

    # Convert StubGenConfig to StubConfig
    stub_config = _convert_to_stub_config(config)
    stub_config.input_path = source_path
    stub_config.output_path = output_path_obj
    stub_config.backend = "ast" if backend == "ast" else "mypy"  # Convert to Literal

    # Initialize backend
    backend_obj: StubBackend
    if backend == "ast":
        backend_obj = ASTBackend(stub_config)
    elif backend == "mypy":
        backend_obj = MypyBackend(stub_config)
    else:
        raise ValueError(f"Unsupported backend: {backend}")

    # Generate stub
    result = await backend_obj.generate_stub(source_path)
    if not isinstance(result, StubResult):
        raise TypeError(f"Expected StubResult, got {type(result)}")

    # Write stub if output path is specified
    if output_path_obj:
        output_path_obj.write_text(result.stub_content)

    return result


# Configure default logging
setup_logging()

# Version information
__version__ = version("twat_coding")
__author__ = "Adam Twardoch"
__license__ = "MIT"

__all__ = [
    "ArgInfo",
    "ASTBackend",
    "Backend",
    "ClassInfo",
    "FunctionInfo",
    "ImportanceLevel",
    "ModuleInfo",
    "MypyBackend",
    "SmartStubGenerator",
    "StubBackend",
    "StubGenConfig",
    "StubResult",
    "generate_stub",
    "RuntimeConfig",
    "StubConfig",
    "ASTError",
    "ConfigError",
    "ErrorCode",
    "MyPyError",
    "StubGenerationError",
]

================
File: src/twat_coding/pystubnik/config.py
================
#!/usr/bin/env -S uv run
"""Configuration system for stub generation."""

import os
import sys
from collections.abc import Sequence
from pathlib import Path
from typing import Any, Literal

from pydantic import BaseModel, Field, field_validator
from pydantic_settings import BaseSettings, SettingsConfigDict

from .errors import ConfigError, ErrorCode


class FileLocations(BaseModel):
    """File locations for stub generation."""

    source_path: Path = Field(..., description="Path to Python source file")
    input_dir: Path = Field(..., description="Base input directory")
    output_dir: Path = Field(..., description="Base output directory")

    model_config = SettingsConfigDict(
        arbitrary_types_allowed=True,
        validate_assignment=True,
    )

    @field_validator("source_path")
    @classmethod
    def validate_source_path(cls, v: Path) -> Path:
        """Validate source file exists."""
        if not v.exists():
            raise ConfigError(
                "Source file does not exist",
                ErrorCode.CONFIG_VALIDATION_ERROR,
                source=str(v),
            )
        return v

    @field_validator("input_dir")
    @classmethod
    def validate_input_dir(cls, v: Path) -> Path:
        """Validate input directory exists."""
        if not v.exists():
            raise ConfigError(
                "Input directory does not exist",
                ErrorCode.CONFIG_VALIDATION_ERROR,
                source=str(v),
            )
        return v

    @field_validator("output_dir")
    @classmethod
    def validate_output_dir(cls, v: Path) -> Path:
        """Validate and create output directory."""
        try:
            v.mkdir(parents=True, exist_ok=True)
            return v
        except Exception as e:
            raise ConfigError(
                f"Failed to create output directory: {e}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(v),
            ) from e

    @property
    def output_path(self) -> Path:
        """Calculate output path based on input path and bases."""
        try:
            rel_path = self.source_path.relative_to(self.input_dir)
            return self.output_dir / rel_path
        except ValueError as e:
            raise ConfigError(
                f"Source file {self.source_path} is not within input directory {self.input_dir}",
                ErrorCode.CONFIG_VALIDATION_ERROR,
            ) from e

    def check_paths(self) -> None:
        """Validate file locations and create output directories."""
        # Create output directory and verify permissions
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        if not os.access(self.output_path.parent, os.W_OK):
            raise ConfigError(
                f"No write permission: {self.output_path.parent}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(self.output_path.parent),
            )


class StubConfig(BaseModel):
    """Configuration for stub generation."""

    model_config = SettingsConfigDict(
        extra="forbid",
        frozen=True,
        validate_assignment=True,
    )

    # Input/Output settings
    input_path: Path = Field(
        ...,
        description="Path to Python source files or directory",
    )
    output_path: Path | None = Field(
        None,
        description="Path to output directory for stubs",
    )
    include_patterns: list[str] = Field(
        default_factory=lambda: ["*.py"],
        description="Glob patterns for files to include",
    )
    exclude_patterns: list[str] = Field(
        default_factory=lambda: ["test_*.py", "*_test.py"],
        description="Glob patterns for files to exclude",
    )

    # Processing settings
    backend: Literal["ast", "mypy"] = Field(
        "ast",
        description="Backend to use for stub generation",
    )
    parallel: bool = Field(
        True,
        description="Enable parallel processing",
    )
    max_workers: int | None = Field(
        None,
        description="Maximum number of worker threads",
        ge=1,
    )

    # Type inference settings
    infer_types: bool = Field(
        True,
        description="Enable type inference",
    )
    preserve_literals: bool = Field(
        False,
        description="Preserve literal values in stubs",
    )
    docstring_type_hints: bool = Field(
        True,
        description="Extract type hints from docstrings",
    )

    # Output settings
    line_length: int = Field(
        88,
        description="Maximum line length for output",
        ge=1,
    )
    sort_imports: bool = Field(
        True,
        description="Sort imports in output",
    )
    add_header: bool = Field(
        True,
        description="Add header to generated files",
    )

    # MyPy-specific settings
    python_version: tuple[int, int] = Field(
        default_factory=lambda: (sys.version_info.major, sys.version_info.minor),
        description="Python version to target",
    )
    no_import: bool = Field(
        False,
        description="Don't import modules, just parse and analyze",
    )
    inspect: bool = Field(
        False,
        description="Import and inspect modules instead of parsing source",
    )
    doc_dir: str = Field(
        "",
        description="Path to .rst documentation directory",
    )
    search_paths: Sequence[str | Path] = Field(
        default_factory=list,
        description="Module search paths",
    )
    interpreter: str | Path = Field(
        default_factory=lambda: sys.executable,
        description="Python interpreter to use",
    )
    ignore_errors: bool = Field(
        True,
        description="Ignore errors during stub generation",
    )
    parse_only: bool = Field(
        False,
        description="Don't do semantic analysis of source",
    )
    include_private: bool = Field(
        False,
        description="Include private objects in stubs",
    )
    modules: Sequence[str] = Field(
        default_factory=list,
        description="List of module names to process",
    )
    packages: Sequence[str] = Field(
        default_factory=list,
        description="List of package names to process recursively",
    )
    files: Sequence[str | Path] = Field(
        default_factory=list,
        description="List of files to process",
    )
    verbose: bool = Field(
        False,
        description="Show more detailed output",
    )
    quiet: bool = Field(
        True,
        description="Show minimal output",
    )
    export_less: bool = Field(
        False,
        description="Don't export imported names",
    )
    importance_patterns: dict[str, float] = Field(
        default_factory=dict,
        description="Dict of regex patterns to importance scores",
    )
    max_docstring_length: int = Field(
        500,
        description="Maximum length for included docstrings",
        ge=1,
    )
    include_type_comments: bool = Field(
        True,
        description="Include type comments in stubs",
    )
    infer_property_types: bool = Field(
        True,
        description="Try to infer property types from docstrings",
    )

    @field_validator("output_path", mode="before")
    @classmethod
    def validate_output_path(cls, v: Any) -> Path | None:
        """Validate and create output directory if it doesn't exist."""
        if v is None:
            return None
        try:
            path = Path(v)
            path.mkdir(parents=True, exist_ok=True)
            return path
        except Exception as e:
            raise ConfigError(
                f"Failed to create output directory: {e}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(v),
            ) from e

    @field_validator("input_path")
    @classmethod
    def validate_input_path(cls, v: Path) -> Path:
        """Validate input path exists."""
        if not v.exists():
            raise ConfigError(
                "Input path does not exist",
                ErrorCode.CONFIG_VALIDATION_ERROR,
                source=str(v),
            )
        return v

    def get_file_locations(self, source_path: Path) -> FileLocations:
        """Create FileLocations for a source file.

        Args:
            source_path: Path to source file

        Returns:
            FileLocations instance

        Raises:
            ConfigError: If paths are invalid
        """
        return FileLocations(
            source_path=source_path,
            input_dir=self.input_path,
            output_dir=self.output_path or self.input_path.parent / "stubs",
        )


class RuntimeConfig(BaseSettings):
    """Runtime configuration for the application."""

    model_config = SettingsConfigDict(
        env_prefix="PYSTUBNIK_",
        env_file=".env",
        extra="ignore",
    )

    # Logging settings
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR"] = "INFO"
    log_file: Path | None = None

    # Performance settings
    cache_dir: Path = Field(default_factory=lambda: Path(".cache"))
    memory_limit: int | None = None  # In megabytes
    timeout: int | None = None  # In seconds

    # Development settings
    debug: bool = False
    profile: bool = False

    @field_validator("cache_dir", mode="before")
    @classmethod
    def validate_cache_dir(cls, v: Any) -> Path:
        """Validate and create cache directory."""
        try:
            path = Path(v)
            path.mkdir(parents=True, exist_ok=True)
            return path
        except Exception as e:
            raise ConfigError(
                f"Failed to create cache directory: {e}",
                ErrorCode.CONFIG_IO_ERROR,
                source=str(v),
            ) from e

================
File: src/twat_coding/pystubnik/errors.py
================
#!/usr/bin/env -S uv run
"""Error handling system for stub generation."""

from enum import Enum
from typing import Any


class ErrorCode(str, Enum):
    """Error codes for stub generation."""

    # AST-related errors (AST*)
    AST_PARSE_ERROR = "AST001"  # Failed to parse Python source
    AST_VISIT_ERROR = "AST002"  # Error during AST node visitation
    AST_TRANSFORM_ERROR = "AST003"  # Error transforming AST to stub

    # Type inference errors (TYPE*)
    TYPE_INFERENCE_ERROR = "TYPE001"  # Failed to infer type
    TYPE_CONFLICT_ERROR = "TYPE002"  # Conflicting type information
    TYPE_VALIDATION_ERROR = "TYPE003"  # Invalid type annotation

    # Configuration errors (CFG*)
    CONFIG_PARSE_ERROR = "CFG001"  # Failed to parse configuration
    CONFIG_VALIDATION_ERROR = "CFG002"  # Invalid configuration
    CONFIG_IO_ERROR = "CFG003"  # Configuration file I/O error

    # Backend errors (BACKEND*)
    BACKEND_ERROR = "BACKEND001"  # Generic backend error
    BACKEND_NOT_FOUND = "BACKEND002"  # Backend not available
    BACKEND_CONFIG_ERROR = "BACKEND003"  # Backend configuration error

    # I/O errors (IO*)
    IO_READ_ERROR = "IO001"  # Failed to read source file
    IO_WRITE_ERROR = "IO002"  # Failed to write stub file
    IO_PERMISSION_ERROR = "IO003"  # Permission denied

    # Processing errors (PROC*)
    PROC_TIMEOUT_ERROR = "PROC001"  # Processing timeout
    PROC_MEMORY_ERROR = "PROC002"  # Out of memory
    PROC_INTERRUPT_ERROR = "PROC003"  # Processing interrupted


class StubGenerationError(Exception):
    """Base class for all stub generation errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
    ) -> None:
        """Initialize the error.

        Args:
            message: Human-readable error message
            code: Error code from ErrorCode enum or string
            details: Additional error details
            source: Source code or file where error occurred
            line_number: Line number where error occurred
        """
        self.code = ErrorCode(code) if isinstance(code, str) else code
        self.details = details or {}
        self.source = source
        self.line_number = line_number
        super().__init__(message)

    def __str__(self) -> str:
        """Return formatted error message."""
        msg = f"[{self.code}] {super().__str__()}"
        if self.source and self.line_number:
            msg = f"{msg}\nAt {self.source}:{self.line_number}"
        if self.details:
            msg = f"{msg}\nDetails: {self.details}"
        return msg


class ASTError(StubGenerationError):
    """AST processing errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.AST_PARSE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
        node_type: str | None = None,
    ) -> None:
        """Initialize AST error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
            node_type: Type of AST node where error occurred
        """
        if node_type:
            details = details or {}
            details["node_type"] = node_type
        super().__init__(message, code, details, source=source, line_number=line_number)


class MyPyError(StubGenerationError):
    """MyPy backend errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.BACKEND_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        line_number: int | None = None,
        mypy_error_code: str | None = None,
    ) -> None:
        """Initialize MyPy error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Source code or file
            line_number: Line number
            mypy_error_code: Original MyPy error code
        """
        if mypy_error_code:
            details = details or {}
            details["mypy_error_code"] = mypy_error_code
        super().__init__(message, code, details, source=source, line_number=line_number)


class ConfigError(StubGenerationError):
    """Configuration errors."""

    def __init__(
        self,
        message: str,
        code: ErrorCode | str = ErrorCode.CONFIG_PARSE_ERROR,
        details: dict[str, Any] | None = None,
        *,
        source: str | None = None,
        config_key: str | None = None,
    ) -> None:
        """Initialize configuration error.

        Args:
            message: Error message
            code: Error code
            details: Additional details
            source: Configuration file path
            config_key: Key in configuration that caused error
        """
        if config_key:
            details = details or {}
            details["config_key"] = config_key
        super().__init__(message, code, details, source=source)

================
File: src/twat_coding/pystubnik/make_stubs_ast.py
================
#!/usr/bin/env python3
"""
Generate shadow Python files containing only signatures and imports,
while truncating large literal strings, bytes, lists, dicts, etc.
"""

import ast
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from pathlib import Path
from typing import Any, cast

from loguru import logger
from pydantic import BaseModel
from rich.console import Console

# Constants
MAX_WORKERS = os.cpu_count() or 1
MAX_SEQUENCE_LENGTH: int = 4  # For lists, dicts, sets, tuples
MAX_STRING_LENGTH: int = 17  # For strings except docstrings
MAX_DOCSTRING_LENGTH: int = 150  # Default max length for docstrings
MAX_FILE_SIZE: int = 3_000  # Default max file size before removing all docstrings
TRUNCATION_MARKER: str = "..."

console = Console()


@dataclass(frozen=True)
class Config:
    """Configuration settings."""

    max_sequence_length: int = MAX_SEQUENCE_LENGTH
    max_string_length: int = MAX_STRING_LENGTH
    max_docstring_length: int = MAX_DOCSTRING_LENGTH
    max_file_size: int = MAX_FILE_SIZE
    truncation_marker: str = TRUNCATION_MARKER


class FileLocations(BaseModel):
    """File locations for shadow file generation."""

    py_path: Path
    in_dir: Path
    out_dir: Path

    class Config:
        arbitrary_types_allowed = True

    @property
    def output_path(self) -> Path:
        """Calculate output path based on input path and bases."""
        rel_path = self.py_path.relative_to(self.in_dir)
        return self.out_dir / rel_path

    def check_paths(self) -> None:
        """Validate file locations and create output directories."""
        if not self.py_path.exists():
            msg = f"Input file not found: {self.py_path}"
            raise FileNotFoundError(msg)
        if not self.py_path.is_relative_to(self.in_dir):
            base = self.in_dir
            msg = f"Input file {self.py_path} is not within base {base}"
            raise ValueError(msg)

        # Create output directory and verify permissions
        self.output_path.parent.mkdir(parents=True, exist_ok=True)
        if not os.access(self.output_path.parent, os.W_OK):
            msg = f"No write permission: {self.output_path.parent}"
            raise PermissionError(msg)


def _is_docstring(node: ast.AST) -> bool:
    """Check if the node is a docstring."""
    parent = getattr(node, "parent", None)
    parent_parent = getattr(parent, "parent", None)
    return isinstance(parent, ast.Expr) and isinstance(
        parent_parent, ast.Module | ast.ClassDef | ast.FunctionDef
    )


def _truncate_string(s: str, max_length: int, marker: str) -> str:
    """Truncate a string to the specified length."""
    if len(s) <= max_length:
        return s
    return f"{s[:max_length]}{marker}"


def _truncate_bytes(b: bytes, max_length: int) -> bytes:
    """Truncate bytes to the specified length."""
    if len(b) <= max_length:
        return b
    return bytes(b[:max_length]) + b"..."


def _truncate_sequence(
    elts: list[ast.expr], max_length: int, config: Config
) -> list[ast.expr]:
    """Truncate a sequence of AST elements."""
    truncated_elts = []
    for i, e in enumerate(elts):
        if i < max_length:
            truncated_elts.append(cast(ast.expr, truncate_literal(e, config)))
        else:
            truncated_elts.append(ast.Constant(value=config.truncation_marker))
            break
    return truncated_elts


def truncate_literal(node: ast.AST, config: Config) -> ast.AST:
    """Truncate literal values to keep them small in the generated shadow file."""
    match node:
        case ast.Constant(value=str() as s):
            if _is_docstring(node):
                return node
            trunc = _truncate_string(
                s, config.max_string_length, config.truncation_marker
            )
            return ast.Constant(value=trunc)

        case ast.Constant(value=bytes() as b):
            trunc = _truncate_bytes(b, config.max_string_length)
            return ast.Constant(value=trunc)

        case ast.List(elts=elts):
            truncated = _truncate_sequence(elts, config.max_sequence_length, config)
            return ast.List(elts=truncated, ctx=ast.Load())

        case ast.Set(elts=elts):
            truncated = _truncate_sequence(elts, config.max_sequence_length, config)
            return ast.Set(elts=truncated)

        case ast.Tuple(elts=elts):
            truncated = _truncate_sequence(elts, config.max_sequence_length, config)
            return ast.Tuple(elts=truncated, ctx=ast.Load())

        case ast.Dict(keys=keys, values=values):
            truncated_pairs = []
            for i, (k, v) in enumerate(zip(keys, values, strict=False)):
                if i < config.max_sequence_length:
                    new_v = cast(ast.expr, truncate_literal(v, config))
                    truncated_pairs.append((k, new_v))
                else:
                    truncated_pairs.append(
                        (ast.Constant(value="..."), ast.Constant(value="..."))
                    )
                    break
            return ast.Dict(
                keys=[k for k, _ in truncated_pairs],
                values=[v for _, v in truncated_pairs],
            )

        case _:
            return node


class SignatureExtractor(ast.NodeTransformer):
    """
    Transform AST to preserve:
      - imports
      - docstrings (subject to length constraints)
      - function & method signatures
      - class-level assignments
    But replace function bodies with an ellipsis.
    """

    def __init__(self, config: Config, file_size: int = 0):
        super().__init__()
        self.config = config
        self.file_size = file_size

    def _preserve_docstring(self, body: list[ast.stmt]) -> list[ast.stmt]:
        """
        If the first statement is a docstring, keep it if it meets size constraints.
        Return that docstring statement (list of 1) or an empty list.
        """
        if not body:
            return []

        # Skip all docstrings if file is too large
        if self.file_size > self.config.max_file_size:
            return []

        match body[0]:
            case ast.Expr(value=ast.Constant(value=str() as docstring)):
                # Skip if docstring is too long
                if len(docstring) > self.config.max_docstring_length:
                    return []
                return [body[0]]
            case _:
                return []

    def _make_ellipsis_expr(self, node: ast.AST, indent: int = 0) -> ast.Expr:
        """Create an ellipsis node with the same location offsets."""
        return ast.Expr(
            value=ast.Constant(value=Ellipsis),
            lineno=getattr(node, "lineno", 1),
            col_offset=getattr(node, "col_offset", 0) + indent,
        )

    def visit_FunctionDef(self, node: ast.FunctionDef) -> ast.FunctionDef:
        """
        Keep:
         - function name
         - decorators
         - signature (args, returns)
         - docstring
        Replace body with '...'
        """
        preserved_doc = self._preserve_docstring(node.body)
        ellipsis_body = [self._make_ellipsis_expr(node, indent=4)]
        return ast.FunctionDef(
            name=node.name,
            args=node.args,
            body=preserved_doc + ellipsis_body,
            decorator_list=node.decorator_list,
            returns=node.returns,
            type_params=[],  # For Python 3.12+
            lineno=node.lineno,
            col_offset=node.col_offset,
        )

    def visit_ClassDef(self, node: ast.ClassDef) -> ast.ClassDef:
        """
        Preserve:
         - class name
         - decorators
         - docstring
         - definitions inside (methods, nested classes, assignments)
        But transform method bodies to ellipses via visit_FunctionDef.
        """
        preserved_doc = self._preserve_docstring(node.body)
        remainder = node.body[len(preserved_doc) :]
        new_body: list[ast.stmt] = []

        for item in remainder:
            match item:
                case ast.FunctionDef():
                    new_body.append(self.visit_FunctionDef(item))
                case _:
                    # Visit child nodes to truncate large literals
                    new_body.append(self.visit(item))

        return ast.ClassDef(
            name=node.name,
            bases=node.bases,
            keywords=node.keywords,
            body=preserved_doc + new_body,
            decorator_list=node.decorator_list,
            type_params=[],  # For Python 3.12+
            lineno=node.lineno,
            col_offset=node.col_offset,
        )

    def visit_Module(self, node: ast.Module) -> ast.Module:
        """Process top-level statements."""
        body: list[ast.stmt] = []
        for item in node.body:
            # Skip empty or purely whitespace expressions
            is_empty_expr = (
                isinstance(item, ast.Expr)
                and isinstance(item.value, ast.Constant)
                and (not item.value.value or item.value.value.isspace())
            )
            if is_empty_expr:
                continue
            body.append(self.visit(item))

        return ast.Module(body=body, type_ignores=[])

    def generic_visit(self, node: ast.AST) -> ast.AST:
        """
        Recurse into all child nodes, then apply literal truncation.
        This ensures we handle nested dictionaries, strings, bytes, etc.
        """
        new_node = super().generic_visit(node)
        return truncate_literal(new_node, self.config)


def generate_shadow_file(
    py_path: Path | str,
    in_dir: Path | str,
    out_dir: Path | str,
    config: Config | None = None,
) -> None:
    """Generate a shadow file with signatures, docstrings, and imports."""
    try:
        locations = FileLocations(
            py_path=Path(py_path),
            in_dir=Path(in_dir),
            out_dir=Path(out_dir),
        )
        locations.check_paths()

        config = config or Config()
        source = locations.py_path.read_text(encoding="utf-8")
        file_size = len(source)
        tree = ast.parse(source)

        # Add parent references for docstring detection
        def _attach_parents(curr: ast.AST, parent: ast.AST | None = None) -> None:
            for child in ast.iter_child_nodes(curr):
                child.parent = curr
                _attach_parents(child, curr)

        _attach_parents(tree)

        transformed_tree = SignatureExtractor(config, file_size).visit(tree)

        output_code = ast.unparse(transformed_tree)
        locations.output_path.write_text(output_code, encoding="utf-8")
        logger.debug(f"> {locations.output_path.relative_to(locations.out_dir)}")

    except Exception as exc:
        logger.error(f"Failed to process {py_path}: {exc!s}")
        raise


def process_directory(
    input_dir: Path | str,
    in_dir: Path | str,
    out_dir: Path | str,
    config: Config | None = None,
) -> None:
    """
    Recursively process all .py files in the given directory, generating shadow files.
    Builds a sorted structure of relative paths and processes them concurrently.
    Failed files are logged and skipped.
    """
    input_dir = Path(input_dir)
    in_dir = Path(in_dir)

    # Build sorted structure of relative paths
    files = sorted(input_dir.rglob("*.py"))
    [f.relative_to(in_dir) for f in files]
    total = len(files)

    if not total:
        logger.warning(f"No Python files found in {input_dir}")
        return

    logger.debug(f"Found {total} Python files to process")
    config = config or Config()

    # Process files concurrently
    successful_paths = []
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = {
            executor.submit(generate_shadow_file, f, in_dir, out_dir, config): f
            for f in files
        }

        for future in as_completed(futures):
            file = futures[future]
            try:
                future.result()
                successful_paths.append(file.relative_to(in_dir))
            except Exception as exc:
                logger.error(f"Failed to process {file}: {exc!s}")

    # Print tree structure of processed files
    if successful_paths:
        logger.debug("Successfully processed files:")
        print_file_tree(successful_paths)
    else:
        logger.error("No files were processed successfully")


def print_file_tree(paths: list[Path]) -> None:
    """Print a tree structure of the given file paths."""

    def add_to_tree(tree: dict, components: list[str]) -> None:
        """Add path components to tree structure."""
        if not components:
            return
        head, *tail = components
        if tail:
            tree[head] = tree.get(head, {})
            add_to_tree(tree[head], tail)
        else:
            tree[head] = None

    def print_tree(tree: dict, prefix: str = "", is_last: bool = True) -> None:
        """Print the tree structure recursively."""
        items = list(tree.items())
        for i, (_name, subtree) in enumerate(items):
            is_last_item = i == len(items) - 1
            if subtree is not None:
                extension = "    " if is_last_item else "│   "
                print_tree(subtree, prefix + extension, is_last_item)

    # Build tree structure
    tree: dict[str, Any] = {}
    for path in sorted(paths):
        components = str(path).split("/")
        add_to_tree(tree, components)

    # Print root and tree
    print_tree(tree)


def cli(
    in_dir: str | Path,
    out_dir: str | Path,
    py_path: str | Path | None = None,
    verbose: bool = False,
) -> None:
    """
    Generate shadow Python files (signatures, imports, docstrings),
    truncating large literals. Writes files to the corresponding
    directory under out_dir.
    """
    logger.remove()
    logger.add(
        console.out,
        format="<blue>{level}</blue> {message}",
        level="DEBUG" if verbose else "INFO",
    )

    try:
        config = Config()
        if py_path is not None:
            path = Path(py_path)
            if path.is_dir():
                process_directory(path, in_dir, out_dir, config)
            else:
                generate_shadow_file(path, in_dir, out_dir, config)
        else:
            process_directory(in_dir, in_dir, out_dir, config)

    except Exception as exc:
        logger.error(f"Application error: {exc!s}")
        raise


if __name__ == "__main__":
    try:
        import fire

        fire.Fire(cli)
    except ImportError:
        logger.error("Please install python-fire: pip install fire")

================
File: src/twat_coding/pystubnik/make_stubs_mypy.py
================
#!/usr/bin/env python3
"""Enhanced MyPy-based stub generator.

Combines MyPy's stubgen with smart stub generation to provide better type information.

This module provides a more sophisticated approach to stub generation by:
1. Using MyPy's stubgen for basic stub generation
2. Enhancing the stubs with smart docstring processing
3. Adding importance-based filtering
4. Proper handling of special cases (dataclasses, properties, etc.)
"""

import sys
from collections.abc import Sequence
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, NamedTuple, cast

from loguru import logger
from mypy import stubgen
from mypy.stubdoc import (
    ArgSig,
    FunctionSig,
    infer_sig_from_docstring,
    parse_all_signatures,
)

# Current Python version
pyversion_current = (sys.version_info.major, sys.version_info.minor)

# Optional fire import for CLI
try:
    import fire  # type: ignore
except ImportError:
    fire = None  # type: ignore


class DocSignature(NamedTuple):
    """Signature information extracted from docstring."""

    name: str
    args: list[tuple[str, str | None, str | None]]  # (name, type, default)
    ret_type: str | None


@dataclass
class StubGenConfig:
    """Configuration for stub generation with smart defaults."""

    pyversion: tuple[int, int] = pyversion_current
    no_import: bool = False
    inspect: bool = False
    doc_dir: str = ""
    search_path: Sequence[str | Path] = field(default_factory=list)
    interpreter: str | Path = sys.executable
    ignore_errors: bool = True
    parse_only: bool = False
    include_private: bool = False
    output_dir: str | Path = ""
    modules: Sequence[str] = field(default_factory=list)
    packages: Sequence[str] = field(default_factory=list)
    files: Sequence[str | Path] = field(default_factory=list)
    verbose: bool = False
    quiet: bool = True
    export_less: bool = False
    include_docstrings: bool = True  # Changed default to True
    importance_patterns: dict[str, float] = field(default_factory=dict)
    max_docstring_length: int = 500
    include_type_comments: bool = True
    infer_property_types: bool = True

    def to_stubgen_options(self) -> stubgen.Options:
        """Convert to MyPy's stubgen.Options."""
        return stubgen.Options(
            pyversion=self.pyversion,
            no_import=self.no_import,
            inspect=self.inspect,
            doc_dir=self.doc_dir,
            search_path=[str(p) for p in self.search_path],
            interpreter=str(self.interpreter),
            ignore_errors=self.ignore_errors,
            parse_only=self.parse_only,
            include_private=self.include_private,
            output_dir=str(self.output_dir),
            modules=list(self.modules),
            packages=list(self.packages),
            files=[str(f) for f in self.files],
            verbose=self.verbose,
            quiet=self.quiet,
            export_less=self.export_less,
            include_docstrings=self.include_docstrings,
        )


class SmartStubGenerator:
    """Enhanced stub generator that combines MyPy's stubgen with smart features."""

    def __init__(self, config: StubGenConfig):
        self.config = config
        self.logger = logger.opt(colors=True)

    def _convert_function_sig(self, sig: FunctionSig | tuple[str, str]) -> DocSignature:
        """Convert FunctionSig to DocSignature.

        Args:
            sig: Function signature from mypy.stubdoc

        Returns:
            Converted signature
        """
        if not isinstance(sig, FunctionSig):
            # Handle tuple format from parse_all_signatures
            name, ret_type = sig
            return DocSignature(name=name, args=[], ret_type=ret_type)

        # Handle FunctionSig format from infer_sig_from_docstring
        args: list[tuple[str, str | None, str | None]] = []
        for arg in sig.args:
            if isinstance(arg, ArgSig):
                # ArgSig attributes are dynamically added by mypy
                arg_dict = cast(dict[str, Any], arg.__dict__)
                type_str = arg_dict.get("type_str")
                type_str = None if type_str == "Any" else type_str
                default = arg_dict.get("default")
                default = None if default == "..." else default
                args.append(
                    (
                        arg_dict.get("name", ""),
                        type_str,
                        default,
                    )
                )
            elif isinstance(arg, tuple):
                name, type_str, default = cast(tuple[str, str, str], arg)
                args.append(
                    (
                        name,
                        type_str if type_str != "Any" else None,
                        default if default != "..." else None,
                    )
                )

        return DocSignature(
            name=sig.name,
            args=args,
            ret_type=sig.ret_type if sig.ret_type != "Any" else None,
        )

    def process_docstring(
        self, docstring: str | None
    ) -> tuple[str | None, list[DocSignature]]:
        """Process docstring to extract type information and signatures.

        Args:
            docstring: The docstring to process

        Returns:
            Tuple of (processed docstring, list of extracted signatures)
        """
        if not docstring:
            return None, []

        # Truncate if too long
        if len(docstring) > self.config.max_docstring_length:
            docstring = docstring[: self.config.max_docstring_length] + "..."

        # Try to infer signatures
        sigs: list[DocSignature] = []
        try:
            inferred_sigs = infer_sig_from_docstring(docstring, "") or []
            sigs.extend(self._convert_function_sig(sig) for sig in inferred_sigs)
        except Exception as e:
            self.logger.debug(f"Failed to infer signatures from docstring: {e}")

        # If doc_dir is specified, try to find additional signatures
        if self.config.doc_dir:
            try:
                doc_sigs, _ = parse_all_signatures([docstring])
                sigs.extend(self._convert_function_sig(sig) for sig in doc_sigs)
            except Exception as e:
                self.logger.debug(f"Failed to parse doc signatures: {e}")

        return docstring, sigs

    def calculate_importance(self, name: str, docstring: str | None = None) -> float:
        """Calculate importance score for a symbol."""
        score = 1.0

        # Check against importance patterns
        for pattern, weight in self.config.importance_patterns.items():
            if pattern in name:
                score *= weight

        # Docstring-based importance
        if docstring:
            # More detailed docstrings might indicate importance
            score *= 1 + (len(docstring.split()) / 100)

            # Check for specific keywords indicating importance
            importance_keywords = {"important", "critical", "essential", "main", "key"}
            if any(keyword in docstring.lower() for keyword in importance_keywords):
                score *= 1.5

        # Name-based importance
        if name.startswith("__") and name.endswith("__"):
            score *= 1.2  # Special methods are usually important
        elif not name.startswith("_"):
            score *= 1.1  # Public methods are usually more important

        return score

    def enhance_stub(self, stub_content: str, module_name: str) -> str:
        """Enhance a generated stub with additional information."""
        # TODO: Implement stub enhancement logic
        return stub_content

    def generate_stubs(self) -> None:
        """Generate enhanced stubs using MyPy's stubgen with additional processing."""
        try:
            # First, use MyPy's stubgen
            options = self.config.to_stubgen_options()
            stubgen.generate_stubs(options)

            # Then enhance the stubs (TODO)
            self.logger.info("<green>Successfully generated stubs</green>")

        except Exception as e:
            self.logger.error(f"<red>Failed to generate stubs: {e}</red>")
            if self.config.verbose:
                import traceback

                self.logger.debug(traceback.format_exc())
            if not self.config.ignore_errors:
                raise


def generate_stubs(
    pyversion: tuple[int, int] = pyversion_current,
    no_import: bool = False,
    inspect: bool = False,
    doc_dir: str = "",
    search_path: Sequence[str | Path] = [],
    interpreter: str | Path = sys.executable,
    ignore_errors: bool = True,
    parse_only: bool = False,
    include_private: bool = False,
    output_dir: str | Path = "",
    modules: Sequence[str] = [],
    packages: Sequence[str] = [],
    files: Sequence[str | Path] = [],
    verbose: bool = False,
    quiet: bool = True,
    export_less: bool = False,
    include_docstrings: bool = True,
    importance_patterns: dict[str, float] | None = None,
    max_docstring_length: int = 500,
    include_type_comments: bool = True,
    infer_property_types: bool = True,
) -> None:
    """Generate smart stubs for modules specified in options.

    This is an enhanced version of MyPy's stub generation that adds:
    - Importance-based filtering
    - Smart docstring processing
    - Better type inference
    - Special case handling

    Args:
        pyversion: Python version as (major, minor) tuple
        no_import: Don't import modules, just parse and analyze
        inspect: Import and inspect modules instead of parsing source
        doc_dir: Path to .rst documentation directory
        search_path: List of module search directories
        interpreter: Path to Python interpreter
        ignore_errors: Ignore errors during stub generation
        parse_only: Don't do semantic analysis of source
        include_private: Include private objects in stubs
        output_dir: Directory to write stub files to
        modules: List of module names to generate stubs for
        packages: List of package names to generate stubs for recursively
        files: List of files/directories to generate stubs for
        verbose: Show more detailed output
        quiet: Show minimal output
        export_less: Don't export imported names
        include_docstrings: Include docstrings in stubs
        importance_patterns: Dict of regex patterns to importance scores
        max_docstring_length: Maximum length for included docstrings
        include_type_comments: Include type comments in stubs
        infer_property_types: Try to infer property types from docstrings

    The function will generate .pyi stub files in the specified output_dir.
    """
    config = StubGenConfig(
        pyversion=pyversion,
        no_import=no_import,
        inspect=inspect,
        doc_dir=doc_dir,
        search_path=search_path,
        interpreter=interpreter,
        ignore_errors=ignore_errors,
        parse_only=parse_only,
        include_private=include_private,
        output_dir=output_dir,
        modules=modules,
        packages=packages,
        files=files,
        verbose=verbose,
        quiet=quiet,
        export_less=export_less,
        include_docstrings=include_docstrings,
        importance_patterns=importance_patterns or {},
        max_docstring_length=max_docstring_length,
        include_type_comments=include_type_comments,
        infer_property_types=infer_property_types,
    )

    generator = SmartStubGenerator(config)
    generator.generate_stubs()


if __name__ == "__main__":
    if fire is None:
        print("Error: python-fire package is required to run this script directly")
        sys.exit(1)
    fire.Fire(generate_stubs)

================
File: src/twat_coding/pystubnik/pypacky2.sh
================
#!/usr/bin/env bash
dir=${0%/*}
if [ "$dir" = "$0" ]; then dir="."; fi
dir=$(grealpath "$dir")
cd "$dir"
workdir=$(grealpath "$dir/work")
cd "$workdir"

PROJ="sluglet"

uv venv --allow-existing --relocatable --python-preference only-system "$dir/venv" # && source work/myvenv/bin/activate
uv venv --allow-existing --relocatable --python-preference only-system "$dir/devenv"

source "$dir/devenv/bin/activate"

uv pip install -U -r "$workdir/dev_requirements.txt"

projdir=$(grealpath "$workdir/$PROJ")

UPROJ=$(echo "$PROJ" | tr '-' '_')
putup --name $UPROJ --package $UPROJ --url https://github.com/twardoch/$PROJ --license Apache-2.0 --description "Python tool to composite two images using multiple mask images" --github-actions $PROJ && cd "$projdir" && pre-commit autoupdate

cd "$projdir"
pre-commit autoupdate
rm "src/$PROJ/skeleton.py"
cp "$workdir/$PROJ.py" "$projdir/src/$PROJ/"

cd "$workdir"
pyodide minify "$PROJ"
MINIFIED_DIR="${PROJ}_stripped"
OUT_DIR="$workdir/_vulture"
mkdir -p "$OUT_DIR"
vulture "$MINIFIED_DIR" --sort-by-size --make-whitelist >"$OUT_DIR/vulture_dead_code.txt"
uv pip install -U "$projdir"

compile_with_nuitka() {
    echo "Compiling with Nuitka..."
    OUT_DIR="$workdir/_nuit"
    mkdir -p "$OUT_DIR"
    MAIN_FILE="$workdir/main.py"
    if [ -n "$MAIN_FILE" ]; then
        nuitka --standalone "$MAIN_FILE" --include-package="$PROJ" --show-modules --show-scons --output-dir="$OUT_DIR" --follow-imports --prefer-source-code
    else
        echo "Main file not found. Skipping Nuitka compilation."
    fi
}
compile_with_nuitka

package_with_pyinstaller() {
    echo "Packaging with PyInstaller..."
    OUT_DIR="$workdir/_pyins"
    mkdir -p "$OUT_DIR"
    MAIN_FILE="$workdir/main.py"
    if [ -n "$MAIN_FILE" ]; then
        pyinstaller --console --onedir -y --workpath "$OUT_DIR/work" --distpath "$OUT_DIR/dist" --specpath "$OUT_DIR/spec" "$MAIN_FILE" -d all --clean --contents-directory . --optimize 0
    else
        echo "Main file not found. Skipping PyInstaller packaging."
    fi
}
package_with_pyinstaller

================
File: src/twat_coding/pystubnik/read_imports.py
================
#!/usr/bin/env python3
"""Extract imports from Python files."""

import ast
from pathlib import Path

import fire


def ast_to_source(node: ast.Import | ast.ImportFrom) -> str:
    """Convert an AST import node to source code.

    Args:
        node: The AST import node

    Returns:
        The source code representation of the import
    """
    if isinstance(node, ast.Import):
        names = [alias.name for alias in node.names]
        return f"import {', '.join(names)}"
    elif isinstance(node, ast.ImportFrom):
        names = [alias.name for alias in node.names]
        module = node.module or ""
        level = "." * node.level
        return f"from {level}{module} import {', '.join(names)}"
    else:
        msg = f"Unexpected node type: {type(node)}"
        raise ValueError(msg)


def extract_imports(source: str) -> list[ast.Import | ast.ImportFrom]:
    """Extract import statements from Python source code.

    Args:
        source: Python source code

    Returns:
        List of import nodes
    """
    tree = ast.parse(source)
    imports: list[ast.Import | ast.ImportFrom] = []

    for node in ast.walk(tree):
        if isinstance(node, ast.Import | ast.ImportFrom):
            imports.append(node)

    return imports


def process_py_file(file_path: str | Path) -> None:
    """Process a Python file to extract imports.

    Args:
        file_path: Path to the Python file
    """
    file_path = Path(file_path)
    source = file_path.read_text()
    imports = extract_imports(source)

    for _node in imports:
        pass


def main() -> None:
    """Main entry point."""
    fire.Fire(process_py_file)


if __name__ == "__main__":
    main()

================
File: src/twat_coding/pystubnik/README.md
================
# pystubnik

A Python package for generating "smart stubs" - an intelligent intermediate representation between full source code and type stubs, optimized for LLM code understanding.

## 1. Overview

Pystubnik creates a "shadow" directory structure that mirrors your Python package, containing smart stubs for all Python files. These smart stubs are designed to be more informative than traditional `.pyi` stub files while being more concise than full source code.

### 1.1. What are Smart Stubs?

Smart stubs are an intermediate representation that includes:
- All function and class signatures with type hints
- All imports (organized and optimized)
- Docstrings (with configurable length limits)
- Important/relevant code sections
- Truncated versions of large data structures and strings
- Simplified function bodies for non-critical code

The verbosity level is automatically adjusted based on the code's importance and complexity.

## 2. Architecture

### 2.1. Backends

#### 2.1.1. AST Backend
- Uses Python's built-in AST module for precise control
- Preserves code structure while reducing verbosity
- Configurable truncation of large literals and sequences
- Maintains type information and docstrings
- Supports Python 3.12+ features (type parameters, etc.)

#### 2.1.2. MyPy Backend
- Leverages mypy's stubgen for type information
- Better type inference capabilities
- Handles special cases (dataclasses, properties)
- Supports type comment extraction

### 2.2. Processors

#### 2.2.1. Import Processor
- Analyzes and organizes imports
- Groups by type (stdlib, third-party, local)
- Handles relative imports
- Detects and removes duplicates

#### 2.2.2. Docstring Processor
- Configurable docstring preservation
- Format detection and conversion
- Type information extraction
- Length-based truncation

#### 2.2.3. Importance Processor
- Scores code elements by importance
- Pattern-based importance detection
- Inheritance-aware scoring
- Configurable filtering

## 3. Usage

```python
from twat_coding.pystubnik import generate_stub, StubGenConfig, PathConfig

# Basic usage
result = generate_stub(
    source_path="path/to/file.py",
    output_path="path/to/output/file.py"
)

# Advanced configuration
config = StubGenConfig(
    paths=PathConfig(
        output_dir="path/to/output",
        doc_dir="path/to/docs",
        search_paths=[],
        modules=[],
        packages=[],
        files=[],
    ),
    processing=ProcessingConfig(
        include_docstrings=True,
        include_private=False,
        include_type_comments=True,
        infer_property_types=True,
        export_less=False,
        importance_patterns={
            r"^test_": 0.5,  # Lower importance for test functions
            r"^main$": 1.0,  # High importance for main functions
        },
    ),
    truncation=TruncationConfig(
        max_sequence_length=4,
        max_string_length=17,
        max_docstring_length=150,
        max_file_size=3_000,
        truncation_marker="...",
    ),
)

result = generate_stub(
    source_path="path/to/file.py",
    output_path="path/to/output/file.py",
    config=config
)

# Process multiple files
from twat_coding.pystubnik import SmartStubGenerator

generator = SmartStubGenerator(
    output_dir="path/to/output",
    include_docstrings=True,
    include_private=False,
    verbose=True,
)

generator.generate()  # Process all configured files
```

## 4. Configuration

Smart stub generation can be customized with various settings:

### 4.1. Path Configuration
- `output_dir`: Directory for generated stubs
- `doc_dir`: Directory containing documentation
- `search_paths`: Module search paths
- `modules`: Module names to process
- `packages`: Package names to process
- `files`: Specific files to process

### 4.2. Processing Configuration
- `include_docstrings`: Whether to include docstrings
- `include_private`: Whether to include private symbols
- `include_type_comments`: Whether to include type comments
- `infer_property_types`: Whether to infer property types
- `export_less`: Whether to minimize exports
- `importance_patterns`: Regex patterns for importance scoring

### 4.3. Truncation Configuration
- `max_sequence_length`: Maximum items in sequences
- `max_string_length`: Maximum length for strings
- `max_docstring_length`: Maximum length for docstrings
- `max_file_size`: File size threshold for truncation
- `truncation_marker`: Marker for truncated content

## 5. Installation

```bash
# Install with all dependencies
pip install twat-coding[pystubnik]

# Install minimal version
pip install twat-coding
```

## 6. Use Cases

1. **LLM Code Understanding**
   - Prepare large codebases for LLM analysis
   - Reduce token usage while maintaining critical information
   - Improve LLM's understanding of code structure

2. **Code Documentation**
   - Generate readable, concise code documentation
   - Maintain type information and signatures
   - Preserve important implementation details

3. **Code Analysis**
   - Quick understanding of large codebases
   - Dependency analysis
   - API surface exploration

4. **Type Stub Generation**
   - Generate enhanced type stubs
   - Better support for documentation tools
   - IDE integration

## 7. Contributing

Contributions are welcome! Please see our contributing guidelines for more information.

## 8. License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: src/twat_coding/__init__.py
================
"""twat-coding package."""

from .__version__ import __version__, version, version_tuple

__all__ = ["__version__", "version", "version_tuple"]

================
File: src/twat_coding/twat_coding.py
================
#!/usr/bin/env python3
"""twat_coding:

Created by Adam Twardoch
"""

from __future__ import annotations

import logging
from dataclasses import dataclass
from typing import Any

__version__ = "0.1.0"

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


@dataclass
class Config:
    """Configuration settings for twat_coding."""

    name: str
    value: str | int | float
    options: dict[str, Any] | None = None


def process_data(
    data: list[Any], config: Config | None = None, *, debug: bool = False
) -> dict[str, Any]:
    """Process the input data according to configuration.

    Args:
        data: Input data to process
        config: Optional configuration settings
        debug: Enable debug mode

    Returns:
        Processed data as a dictionary

    Raises:
        ValueError: If input data is invalid
    """
    if debug:
        logger.setLevel(logging.DEBUG)
        logger.debug("Debug mode enabled")

    if not data:
        msg = "Input data cannot be empty"
        raise ValueError(msg)

    # TODO: Implement data processing logic
    result: dict[str, Any] = {}
    return result


def main() -> None:
    """Main entry point for twat_coding."""
    try:
        # Example usage
        config = Config(name="default", value="test", options={"key": "value"})
        result = process_data([], config=config)
        logger.info("Processing completed: %s", result)

    except Exception as e:
        logger.exception("An error occurred: %s", str(e))
        raise


if __name__ == "__main__":
    main()

================
File: tests/test_package.py
================
"""Test suite for twat_coding."""


def test_version() -> None:
    """Verify package exposes version."""
    import twat_coding

    assert twat_coding.__version__

================
File: .gitignore
================
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private

================
File: .pre-commit-config.yaml
================
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]

================
File: cleanup.py
================
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [
#   "ruff>=0.9.6",
#   "pytest>=8.3.4",
#   "mypy>=1.15.0",
# ]
# ///
# this_file: cleanup.py

"""
Cleanup tool for managing repository tasks and maintaining code quality.

This script provides a comprehensive set of commands for repository maintenance:

When to use each command:

- `cleanup.py status`: Use this FIRST when starting work to check the current state
  of the repository. It shows file structure, git status, and runs all code quality
  checks. Run this before making any changes to ensure you're starting from a clean state.

- `cleanup.py venv`: Run this when setting up the project for the first time or if
  your virtual environment is corrupted/missing. Creates a new virtual environment
  using uv.

- `cleanup.py install`: Use after `venv` or when dependencies have changed. Installs
  the package and all development dependencies in editable mode.

- `cleanup.py update`: Run this when you've made changes and want to commit them.
  It will:
  1. Show current status (like `status` command)
  2. Stage and commit any changes with a generic message
  Use this for routine maintenance commits.

- `cleanup.py push`: Run this after `update` when you want to push your committed
  changes to the remote repository.

Workflow Example:
1. Start work: `cleanup.py status`
2. Make changes to code
3. Commit changes: `cleanup.py update`
4. Push to remote: `cleanup.py push`

The script maintains a CLEANUP.log file that records all operations with timestamps.
It also includes content from README.md at the start and TODO.md at the end of logs
for context.

Required Files:
- LOG.md: Project changelog
- README.md: Project documentation
- TODO.md: Pending tasks and future plans
"""

import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path
from typing import NoReturn

# Configuration
IGNORE_PATTERNS = [
    ".git",
    ".venv",
    "__pycache__",
    "*.pyc",
    "dist",
    "build",
    "*.egg-info",
]
REQUIRED_FILES = ["LOG.md", ".cursor/rules/0project.mdc", "TODO.md"]
LOG_FILE = Path("CLEANUP.log")

# Ensure we're working from the script's directory
os.chdir(Path(__file__).parent)


def new() -> None:
    """Remove existing log file."""
    if LOG_FILE.exists():
        LOG_FILE.unlink()


def prefix() -> None:
    """Write README.md content to log file."""
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)


def suffix() -> None:
    """Write TODO.md content to log file."""
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
        log_message(content)


def log_message(message: str) -> None:
    """Log a message to file and console with timestamp."""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"{timestamp} - {message}\n"
    with LOG_FILE.open("a") as f:
        f.write(log_line)


def print_log() -> None:
    """Print the contents of the CLEANUP.log file."""
    if LOG_FILE.exists():
        print(LOG_FILE.read_text())
    else:
        print("Log file does not exist")


def run_command(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
    """Run a shell command and return the result."""
    try:
        result = subprocess.run(cmd, check=check, capture_output=True, text=True)
        if result.stdout:
            log_message(result.stdout)
        return result
    except subprocess.CalledProcessError as e:
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        if check:
            raise
        return subprocess.CompletedProcess(cmd, 1, "", str(e))


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in the system."""
    try:
        subprocess.run(["which", cmd], check=True, capture_output=True)
        return True
    except subprocess.CalledProcessError:
        return False


class Cleanup:
    """Main cleanup tool class."""

    def __init__(self) -> None:
        self.workspace = Path.cwd()

    def _print_header(self, message: str) -> None:
        """Print a section header."""
        log_message(f"\n=== {message} ===")

    def _check_required_files(self) -> bool:
        """Check if all required files exist."""
        missing = False
        for file in REQUIRED_FILES:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
                missing = True
        return not missing

    def _generate_tree(self) -> None:
        """Generate and display tree structure of the project."""
        if not check_command_exists("tree"):
            log_message("Warning: 'tree' command not found. Skipping tree generation.")
            return None

        try:
            # Create/overwrite the file with YAML frontmatter
            rules_dir = Path(".cursor/rules")
            rules_dir.mkdir(parents=True, exist_ok=True)
            # Get tree output
            tree_result = run_command(
                ["tree", "-a", "-I", ".git", "--gitignore", "-n", "-h", "-I", "*_cache"]
            )
            tree_text = tree_result.stdout
            # Write frontmatter and tree output to file
            with open(rules_dir / "filetree.mdc", "w") as f:
                f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
                f.write(tree_text)

            # Log the contents
            log_message("\nProject structure:")
            log_message(tree_text)

        except Exception as e:
            log_message(f"Failed to generate tree: {e}")
        return None

    def _git_status(self) -> bool:
        """Check git status and return True if there are changes."""
        result = run_command(["git", "status", "--porcelain"], check=False)
        return bool(result.stdout.strip())

    def _venv(self) -> None:
        """Create and activate virtual environment using uv."""
        log_message("Setting up virtual environment")
        try:
            run_command(["uv", "venv"])
            # Activate the virtual environment
            venv_path = self.workspace / ".venv" / "bin" / "activate"
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                os.environ["PATH"] = (
                    f"{self.workspace / '.venv' / 'bin'}{os.pathsep}{os.environ['PATH']}"
                )
                log_message("Virtual environment created and activated")
            else:
                log_message("Virtual environment created but activation failed")
        except Exception as e:
            log_message(f"Failed to create virtual environment: {e}")

    def _install(self) -> None:
        """Install package in development mode with all extras."""
        log_message("Installing package with all extras")
        try:
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
        except Exception as e:
            log_message(f"Failed to install package: {e}")

    def _run_checks(self) -> None:
        """Run code quality checks using ruff and pytest."""
        log_message("Running code quality checks")

        try:
            # Run ruff checks
            log_message(">>> Running code fixes...")
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "check",
                    "--fix",
                    "--unsafe-fixes",
                    "src",
                    "tests",
                ],
                check=False,
            )
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "format",
                    "--respect-gitignore",
                    "src",
                    "tests",
                ],
                check=False,
            )

            # Run type checks
            log_message(">>>Running type checks...")
            run_command(["python", "-m", "mypy", "src", "tests"], check=False)

            # Run tests
            log_message(">>> Running tests...")
            run_command(["python", "-m", "pytest", "tests"], check=False)

            log_message("All checks completed")
        except Exception as e:
            log_message(f"Failed during checks: {e}")

    def status(self) -> None:
        """Show current repository status: tree structure, git status, and run checks."""
        prefix()  # Add README.md content at start
        self._print_header("Current Status")

        # Check required files
        self._check_required_files()

        # Show tree structure
        self._generate_tree()

        # Show git status
        result = run_command(["git", "status"], check=False)
        log_message(result.stdout)

        # Run additional checks
        self._print_header("Environment Status")
        self._venv()
        self._install()
        self._run_checks()

        suffix()  # Add TODO.md content at end

    def venv(self) -> None:
        """Create and activate virtual environment."""
        self._print_header("Virtual Environment Setup")
        self._venv()

    def install(self) -> None:
        """Install package with all extras."""
        self._print_header("Package Installation")
        self._install()

    def update(self) -> None:
        """Show status and commit any changes if needed."""
        # First show current status
        self.status()

        # Then handle git changes if any
        if self._git_status():
            log_message("Changes detected in repository")
            try:
                # Add all changes
                run_command(["git", "add", "."])
                # Commit changes
                commit_msg = "Update repository files"
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
            except Exception as e:
                log_message(f"Failed to commit changes: {e}")
        else:
            log_message("No changes to commit")

    def push(self) -> None:
        """Push changes to remote repository."""
        self._print_header("Pushing Changes")
        try:
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
        except Exception as e:
            log_message(f"Failed to push changes: {e}")


def print_usage() -> None:
    """Print usage information."""
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")


def main() -> NoReturn:
    """Main entry point."""
    new()  # Clear log file

    if len(sys.argv) < 2:
        print_usage()
        sys.exit(1)

    command = sys.argv[1]
    cleanup = Cleanup()

    try:
        if command == "status":
            cleanup.status()
        elif command == "venv":
            cleanup.venv()
        elif command == "install":
            cleanup.install()
        elif command == "update":
            cleanup.update()
        elif command == "push":
            cleanup.push()
        else:
            print_usage()
            sys.exit(1)
    except Exception as e:
        log_message(f"Error: {e}")
        sys.exit(1)

    print_log()  # Print the log contents at the end
    sys.exit(0)


if __name__ == "__main__":
    main()

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: MANIFEST.in
================
# This MANIFEST.in file ensures that all necessary files are included in the package distribution.
recursive-include src/twat_coding/data *
include src/twat_coding/py.typed

================
File: mypy.ini
================
[mypy]
python_version = 3.12
ignore_missing_imports = True
disallow_untyped_defs = True
warn_return_any = True
warn_unused_configs = True
check_untyped_defs = True
disallow_incomplete_defs = True
disallow_untyped_decorators = True
no_implicit_optional = True

[mypy-tests.*]
disallow_untyped_defs = False

================
File: package.toml
================
# Package configuration
[package]
include_cli = true        # Include CLI boilerplate
include_logging = true    # Include logging setup
use_pydantic = true      # Use Pydantic for data validation
use_rich = true          # Use Rich for terminal output

[features]
mkdocs = false           # Enable MkDocs documentation
vcs = true              # Initialize Git repository
github_actions = true   # Add GitHub Actions workflows

================
File: pyproject.toml
================
# this_file: pyproject.toml
# this_project: twat-coding

[project]
name = "twat_coding"
dynamic = ["version"]
description = "A Python package for generating type stubs"
authors = [
    { name = "Adam Twardoch", email = "adam@twardoch.com" }
]
dependencies = [
    "pydantic>=2.6.1",
    "pydantic-settings>=2.1.0",
    "loguru>=0.7.2",
    "rich>=13.7.0",
    "click>=8.1.7",
    "psutil>=5.9.8",
    "memory-profiler>=0.61.0",
    "docstring-parser>=0.15",
    "mypy>=1.8.0",
    "black>=24.1.1",
    "isort>=5.13.2",
    # File importance analysis dependencies
    "networkx>=3.2.1",     # For import graph analysis
    "radon>=6.0.1",       # For code complexity metrics
    "coverage>=7.4.1",    # For test coverage analysis
    "pydocstyle>=6.3.0", # For docstring quality checks
    "importlab>=0.8",    # For import graph building
    "toml>=0.10.2",      # For pyproject.toml parsing
    "types-toml>=0.10.8.7",    # Type stubs for toml
    "mypy-extensions>=1.0.0",  # Additional type system features
]
requires-python = ">=3.12"
readme = "README.md"
license = { text = "Apache-2.0" }
keywords = [
    "twat",
    "coding",
    "development"
]
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]

[project.optional-dependencies]
test = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
    "pytest-asyncio>=0.23.5",
    "pytest-benchmark>=4.0.0",
    "pytest-golden>=0.2.2",
    "pytest-memray>=1.5.0",
]
dev = [
    "black>=24.1.1",
    "ruff>=0.2.1",
    "mypy>=1.8.0",
    "pre-commit>=3.6.0",
    "python-semantic-release>=8.7.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
target-version = "py312"
line-length = 88
fix = true
unsafe-fixes = true
output-format = "github"

[tool.ruff.lint]
select = [
    "E",   # pycodestyle errors
    "W",   # pycodestyle warnings
    "F",   # pyflakes
    "I",   # isort
    "C",   # flake8-comprehensions
    "B",   # flake8-bugbear
    "UP",  # pyupgrade
]
ignore = []

[tool.ruff.lint.isort]
known-first-party = ["twat_coding"]
combine-as-imports = true

[tool.black]
line-length = 88
target-version = ["py312"]
include = '\.pyi?$'

[tool.mypy]
python_version = "3.12"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.pytest.ini_options]
minversion = "8.0"
addopts = "-ra -q --cov=twat_coding"
testpaths = ["tests"]

[project.scripts]
twat-coding = "twat_coding.__main__:main"

[project.urls]
Homepage = "https://github.com/twardoch/twat-coding"
Documentation = "https://github.com/twardoch/twat-coding#readme"
Issues = "https://github.com/twardoch/twat-coding/issues"
Source = "https://github.com/twardoch/twat-coding"

[tool.coverage.paths]
twat_coding = ["src/twat_coding", "*/twat-coding/src/twat_coding"]
tests = ["tests", "*/twat-coding/tests"]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "raise NotImplementedError",
    "if __name__ == .__main__.:",
    "if TYPE_CHECKING:",
]
ignore_errors = true
omit = [
    "tests/*",
    "setup.py",
    "src/twat_coding/__about__.py",
]

[tool.hatch.build.targets.wheel]
packages = ["src/twat_coding"]
artifacts = [
    "src/twat_coding/py.typed"
]

[tool.hatch.envs.default]
dependencies = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
]

[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov {args:tests}"
cov-report = ["- coverage combine", "coverage report"]
cov = ["test-cov", "cov-report"]
lint = ["ruff check src/twat_coding tests", "ruff format --respect-gitignore src/twat_coding tests"]
fix = ["ruff check  --fix --unsafe-fixes src/twat_coding tests", "ruff format --respect-gitignore src/twat_coding tests"]

[tool.hatch.envs.lint]
dependencies = [
    "black>=24.1.1",
    "mypy>=1.8.0",
    "ruff>=0.2.1",
]

[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_coding tests}"
style = ["ruff check {args:.}", "ruff format --check {args:.}"]
fmt = ["ruff format --respect-gitignore {args:.}", "ruff check --fix {args:.}"]
all = ["style", "typing"]

[tool.hatch.envs.test]
dependencies = [
    "pytest>=8.0.0",
    "pytest-cov>=4.1.0",
]

[tool.hatch.version]
path = "src/twat_coding/__version__.py"

[tool.pytest-benchmark]
warmup = true

[project.entry-points."twat.plugins"]
coding = "twat_coding"

================
File: README.md
================
# twat-coding

A collection of coding-related tools and utilities, designed to enhance code understanding and analysis.

## Packages

### pystubnik

A sophisticated tool for generating "smart stubs" - an intelligent intermediate representation between full source code and type stubs, optimized for LLM code understanding. Smart stubs preserve essential code structure, types, and documentation while reducing verbosity, making large codebases more digestible for LLMs.

Key features:
- Multiple backends (AST and MyPy) for comprehensive stub generation
- Smart import analysis and organization
- Configurable docstring processing and formatting
- Importance-based code filtering
- Support for Python 3.12+ features

[Learn more about pystubnik](src/twat_coding/pystubnik/README.md)

## Installation

```bash
# Install the base package
pip install twat-coding

# Install with pystubnik support
pip install twat-coding[pystubnik]
```

## Development

This project uses modern Python features and follows best practices:
- Type hints and dataclasses for robust data structures
- Protocol-based interfaces for flexibility
- Comprehensive error handling and logging
- Parallel processing for performance

## Contributing

Contributions are welcome! Please see our contributing guidelines for more information.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: TODO.md
================
Before you do any changes or if I say "cleanup", run the `./cleanup.py install && source .venv/bin/activate && ./cleanup.py update | cat` script in the main folder. Analyze the results, describe recent changes @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `./cleanup.py update | cat` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done, and `- [!]` if they're NEXT TODO. 

Don't use `pip`, use `uv pip`. Ignore minor problems like "line too long", FOCUS ON MAJOR PROBLEMS! 

# TODO

This TODO.md is a detailed development specification for making `pystubnik` functional ASAP, focusing on integrating the new file importance analysis and eliminating standalone scripts.

## 1. Dependencies and Environment (URGENT)

- [x] **Add New Dependencies to `pyproject.toml`:**
  ```toml
  [project]
  dependencies = [
      # ... existing deps ...
      "networkx>=3.2.1",     # For import graph analysis
      "radon>=6.0.1",       # For code complexity metrics
      "coverage>=7.4.1",    # For test coverage analysis
      "pydocstyle>=6.3.0", # For docstring quality checks
      "importlab>=0.8",    # For import graph building
      "toml>=0.10.2",      # For pyproject.toml parsing
  ]
  ```

- [x] **Fix Import Resolution:**
  - Added missing type stubs: `types-toml`, replaced `types-pydocstyle` with `mypy-extensions`
  - Simplified import graph building to not rely on importlab
  - Added proper error handling for optional dependencies

## 2. Code Quality Fixes

- [x] **Fix Critical Linter Errors:**
  - Fixed `Numbers.percentage` attribute error in `file_importance.py`
  - Added proper type hints for importlab classes
  - Fixed line length issues in `file_importance.py`
  - Added error handling for missing dependencies
  - Fixed type errors in centrality calculation

- [x] **Fix Remaining Linter Errors:**
  - Removed unused imports in `backends/__init__.py` and `processors/__init__.py`
  - Replaced deprecated type hints with modern syntax:
    - Replaced `typing.Dict` with `dict`
    - Replaced `typing.Type` with `type`
    - Used `X | Y` instead of `Union[X, Y]`
  - Fixed line length issues in:
    - `config.py`
    - `core/utils.py`
    - `types/docstring.py`
  - Refactored complex functions:
    - `truncate_literal` in `make_stubs_ast.py`
    - `calculate_importance` in `importance.py`
    - `extract_types` in `types/docstring.py`

- [!] **Fix Type Checking Errors:**
  The following type errors need to be fixed:
  1. In `type_system.py`:
     ```python
     # Fix incompatible type in issubclass call by:
     # 1. Import the correct type from typing._TypeAliasForm
     # 2. Use isinstance() instead of issubclass() for type checking
     # 3. Add proper type guards
     ```
  2. In `make_stubs_ast.py`:
     ```python
     # Fix bytes/str type mismatch by:
     # 1. Add explicit encoding when converting between bytes and str
     # 2. Use proper type annotations for binary data
     # 3. Add error handling for encoding/decoding failures
     ```
  3. In `make_stubs_ast.py`:
     ```python
     # Fix AST parent attribute error by:
     # 1. Create a custom AST node class that includes parent attribute
     # 2. Use ast.fix_missing_locations after modifying AST
     # 3. Add proper type hints for the parent attribute
     ```
  4. In `docstring.py`:
     ```python
     # Fix type annotation by:
     # 1. Update type hints to use modern Python syntax
     # 2. Add proper type guards for optional attributes
     # 3. Use TypeVar for generic type parameters
     ```

- [!] **Improve Code Organization:**
  Create a new `utils/` directory with the following structure and functionality:
  ```
  utils/
  ├── graph_utils.py       # Import graph building and analysis
  │   ├── build_import_graph()  # Build graph from Python files
  │   ├── analyze_centrality()  # Calculate node importance
  │   └── resolve_imports()     # Handle import resolution
  │
  ├── metrics.py          # Code complexity and quality metrics
  │   ├── calculate_complexity()  # Using radon
  │   ├── check_coverage()       # Using coverage.py
  │   └── evaluate_docs()        # Using pydocstyle
  │
  ├── config.py          # Configuration handling
  │   ├── load_config()   # Load from pyproject.toml
  │   ├── validate()      # Validate settings
  │   └── merge()         # Merge with defaults
  │
  └── logging.py         # Centralized logging
      ├── setup_logging()  # Configure loguru
      ├── log_error()     # Error handling
      └── log_metrics()   # Metrics reporting
  ```

  Move the following functions from their current locations:
  1. From `file_importance.py`:
     - `calculate_file_scores()` → `metrics.py`
     - `build_import_graph()` → `graph_utils.py`
  2. From `importance.py`:
     - `calculate_importance()` → `metrics.py`
  3. From `core/utils.py`:
     - `setup_logging()` → `logging.py`
     - `normalize_path()` → `config.py`

- [!] **Add Basic Tests:**
  Create the following test files with specific test cases:
  ```
  tests/
  ├── processors/
  │   └── test_file_importance.py
  │       ├── test_graph_building()  # Test import graph construction
  │       ├── test_metrics()         # Test each metric calculation
  │       ├── test_config()          # Test config validation
  │       └── test_error_handling()  # Test dependency errors
  │
  ├── test_utils/
  │   ├── test_files/               # Sample Python files for testing
  │   │   ├── simple_module.py
  │   │   ├── complex_module.py
  │   │   └── error_module.py
  │   └── fixtures.py               # Common test fixtures
  │
  └── conftest.py                   # pytest configuration
  ```

  Add performance benchmarks:
  ```python
  # benchmarks/test_performance.py
  def test_large_codebase():
      """Test processing speed on large codebase."""
      # Process >1000 Python files
      # Measure memory usage
      # Track processing time
  ```

## 3. Feature Integration

- [x] **Complete File Importance Integration:**
  ```python
  # In ImportanceProcessor.process():
  def process(self, stub_result: StubResult) -> StubResult:
      # Calculate file scores
      package_dir = str(stub_result.source_path.parent)
      self._file_scores = calculate_file_scores(package_dir, self.config.file_importance)
      
      # Process AST
      tree = ast.parse(stub_result.stub_content)
      for node in ast.walk(tree):
          if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
              score = self.calculate_importance(
                  name=node.name,
                  docstring=ast.get_docstring(node),
                  is_public=not node.name.startswith('_'),
                  is_special=node.name.startswith('__'),
                  extra_info={"file_path": stub_result.source_path}
              )
              # TODO: Store score in node.importance_score
              
      return stub_result
  ```

- [!] **Enhance Configuration System:**
  Create a comprehensive configuration system with the following components:

  1. `FileImportanceConfig` class:
  ```python
  @dataclass
  class FileImportanceConfig:
      exclude_dirs: list[str]
      centrality_measure: Literal["pagerank", "betweenness", "eigenvector"]
      coverage_data_path: Path | None
      weights: dict[str, float]
      min_coverage_threshold: float
      max_complexity_threshold: float
      doc_quality_checks: list[str]
      cache_results: bool
  ```

  2. Configuration validation:
  ```python
  def validate_config(config: FileImportanceConfig) -> None:
      """Validate configuration values."""
      # Check thresholds are between 0 and 1
      # Verify centrality measure is valid
      # Ensure weights sum to 1.0
      # Validate paths exist
  ```

  3. CLI integration:
  ```python
  @click.command()
  @click.option("--config", type=click.Path(), help="Config file path")
  @click.option("--exclude", multiple=True, help="Dirs to exclude")
  @click.option("--measure", type=click.Choice(["pagerank", "betweenness"]))
  def main(config: str, exclude: list[str], measure: str) -> None:
      """Process files with importance analysis."""
      # Load config from file
      # Override with CLI options
      # Run processing
  ```

  4. Documentation:
  ```markdown
  # Configuration Guide
  
  ## 4. File Importance Settings
  - exclude_dirs: List of directories to exclude
  - centrality_measure: Graph centrality algorithm
  - weights: Metric importance weights
  
  ## 5. Examples
  ```toml
  [importance]
  exclude_dirs = ["tests", "examples"]
  centrality_measure = "pagerank"
  weights = { complexity = 0.3, coverage = 0.3, docs = 0.4 }
  ```
  ```

- [!] **Improve Processors:**
  Enhance the processor system with the following improvements:

  1. Update `DocstringProcessor`:
  ```python
  class DocstringProcessor:
      """Process docstrings with quality metrics."""
      
      def process_file_docstring(self) -> float:
          """Handle file-level docstrings."""
          
      def use_importance_score(self) -> None:
          """Incorporate file importance."""
          
      def check_quality(self) -> float:
          """Check docstring quality."""
  ```

  2. Update `ImportProcessor`:
  ```python
  class ImportProcessor:
      """Handle imports with caching."""
      
      def process_imports(self) -> None:
          """Process with cached graph."""
          
      def handle_circular(self) -> None:
          """Handle circular imports."""
          
      def support_namespace(self) -> None:
          """Support namespace packages."""
  ```

  3. Add progress reporting:
  ```python
  from rich.progress import Progress
  
  def process_files() -> None:
      """Process files with progress bar."""
      with Progress() as progress:
          task = progress.add_task("Processing", total=len(files))
          for file in files:
              process_file(file)
              progress.update(task, advance=1)
  ```

  4. Add caching system:
  ```python
  class ImportanceCache:
      """Cache file importance scores."""
      
      def __init__(self, cache_dir: Path) -> None:
          self.cache_dir = cache_dir
          
      def get_score(self, file_path: Path) -> float:
          """Get cached score."""
          
      def set_score(self, file_path: Path, score: float) -> None:
          """Cache new score."""
  ```

## 6. Documentation and Examples

- [ ] **Update Documentation:**
  Create comprehensive documentation covering:
  1. Architecture overview
  2. Metric explanations
  3. Configuration guide
  4. Troubleshooting guide
  5. Performance tips
  6. Migration guide

- [ ] **Add Examples:**
  Create example code for:
  1. Basic usage
  2. Custom metrics
  3. Integration patterns
  4. Configuration examples
  5. Performance optimization

## 7. Testing and Validation

- [ ] **Add Comprehensive Tests:**
  Add tests for:
  1. Unit tests (current coverage is only 1%)
  2. Integration tests
  3. Performance tests
  4. Edge case tests
  5. Regression tests

- [ ] **Add Benchmarks:**
  Create benchmarks for:
  1. Processing time
  2. Memory usage
  3. Baseline comparison
  4. Bottleneck profiling
  5. Optimization documentation

## 8. Final Steps

- [ ] **Clean Up Old Code:**
  1. Remove standalone scripts
  2. Update imports
  3. Remove deprecated code
  4. Update documentation

- [ ] **Release Preparation:**
  1. Update version numbers
  2. Update changelog
  3. Update README
  4. Prepare release notes

## 9. Next Actions

1. [x] Add new dependencies to `pyproject.toml`
2. [x] Fix import resolution issues
3. [x] Fix critical linter errors
4. [x] Fix remaining linter errors
5. [!] Fix type checking errors
6. [!] Improve code organization
7. [!] Add basic tests
8. [!] Enhance configuration system

## 10. Notes

- The file importance analysis is now integrated into the main package
- The new system combines both file-level and symbol-level importance
- All changes maintain backward compatibility
- Performance impact should be monitored
- Optional dependencies are now properly handled with graceful fallbacks
- Current test coverage is only 1%, needs immediate attention
- Several complex functions need refactoring
- Type system needs modernization (using newer Python type hints)

---



================================================================
End of Codebase
================================================================
